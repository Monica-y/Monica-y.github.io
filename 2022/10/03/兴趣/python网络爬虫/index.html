<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"monica-y.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="python 爬虫学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫学习">
<meta property="og:url" content="https://monica-y.github.io/2022/10/03/%E5%85%B4%E8%B6%A3/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="python 爬虫学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-10-03T03:26:19.887Z">
<meta property="article:modified_time" content="2022-10-07T15:28:43.183Z">
<meta property="article:author" content="Monica">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://monica-y.github.io/2022/10/03/%E5%85%B4%E8%B6%A3/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>python爬虫学习 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monica-y.github.io/2022/10/03/%E5%85%B4%E8%B6%A3/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monica">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          python爬虫学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-03 11:26:19" itemprop="dateCreated datePublished" datetime="2022-10-03T11:26:19+08:00">2022-10-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-07 23:28:43" itemprop="dateModified" datetime="2022-10-07T23:28:43+08:00">2022-10-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          
            <div class="post-description">python 爬虫学习笔记</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="1-爬虫简介"><a href="#1-爬虫简介" class="headerlink" title="1 爬虫简介"></a>1 爬虫简介</h2><p>什么是爬虫：通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。<br>爬虫的价值：抓取互联网上的数据，有了大量的数据，就如同有了一个数据银行一样，下一步做的就是如何将这些爬取的数据产品化、商品化。</p>
<h2 id="2-爬虫合法性探究"><a href="#2-爬虫合法性探究" class="headerlink" title="2 爬虫合法性探究"></a>2 爬虫合法性探究</h2><p>1)在法律中是不被禁止<br>2)具有违法风险<br>3)善意爬虫，恶意爬虫</p>
<p>爬虫带来的风险可以体现在如下2方面：<br>1)爬虫干扰了被访问网站的正常运营<br>2)爬虫抓取了受到法律保护的特定类型的数据或信息</p>
<p>如何正确使用爬虫：<br>1)时常优化自己的程序，避免干扰被访问网站的正常运行<br>2)在使用、传播爬取到的数据时，审查抓取到的内容，如果发现了涉及到用户隐私、商业机密等敏感内容需要及时停止爬取或传播</p>
<h2 id="3-爬虫初始深入"><a href="#3-爬虫初始深入" class="headerlink" title="3 爬虫初始深入"></a>3 爬虫初始深入</h2><h3 id="3-1-爬虫在使用场景中的分类"><a href="#3-1-爬虫在使用场景中的分类" class="headerlink" title="3.1 爬虫在使用场景中的分类"></a>3.1 爬虫在使用场景中的分类</h3><h4 id="3-1-1-通用爬虫"><a href="#3-1-1-通用爬虫" class="headerlink" title="3.1.1 通用爬虫"></a>3.1.1 通用爬虫</h4><p>抓取系统重要组成部分。抓取的是一整张页面数据。</p>
<h4 id="3-1-2-聚焦爬虫"><a href="#3-1-2-聚焦爬虫" class="headerlink" title="3.1.2 聚焦爬虫"></a>3.1.2 聚焦爬虫</h4><p>是建立在通用爬虫的基础之上。抓取的是页面中特定的局部内容。</p>
<h4 id="3-1-3-增量式爬虫"><a href="#3-1-3-增量式爬虫" class="headerlink" title="3.1.3 增量式爬虫"></a>3.1.3 增量式爬虫</h4><p>监测网站中数据更新的情况。只会抓取网站中最新更新出来的数据。</p>
<h3 id="3-2-反爬机制"><a href="#3-2-反爬机制" class="headerlink" title="3.2 反爬机制"></a>3.2 反爬机制</h3><p>门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。</p>
<h3 id="3-3-反反爬策略"><a href="#3-3-反反爬策略" class="headerlink" title="3.3 反反爬策略"></a>3.3 反反爬策略</h3><p>爬虫程序可以通过制定相关的策略或技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站数据。</p>
<h3 id="3-4-robots-txt协议"><a href="#3-4-robots-txt协议" class="headerlink" title="3.4 robots.txt协议"></a>3.4 robots.txt协议</h3><p>君子协议。规定了网站中哪些数据可以被爬虫爬取，哪些数据不可以被爬取。<br>例如访问<a href="https://www.taobao.com/robots.txt" target="_blank" rel="noopener">淘宝的robots协议</a></p>
<h2 id="4-http和https协议"><a href="#4-http和https协议" class="headerlink" title="4 http和https协议"></a>4 http和https协议</h2><h3 id="4-1-http协议"><a href="#4-1-http协议" class="headerlink" title="4.1 http协议"></a>4.1 http协议</h3><p>http就是服务器和客户端进行数据交互的一种形式。</p>
<h4 id="4-1-1-常用请求头信息"><a href="#4-1-1-常用请求头信息" class="headerlink" title="4.1.1 常用请求头信息"></a>4.1.1 常用请求头信息</h4><p>1)User-Agent:请求载体的身份标识。(Google浏览器F12进入network模块，随便打开一个数据包)<br>2)Connection:请求完毕后，是断开连接还是保持连接</p>
<h4 id="4-1-2-常用响应头信息"><a href="#4-1-2-常用响应头信息" class="headerlink" title="4.1.2 常用响应头信息"></a>4.1.2 常用响应头信息</h4><p>Content-Type:服务器响应回客户端的数据类型。</p>
<h3 id="4-2-https"><a href="#4-2-https" class="headerlink" title="4.2 https"></a>4.2 https</h3><p>安全的超文本传输协议。</p>
<h4 id="4-2-1-加密方式"><a href="#4-2-1-加密方式" class="headerlink" title="4.2.1 加密方式"></a>4.2.1 加密方式</h4><p>1)对称密钥加密<br>2)非对称密钥加密<br>3)证书密钥加密</p>
<h2 id="5-requests模块"><a href="#5-requests模块" class="headerlink" title="5 requests模块"></a>5 requests模块</h2><h3 id="5-1-python网络请求的相关模块"><a href="#5-1-python网络请求的相关模块" class="headerlink" title="5.1 python网络请求的相关模块"></a>5.1 python网络请求的相关模块</h3><p>python中涉及网络请求的模块:<br>1)urllib模块(古老)<br>2)requests模块(普遍使用)</p>
<p>requests模块：python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。<br>作用：模拟浏览器发请求。</p>
<h3 id="5-2-如何使用requests模块"><a href="#5-2-如何使用requests模块" class="headerlink" title="5.2 如何使用requests模块"></a>5.2 如何使用requests模块</h3><h4 id="5-2-1-requests模块的编码流程"><a href="#5-2-1-requests模块的编码流程" class="headerlink" title="5.2.1 requests模块的编码流程"></a>5.2.1 requests模块的编码流程</h4><p>也是浏览器发请求的过程<br>1)指定url<br>2)发起请求<br>3)获取响应数据<br>4)持久化存储(将数据存储到本地或是数据库)</p>
<h4 id="5-2-2-爬取搜狗首页的页面数据"><a href="#5-2-2-爬取搜狗首页的页面数据" class="headerlink" title="5.2.2 爬取搜狗首页的页面数据"></a>5.2.2 爬取搜狗首页的页面数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需求：爬取搜狗首页的页面数据</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 1 指定url</span></span><br><span class="line">    url = <span class="string">'https://www.sogou.com/'</span></span><br><span class="line">    <span class="comment"># 2 发起请求</span></span><br><span class="line">    <span class="comment"># get方法会返回一个响应对象</span></span><br><span class="line">    response = requests.get(url=url)</span><br><span class="line">    <span class="comment"># 3 获取响应数据,text返回的是字符串形式的响应数据</span></span><br><span class="line">    page_text = response.text</span><br><span class="line">    print(page_text)</span><br><span class="line">    <span class="comment"># 4 持久化存储</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./爬虫实战/sougou.html'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line">    print(<span class="string">'爬取数据结束'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="6-requests巩固深入案例介绍"><a href="#6-requests巩固深入案例介绍" class="headerlink" title="6 requests巩固深入案例介绍"></a>6 requests巩固深入案例介绍</h2><p>需求一：爬取搜狗指定词条对应的搜索结果页面，也就是一个简易网页采集器<br>需求二：破解百度翻译<br>需求三：爬取豆瓣电影分类排行榜 <a href="https://movie.douban.com/" target="_blank" rel="noopener">https://movie.douban.com/</a> 中的电影详情数据<br>需求四：爬取肯德基餐厅查询<a href="http://www.kfc.com.cn/kfccda/index.aspx" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/index.aspx</a> 中指定地点的餐厅数据<br>数据五：爬取国家药品监督管理总局基于中华人民共和国化妆品生产许可证相关数据<a href="http://scxk.nmpa.gov.cn:81/xk/" target="_blank" rel="noopener">http://scxk.nmpa.gov.cn:81/xk/</a></p>
<h2 id="7-requests模块巩固深入案例之简易网页采集器"><a href="#7-requests模块巩固深入案例之简易网页采集器" class="headerlink" title="7 requests模块巩固深入案例之简易网页采集器"></a>7 requests模块巩固深入案例之简易网页采集器</h2><h3 id="7-1-UA检测和UA伪装"><a href="#7-1-UA检测和UA伪装" class="headerlink" title="7.1 UA检测和UA伪装"></a>7.1 UA检测和UA伪装</h3><p>UA检测<br>UA:User-Agent(请求载体的身份表示)<br>UA伪装：门户网站的服务器会检测对应请求的载体身份标识，如果检测到请求的载体身份标识为某一款浏览器，说明该请求是一个正常的请求。但是，如果检测到请求的载体身份标识不是基于某一款浏览器的，则表示该请求为不正常的请求(爬虫)，服务器很有可能拒绝该次请求。<br>UA伪装<br>让爬虫对应的请求载体身份标识伪装成某一款浏览器。</p>
<h3 id="7-2-网页采集器源码"><a href="#7-2-网页采集器源码" class="headerlink" title="7.2 网页采集器源码"></a>7.2 网页采集器源码</h3><p>由于信息头中的信息不完整，导致遇到百度安全验证，没有成功爬取到数据，<a href="https://www.bilibili.com/read/cv15625175" target="_blank" rel="noopener">解决方法</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#UA伪装：将对应的User-Agent封装到一个字典中</span></span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">     <span class="string">"Accept"</span>: <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9"</span>,</span><br><span class="line">     <span class="string">"Accept-Language"</span>: <span class="string">"zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7"</span>,</span><br><span class="line">     <span class="string">"Connection"</span>: <span class="string">"keep-alive"</span>,</span><br><span class="line">     <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate, br"</span>,</span><br><span class="line">     <span class="string">"Host"</span>: <span class="string">"www.baidu.com"</span>,</span><br><span class="line">     <span class="comment"># 需要更换Cookie</span></span><br><span class="line">     <span class="string">"Cookie"</span>: <span class="string">'BIDUPSID=DBD0225D9BF8DEB79CA1C7887D1732C0; PSTM=1649423443; H_WISE_SIDS=110085_127969_174443_184716_189755_194085_194519_194529_195342_196427_197242_197471_197711_197956_199022_199568_201108_201193_201700_202284_202651_203190_203310_203504_203517_204031_204123_204255_204726_204779_204864_204905_205218_205235_205379_205548_205568_205909_206008_206167_207021_207178_207235_207473_207616_207670_207715_207716_207831_207863_207887_208055_208115_208225_208267_208271_208312_208344_208494_208716_208721_208757_208758_208771_208773_208790_209115_209295_209310_209455_209473_209486_209519_209568_209630_209674_209704_209748_209817; BDUSS=EZXNm9pcHpIaXhoRkx5bnR-LTVmR0RFc0Nvb3JNVEJMR3dBUEZaZDVKSnVFYmxpRVFBQUFBJCQAAAAAAAAAAAEAAAAqpKZT1u648MvvwcEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG6EkWJuhJFiU; BDUSS_BFESS=EZXNm9pcHpIaXhoRkx5bnR-LTVmR0RFc0Nvb3JNVEJMR3dBUEZaZDVKSnVFYmxpRVFBQUFBJCQAAAAAAAAAAAEAAAAqpKZT1u648MvvwcEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG6EkWJuhJFiU; H_WISE_SIDS_BFESS=110085_127969_174443_184716_189755_194085_194519_194529_195342_196427_197242_197471_197711_197956_199022_199568_201108_201193_201700_202284_202651_203190_203310_203504_203517_204031_204123_204255_204726_204779_204864_204905_205218_205235_205379_205548_205568_205909_206008_206167_207021_207178_207235_207473_207616_207670_207715_207716_207831_207863_207887_208055_208115_208225_208267_208271_208312_208344_208494_208716_208721_208757_208758_208771_208773_208790_209115_209295_209310_209455_209473_209486_209519_209568_209630_209674_209704_209748_209817; BAIDUID=03278F35A48ECC64A1E2A882027FEF73:SL=0:NR=50:FG=1; ZFY=rClc:AfuGhuwKe3KzODcg9SGhZEHh:BE4dmMgsfV:A:A3R8:C; BAIDUID_BFESS=03278F35A48ECC64A1E2A882027FEF73:SL=0:NR=50:FG=1; channel=google; H_PS_PSSID=37156_36557_37357_36885_34813_37403_37398_36806_36789_37444_26350_37284_37364_37457; BA_HECTOR=8l0k8h8k040h04012k0k9a151hjkvq21b; Hm_lvt_55b574651fcae74b0a9f1cf9c8d7c93a=1662514438,1662601585,1663160955,1664790536; ab_sr=1.0.1_NTM0ZGRhNjNjMmE2YTcyZTMxNTJkZGY1MTdmNGFkYzhiZWQzYjE3ODRmYmE4ZmY2OTQ4NjA2N2RlMzMzMDg0NDNhZDE1NzJhNTdkMDU3MTEzYmIyM2RlZDY0ZmFjMjA1Njk1NWY4YmExODJjYmFjODY5MDg3NjlkZjIwY2JhMDAwN2RlMWFlYWJiYTAwMjQ2MDhjZDkwY2IyNmQyYTg0MTczMWI0MDM4MDJmZmQzNmE1NDQ0MGZmZTU1ZmI1Mzhm; baikeVisitId=56320696-49f7-4da7-8213-0e20716f3b1a; RT="z=1&amp;dm=baidu.com&amp;si=cr3jt70ia88&amp;ss=l8slat3w&amp;sl=3&amp;tt=4x3&amp;bcn=https://fclog.baidu.com/log/weirwood?type=perf"; Hm_lpvt_55b574651fcae74b0a9f1cf9c8d7c93a=1664792369'</span>&#125;</span><br><span class="line">    <span class="comment"># 1 指定url</span></span><br><span class="line">    url = <span class="string">'https://www.baidu.com/s?ie=UTF-8&amp;'</span></span><br><span class="line">    <span class="comment"># 处理url携带的参数：封装到字典中</span></span><br><span class="line">    kw = input(<span class="string">'enter a word:'</span>)</span><br><span class="line">    param = &#123;<span class="string">'wd'</span>:kw&#125;</span><br><span class="line">    <span class="comment"># 2 发起请求 对指定的url发起的请求对应的url是携带参数的，并且请求过程中处理了参数</span></span><br><span class="line">    <span class="comment"># headers为数据包的头部信息</span></span><br><span class="line">    response = requests.get(url=url,params=param,headers=headers,timeout=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 用于解决乱码</span></span><br><span class="line">    response.encoding = response.apparent_encoding</span><br><span class="line">    <span class="comment"># 3 获取响应数据,text返回的是字符串形式的响应数据</span></span><br><span class="line">    page_text = response.text</span><br><span class="line">    <span class="comment"># 4 持久化存储</span></span><br><span class="line">    filename = <span class="string">'./爬虫实战/'</span>+kw+<span class="string">'.html'</span></span><br><span class="line">    <span class="keyword">with</span> open(filename,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line">    print(<span class="string">'爬取数据结束'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="8-requests模块巩固深入案例之破解百度翻译"><a href="#8-requests模块巩固深入案例之破解百度翻译" class="headerlink" title="8 requests模块巩固深入案例之破解百度翻译"></a>8 requests模块巩固深入案例之破解百度翻译</h2><p>ajax可以实现在不刷新页面的前提下提交请求。<br>经过使用google开发者工具分析(想看kw需要点击headers模块旁边的payload模块)，得到了百度翻译的相关要点如下：<br>1)对应请求是一个post请求(携带了参数)<br>2)响应数据是一组json数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 1 进行UA伪装</span></span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">     <span class="string">"Accept"</span>: <span class="string">"application/json, text/javascript, */*; q=0.01"</span>,</span><br><span class="line">     <span class="string">"Accept-Language"</span>: <span class="string">"zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7"</span>,</span><br><span class="line">     <span class="string">"Connection"</span>: <span class="string">"keep-alive"</span>,</span><br><span class="line">     <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate, br"</span>,</span><br><span class="line">     <span class="comment"># 需要更换Cookie</span></span><br><span class="line">     <span class="string">"Cookie"</span>: <span class="string">'BIDUPSID=DBD0225D9BF8DEB79CA1C7887D1732C0; PSTM=1649423443; H_WISE_SIDS=110085_127969_174443_184716_189755_194085_194519_194529_195342_196427_197242_197471_197711_197956_199022_199568_201108_201193_201700_202284_202651_203190_203310_203504_203517_204031_204123_204255_204726_204779_204864_204905_205218_205235_205379_205548_205568_205909_206008_206167_207021_207178_207235_207473_207616_207670_207715_207716_207831_207863_207887_208055_208115_208225_208267_208271_208312_208344_208494_208716_208721_208757_208758_208771_208773_208790_209115_209295_209310_209455_209473_209486_209519_209568_209630_209674_209704_209748_209817; BDUSS=EZXNm9pcHpIaXhoRkx5bnR-LTVmR0RFc0Nvb3JNVEJMR3dBUEZaZDVKSnVFYmxpRVFBQUFBJCQAAAAAAAAAAAEAAAAqpKZT1u648MvvwcEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG6EkWJuhJFiU; BDUSS_BFESS=EZXNm9pcHpIaXhoRkx5bnR-LTVmR0RFc0Nvb3JNVEJMR3dBUEZaZDVKSnVFYmxpRVFBQUFBJCQAAAAAAAAAAAEAAAAqpKZT1u648MvvwcEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG6EkWJuhJFiU; H_WISE_SIDS_BFESS=110085_127969_174443_184716_189755_194085_194519_194529_195342_196427_197242_197471_197711_197956_199022_199568_201108_201193_201700_202284_202651_203190_203310_203504_203517_204031_204123_204255_204726_204779_204864_204905_205218_205235_205379_205548_205568_205909_206008_206167_207021_207178_207235_207473_207616_207670_207715_207716_207831_207863_207887_208055_208115_208225_208267_208271_208312_208344_208494_208716_208721_208757_208758_208771_208773_208790_209115_209295_209310_209455_209473_209486_209519_209568_209630_209674_209704_209748_209817; BAIDUID=03278F35A48ECC64A1E2A882027FEF73:SL=0:NR=50:FG=1; ZFY=rClc:AfuGhuwKe3KzODcg9SGhZEHh:BE4dmMgsfV:A:A3R8:C; BAIDUID_BFESS=03278F35A48ECC64A1E2A882027FEF73:SL=0:NR=50:FG=1; H_PS_PSSID=37156_36557_37357_36885_34813_37403_37398_36806_36789_37444_26350_37284_37364_37457; BA_HECTOR=8l0k8h8k040h04012k0k9a151hjkvq21b; RT="z=1&amp;dm=baidu.com&amp;si=v42m1kx0xcg&amp;ss=l8srkef6&amp;sl=0&amp;tt=0&amp;bcn=https://fclog.baidu.com/log/weirwood?type=perf&amp;ul=1u2&amp;hd=1xr"; APPGUIDE_10_0_2=1; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1664801076; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1664801137; ab_sr=1.0.1_NDFlOWRiZjY2ODY2MmNhNWU0NDkwMzEwODg2NmZiNDhhYTcwNzgyMzM0MDBlOWViZGU4Y2JkNDY1YThkYjA3ODQ2MGUyYjZhYWEwZmQxYjdiN2UxYzBkZmVmNTdkNzY5NmQ5OTQ3NzYyMDY2Nzc0MzhlMDVhMDZjZmNjZGVhMDNmMjYzZWM4OTRiODE5YTI5MTY2YTA4NWQ2NDBlYzBmMTJjMGUxOWY3Y2QxMjMzNGJmZTlkMjA0NDlhNGIxMDEz'</span>&#125;</span><br><span class="line">    <span class="comment"># 2 指定url</span></span><br><span class="line">    post_url = <span class="string">'https://fanyi.baidu.com/sug'</span></span><br><span class="line">    <span class="comment"># 3 post请求参数处理(同get请求一致)</span></span><br><span class="line">    word = input(<span class="string">'enter a word:'</span>)</span><br><span class="line">    data = &#123;<span class="string">'kw'</span>:word&#125;</span><br><span class="line">    <span class="comment"># 4 请求发送</span></span><br><span class="line">    response = requests.post(url=post_url,data=data,headers=headers)</span><br><span class="line">    <span class="comment"># 5 获取响应数据:json方法返回的是obj(如果确认响应数据是json类型的，才可以使用json，从content-Type获得)</span></span><br><span class="line">    dic_obj = response.json()</span><br><span class="line">    <span class="comment"># 6 持久化存储</span></span><br><span class="line">    filename = <span class="string">'./爬虫实战/'</span>+word+<span class="string">'.json'</span></span><br><span class="line">    fp = open(filename,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    json.dump(dic_obj,fp=fp,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">"数据爬取结束"</span>)</span><br></pre></td></tr></table></figure>
<p>在实战的过程中注意根据抓包工具分析的结果灵活地修改header信息。</p>
<h2 id="9-requests模块巩固深入案例之豆瓣电影"><a href="#9-requests模块巩固深入案例之豆瓣电影" class="headerlink" title="9 requests模块巩固深入案例之豆瓣电影"></a>9 requests模块巩固深入案例之豆瓣电影</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url = <span class="string">'https://movie.douban.com/j/chart/top_list?'</span></span><br><span class="line">    param = &#123;<span class="string">'type'</span>:<span class="string">'24'</span>,</span><br><span class="line">            <span class="string">'interval_id'</span>:<span class="string">'100:90'</span>,</span><br><span class="line">            <span class="string">'action'</span>:<span class="string">''</span>, </span><br><span class="line">            <span class="comment"># 表示开始的电影编号</span></span><br><span class="line">            <span class="string">'start'</span>:<span class="string">'0'</span>,</span><br><span class="line">            <span class="comment"># 表示一次请求取出的电影个数</span></span><br><span class="line">            <span class="string">'limit'</span>:<span class="string">'20'</span>&#125;</span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">               <span class="string">'Accept'</span>:<span class="string">"*/*"</span>,</span><br><span class="line">               <span class="string">'Accept-Language'</span>:<span class="string">"zh-CN,zh;q=0.9"</span>,</span><br><span class="line">               <span class="string">'Connection'</span>:<span class="string">"keep-alive"</span>,</span><br><span class="line">               <span class="string">'Accept-Encoding'</span>:<span class="string">"gzip, deflate, br"</span>,</span><br><span class="line">               <span class="string">'Cookie'</span>:<span class="string">'bid=1uxw3rydo9g; ll="118375"; _ga=GA1.2.1418048783.1664847198; _gid=GA1.2.1522197539.1664847198; Hm_lvt_19fc7b106453f97b6a84d64302f21a04=1664847600; Hm_lpvt_19fc7b106453f97b6a84d64302f21a04=1664847600; _ck_desktop_mode=1; vmode=pc; _pk_ref.100001.4cf6=["","",1664847659,"https://m.douban.com/"]; _pk_ses.100001.4cf6=*; ap_v=0,6.0; __utma=30149280.1418048783.1664847198.1664847664.1664847664.1; __utmb=30149280.0.10.1664847664; __utmc=30149280; __utmz=30149280.1664847664.1.1.utmcsr=m.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utma=223695111.1418048783.1664847198.1664847664.1664847664.1; __utmb=223695111.0.10.1664847664; __utmc=223695111; __utmz=223695111.1664847664.1.1.utmcsr=m.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; _pk_id.100001.4cf6=704fb404cefb3879.1664847659.1.1664847668.1664847659.'</span>&#125;</span><br><span class="line">    response = requests.get(url=url,params=param,headers=headers)</span><br><span class="line">    <span class="comment"># 从抓包工具得到response最外面是一个[]</span></span><br><span class="line">    list_response = response.json()</span><br><span class="line">    fp = open(<span class="string">'./爬虫实战/douban.json'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    json.dump(list_response,fp = fp,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">"爬取结束"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="10-实战练习"><a href="#10-实战练习" class="headerlink" title="10 实战练习"></a>10 实战练习</h2><p>注意这个url不能删减。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url = <span class="string">'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'</span></span><br><span class="line">    keyword = input(<span class="string">'input a city:'</span>)</span><br><span class="line">    param = &#123;<span class="string">'cname'</span>:<span class="string">''</span>, </span><br><span class="line">             <span class="string">'pid'</span>:<span class="string">''</span>, </span><br><span class="line">             <span class="string">'keyword'</span>:keyword,</span><br><span class="line">             <span class="string">'pageIndex'</span>:<span class="string">'1'</span>,</span><br><span class="line">             <span class="string">'pageSize'</span>:<span class="string">'10'</span>&#125;</span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">               <span class="string">'Accept'</span>:<span class="string">'appliacation/json, text/javascript, */*;q=0.01'</span>,</span><br><span class="line">               <span class="string">'Accept-Language'</span>:<span class="string">'zh-CN,zh;q=0.9'</span>,</span><br><span class="line">               <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</span><br><span class="line">               <span class="string">'Accept-Encoding'</span>:<span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">               <span class="string">'Cookie'</span>:<span class="string">'ASP.NET_SessionId=qhtoewbc1sro4z45gs1kqldj; route-cell=ksa; Hm_lvt_1039f1218e57655b6677f30913227148=1664851918; Hm_lpvt_1039f1218e57655b6677f30913227148=1664851918; SERVERID=a8ef50e74cdb73da974d5f9b427a5159|1664852839|1664851918'</span>&#125;</span><br><span class="line">    data = requests.post(url=url,params=param,headers=headers)</span><br><span class="line">    data_json = data.json()</span><br><span class="line">    fp = open(<span class="string">'./爬虫实战/kfc.json'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    json.dump(data_json,fp=fp,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">"爬取结束"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="11-综合练习之药监总局1"><a href="#11-综合练习之药监总局1" class="headerlink" title="11 综合练习之药监总局1"></a>11 综合练习之药监总局1</h2><p>系统维护，10.8 9；00以后可以访问</p>
<h2 id="12-综合练习之药监总局2"><a href="#12-综合练习之药监总局2" class="headerlink" title="12 综合练习之药监总局2"></a>12 综合练习之药监总局2</h2><p>系统维护，10.8 9；00以后可以访问</p>
<h2 id="13-综合练习之药监总局3-screenflow"><a href="#13-综合练习之药监总局3-screenflow" class="headerlink" title="13 综合练习之药监总局3 screenflow"></a>13 综合练习之药监总局3 screenflow</h2><p>系统维护，10.8 9；00以后可以访问</p>
<h2 id="14-综合练习之药监总局4-screenflow"><a href="#14-综合练习之药监总局4-screenflow" class="headerlink" title="14 综合练习之药监总局4 screenflow"></a>14 综合练习之药监总局4 screenflow</h2><p>系统维护，10.8 9；00以后可以访问</p>
<h2 id="15-总结回顾"><a href="#15-总结回顾" class="headerlink" title="15 总结回顾"></a>15 总结回顾</h2><p>系统维护，10.8 9；00以后可以访问</p>
<h2 id="16-数据解析概述"><a href="#16-数据解析概述" class="headerlink" title="16 数据解析概述"></a>16 数据解析概述</h2><h3 id="16-1-聚焦爬虫"><a href="#16-1-聚焦爬虫" class="headerlink" title="16.1 聚焦爬虫"></a>16.1 聚焦爬虫</h3><p>聚焦爬虫：爬取页面中指定的页面内容。可能会用到数据解析。<br>编码流程：<br>1)指定url<br>2)发起请求<br>3)获取响应数据<br>4)数据解析<br>5)持久化存储</p>
<h3 id="16-2-数据解析分类"><a href="#16-2-数据解析分类" class="headerlink" title="16.2 数据解析分类"></a>16.2 数据解析分类</h3><p>1)正则表达式<br>2)bs4<br>3)xpath(通用性较强)</p>
<h3 id="16-3-数据解析原理"><a href="#16-3-数据解析原理" class="headerlink" title="16.3 数据解析原理"></a>16.3 数据解析原理</h3><p>解析数据可能存在的地方：<br>1)标签的中间。<br>2)对应标签的属性中。</p>
<p>解析的步骤：<br>1)进行指定标签的定位<br>2)标签或者标签对应的属性中存储的数据进行提取。(解析)</p>
<h2 id="17-图片数据爬取"><a href="#17-图片数据爬取" class="headerlink" title="17 图片数据爬取"></a>17 图片数据爬取</h2><h3 id="17-1-正则表达式"><a href="#17-1-正则表达式" class="headerlink" title="17.1 正则表达式"></a>17.1 正则表达式</h3><p>一、限定符<br>‘?’表示’d’需要出现0次或是1次，’d’可有可无，used?可以和use匹配，也可以和used匹配。<br>‘<em>‘表示’b’可以出现0次或多次。ab\</em>c可以和ac匹配,也可以和abbbc匹配。<br>‘+表示’b’可以出现1次或是多次。ab+c可以和abc匹配，也可以和abbbc匹配。<br>使用{}来限制允许字符出现的次数。如果要限制b出现的次数在2到6之间，使用ab{2,6}c即可。如果是要限制b出现的次数在2次以上，那么可以使用ab{2,}c来实现。<br>‘()’可以用来限制{}内数字的作用域，如我们需要在字符串中ab多次出现，可以使用(ab)+来匹配具有ababab特征的字符串。<br>二、或运算<br>‘a (cat|dog)’可以用来匹配”a cat”或者”a dog”这样的字符串。<br>三、字符类<br>‘[]’内的内容代表要求匹配的字符只能取自于它们。例如’[abc]+’会与’aabbcc’匹配，但是不会与’dog’匹配。<br>同时我们可以在’[]’内指定字符的范围，例如’[a-z]+’代表所有小写英文字符。’[a-zA-Z]+’表示所有的英文字符。’[a-zA-Z0-9]+’代表所有英文字符和数字。<br>‘^’表示非，例如’<sup><a href="#fn_0-9" id="reffn_0-9">0-9</a></sup>+’表示所有的非数字字符(包括换行符)。<br>四、元字符<br>正则表达式中预先定义好了一系列常用的字符类型，比如数字、空白符、单词开头、结尾等等。正则表达式中的大多数元字符都以反斜杠开头：<br>1)’\d’代表数字字符。’\d’+等价于’[0-9]+’。<br>2)’\w’代表单词字符。也就是所用的英文字符、数字和下划线。<br>3)’\s’代表空白符(包含Tab和换行符)。<br>4)’\D’代表非数字字符。<br>5)’\W’代表非单词字符。<br>6)’\S’代表非空白字符。<br>‘.’在正则表达式中是一个特殊字符，它代表任意字符，但不包含换行符。<br>‘^’匹配行首，’$’匹配行尾。例如’^a’只会匹配位于行首的a，’a$’只会匹配位于行尾的a。<br>五、贪婪与懒惰匹配<br>在*，+，{}去匹配字符串的时候，默认会去匹配尽可能多的字符。例如我们使用’&lt;.+&gt;’会匹配到完整的html语句。但是如果我们在+之后加上一个？，则会将正则表达式中默认的贪婪匹配切换为懒惰匹配，’&lt;.+?&gt;’就可以匹配到所有位于’&lt;&gt;’内的标签。<br>举个栗子1：匹配所有的RGB颜色值需要使用的语句是’#[a-fA-F0-9]{6}\b’,其中’\b’表示单词字符的边界，这样可以避免这里的文本也被识别成RGB颜色值。<br>举个栗子2：IPV4地址匹配需要使用的语句是’\d+.\d+.\d+.\d+’,’.‘是用反斜杠做转义，其作用是匹配字典。但是上述语句没有限定IP地址的范围是0-255，我们可以这样来限定：’25[0-5]|2[0-4]\d|[01]?\d\d?’。这是分情况讨论了三位中每一位数字的情况，同时IP地址有可能只有两位或是一位，所以我们在表达式中加了问号来表示这种情况。在实际匹配中，我们使用’上述表达式{3}上述表达式’就可以完成没有前导0的IP地址的匹配。<br>实战源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">key = <span class="string">'saas and sas and saaas'</span></span><br><span class="line">re.findall(<span class="string">'sa&#123;1,2&#125;s'</span>,key)</span><br></pre></td></tr></table></figure>
<p>问题在于我们如何爬取网页上的图片数据呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 如何获取图片数据</span></span><br><span class="line">    url = <span class="string">'https://monica-y.github.io/images/快速排序.png'</span></span><br><span class="line">    <span class="comment"># content返回的是二进制形式的图片数据</span></span><br><span class="line">    <span class="comment"># text(字符串),content(二进制),json(对象)</span></span><br><span class="line">    img_data = requests.get(url=url).content</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./爬虫实战/blog.png'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(img_data)</span><br></pre></td></tr></table></figure>
<h2 id="18-正则解析案例1"><a href="#18-正则解析案例1" class="headerlink" title="18 正则解析案例1"></a>18 正则解析案例1</h2><p>爬取百度百科词条诸葛亮对应页面下的所有图案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url = <span class="string">'https://baike.baidu.com/item/诸葛亮/21048'</span></span><br><span class="line">    <span class="comment"># UA伪装</span></span><br><span class="line">    header = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>:<span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Accept-Language'</span>:<span class="string">'zh-CN,zh;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</span><br><span class="line">    <span class="string">'Accept-Encoding'</span>:<span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>:<span class="string">'zhishiTopicRequestTime=1664883739169; BAIKE_SHITONG=&#123;"data":"df15e34a625616491a92c5f4ebee725d2b8040b74ceff0e59f4bd890e7110ab4111cf9176d8512c95b6b008a45b3508b1e58bd011af8dbf7d35f78fc2c089ae299e42fe5c840bc39c68acb020d9a3e973d22e51741597a44ff12bfa6fbcf237c","key_id":"10","sign":"7721bcc8"&#125;; BIDUPSID=DBD0225D9BF8DEB79CA1C7887D1732C0; PSTM=1649423443; H_WISE_SIDS=110085_127969_174443_184716_189755_194085_194519_194529_195342_196427_197242_197471_197711_197956_199022_199568_201108_201193_201700_202284_202651_203190_203310_203504_203517_204031_204123_204255_204726_204779_204864_204905_205218_205235_205379_205548_205568_205909_206008_206167_207021_207178_207235_207473_207616_207670_207715_207716_207831_207863_207887_208055_208115_208225_208267_208271_208312_208344_208494_208716_208721_208757_208758_208771_208773_208790_209115_209295_209310_209455_209473_209486_209519_209568_209630_209674_209704_209748_209817; BDUSS=EZXNm9pcHpIaXhoRkx5bnR-LTVmR0RFc0Nvb3JNVEJMR3dBUEZaZDVKSnVFYmxpRVFBQUFBJCQAAAAAAAAAAAEAAAAqpKZT1u648MvvwcEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG6EkWJuhJFiU; BDUSS_BFESS=EZXNm9pcHpIaXhoRkx5bnR-LTVmR0RFc0Nvb3JNVEJMR3dBUEZaZDVKSnVFYmxpRVFBQUFBJCQAAAAAAAAAAAEAAAAqpKZT1u648MvvwcEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG6EkWJuhJFiU; H_WISE_SIDS_BFESS=110085_127969_174443_184716_189755_194085_194519_194529_195342_196427_197242_197471_197711_197956_199022_199568_201108_201193_201700_202284_202651_203190_203310_203504_203517_204031_204123_204255_204726_204779_204864_204905_205218_205235_205379_205548_205568_205909_206008_206167_207021_207178_207235_207473_207616_207670_207715_207716_207831_207863_207887_208055_208115_208225_208267_208271_208312_208344_208494_208716_208721_208757_208758_208771_208773_208790_209115_209295_209310_209455_209473_209486_209519_209568_209630_209674_209704_209748_209817; BAIDUID=03278F35A48ECC64A1E2A882027FEF73:SL=0:NR=50:FG=1; ZFY=rClc:AfuGhuwKe3KzODcg9SGhZEHh:BE4dmMgsfV:A:A3R8:C; BAIDUID_BFESS=03278F35A48ECC64A1E2A882027FEF73:SL=0:NR=50:FG=1; channel=google; BAIDU_SSP_lcr=https://www.google.com/; BAIDU_WISE_UID=wapp_1664883581852_651; Hm_lvt_55b574651fcae74b0a9f1cf9c8d7c93a=1662601585,1663160955,1664790536,1664883739; Hm_lpvt_55b574651fcae74b0a9f1cf9c8d7c93a=1664883739; baikeVisitId=b4b0fd02-5d25-4b56-be74-13522b7d6d99; ab_sr=1.0.1_ODgzYTBmOGUwMTU0OGMwYmQ3MmE0Njc1NDYyMzEzMTEzOGM1NTE3MjhiNTUxNWEwZDBkOWFhZTQ0OTAzZjIwM2ExYmFlYWEwMzU0ZmJjNTRlYzk4OTA2ZjBjZWMyYTc0ZGE0Mjc5YzZlMjMxYzllOTJiOTk2YTRhY2Q0MzUyYzBiZmIyOTdlZDgxNTc0OWRjOGNiZGYyYjE2MzkyNDE3ZDRkMDljN2UwODQ5YTY0NWE2NDMzNDFkZmRmMDlmMDMy; RT="z=1&amp;dm=baidu.com&amp;si=784f20e0-98cd-4c2d-98c3-01ce1b209343&amp;ss=l8u4p2h9&amp;sl=5&amp;tt=bg2&amp;bcn=https://fclog.baidu.com/log/weirwood?type=perf&amp;ld=3igr&amp;cl=5kvg&amp;ul=417ak"'</span>&#125;</span><br><span class="line">    <span class="comment"># 使用通用爬虫对url对应的一整张页面进行爬取</span></span><br><span class="line">    page_text = requests.get(url=url,headers=header).text</span><br><span class="line">    <span class="comment"># fp = open('./爬虫实战/zgl.html','w',encoding='utf-8')</span></span><br><span class="line">    <span class="comment"># fp.write(page_text.text)</span></span><br><span class="line">    <span class="comment"># 使用聚焦爬虫将页面中所有的图片进行解析/提取</span></span><br><span class="line">    <span class="comment"># &lt;div class="lemma-picture J-lemma-picture text-pic layout-right" style="width:220px; float: right;"&gt;</span></span><br><span class="line">    <span class="comment"># ex = '&lt;div class = "content"&gt;.*?&lt;img class src="(.*?)" alt.*?&lt;/div&gt;'</span></span><br><span class="line">    ex = <span class="string">'data-src="(.*?)"'</span></span><br><span class="line">    <span class="comment"># re.S单行匹配，re.M多行匹配</span></span><br><span class="line">    img_src = re.findall(ex,page_text,re.S)</span><br><span class="line">    <span class="comment"># print(img_src)</span></span><br><span class="line">    <span class="comment"># 创建一个文件夹，保存所有的图片</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'./爬虫实战/诸葛亮图片'</span>):</span><br><span class="line">        os.mkdir(<span class="string">'./爬虫实战/诸葛亮图片'</span>)</span><br><span class="line">    cnt = <span class="string">'a'</span></span><br><span class="line">    <span class="keyword">for</span> src <span class="keyword">in</span> img_src:</span><br><span class="line">        <span class="comment"># 请求到了图片的二进制数据</span></span><br><span class="line">        img_data = requests.get(url=src,headers=header).content</span><br><span class="line">        <span class="comment"># 生成图片名称</span></span><br><span class="line">        filename = cnt+<span class="string">'.webp'</span></span><br><span class="line">        cnt = cnt+<span class="string">'a'</span></span><br><span class="line">        img_path = <span class="string">'./爬虫实战/诸葛亮图片/'</span>+filename</span><br><span class="line">        <span class="keyword">with</span> open(img_path,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            fp.write(img_data)</span><br><span class="line">            print(filename,<span class="string">'下载成功'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="19-正则解析案例2"><a href="#19-正则解析案例2" class="headerlink" title="19 正则解析案例2"></a>19 正则解析案例2</h2><p>在url中设置变量的方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://www.qiushibaike.com/pic/page/%d/?s=5184961'</span></span><br><span class="line"><span class="comment"># (1,36)是页码范围</span></span><br><span class="line"><span class="keyword">for</span> pageNum <span class="keyword">in</span>(<span class="number">1</span>,<span class="number">36</span>):</span><br><span class="line">    <span class="comment"># 对应页码的url</span></span><br><span class="line">    new_url = format(url%pageNum)</span><br><span class="line"><span class="comment"># '&#123;&#125;'.format()</span></span><br></pre></td></tr></table></figure>
<h2 id="20-bs4解析概述"><a href="#20-bs4解析概述" class="headerlink" title="20 bs4解析概述"></a>20 bs4解析概述</h2><h3 id="20-1-bs4解析原理"><a href="#20-1-bs4解析原理" class="headerlink" title="20.1 bs4解析原理"></a>20.1 bs4解析原理</h3><p>python特有的解析方式。<br>bs4数据解析的原理：<br>1)实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中 。<br>2)通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取。</p>
<h3 id="20-2-beautifulSoup实例化"><a href="#20-2-beautifulSoup实例化" class="headerlink" title="20.2 beautifulSoup实例化"></a>20.2 beautifulSoup实例化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure>
<p>1.将本地的html文档中的数据加载到该对象中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fp = open(<span class="string">'./爬虫实战/zgl.html'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">soup = BeautifulSoup(fp,<span class="string">'lxml'</span>)</span><br></pre></td></tr></table></figure>
<p>2.将互联网上获取的页面源码加载到该对象中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">page_text = response.text</span><br><span class="line">soup = BeautifulSoup(page_text,<span class="string">'lxml'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="21-bs4解析具体使用详解"><a href="#21-bs4解析具体使用详解" class="headerlink" title="21 bs4解析具体使用详解"></a>21 bs4解析具体使用详解</h2><h3 id="21-1-beautifulSoup提供的用于数据解析的方法和属性"><a href="#21-1-beautifulSoup提供的用于数据解析的方法和属性" class="headerlink" title="21.1 beautifulSoup提供的用于数据解析的方法和属性"></a>21.1 beautifulSoup提供的用于数据解析的方法和属性</h3><p>soup.tagName 返回的是html中第一次出现的tagName标签。<br>soup.find():<br>1.soup.find(tagName)等同于soup.div<br>2.属性定位 soup.find(‘a’,class_/id/attr=”declare-details”)<br>soup.find_all(‘tagName’):返回符合要求的所有标签(列表)<br>soup.select():<br>1.select(‘某种选择器(id,class,标签…选择器)’)，返回的是一个列表。<br>2.soup.select(‘.header &gt; .layout &gt; .declare-wrap’),层级选择器，’&gt;’表示的是一个层级,中间不可以跨层级。<br>3.soup.select(‘.header .declare-wrap’)，空格可以表示多个层级。</p>
<h3 id="21-2-获取标签之间的文本数据"><a href="#21-2-获取标签之间的文本数据" class="headerlink" title="21.2 获取标签之间的文本数据"></a>21.2 获取标签之间的文本数据</h3><p>soup.a.text/string/get_text(),具体使用时注意加上下标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup.select(<span class="string">'.header .declare'</span>)[<span class="number">0</span>].text</span><br></pre></td></tr></table></figure>
<p>text/get_text():可以获得该标签下所有的文本内容，即也会获取到子标签的文本内容。<br>string:只可以获取该标签下直系的文本内容。</p>
<h3 id="21-3-获取标签中属性值"><a href="#21-3-获取标签中属性值" class="headerlink" title="21.3 获取标签中属性值"></a>21.3 获取标签中属性值</h3><p>soup.a[‘href’],使用实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup.select(<span class="string">'.header .declare-details'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</span><br></pre></td></tr></table></figure>
<h2 id="22-bs4解析案例实战"><a href="#22-bs4解析案例实战" class="headerlink" title="22 bs4解析案例实战"></a>22 bs4解析案例实战</h2><p>从诗词名句网上爬取一整部的三国演义小说。遇到的问题有：<br>1.乱码问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于解决乱码</span></span><br><span class="line">page_text.encoding = page_text.apparent_encoding</span><br></pre></td></tr></table></figure>
<p>2.NoneType无法调用get_text()函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> text_tag <span class="keyword">is</span> <span class="literal">None</span>:<span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<p>完整源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 对首页的页面数据进行爬取</span></span><br><span class="line">    url = <span class="string">'http://www.purepen.com/sgyy/'</span></span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>:<span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Accept-Encoding'</span>:<span class="string">'gzip, deflate'</span>,</span><br><span class="line">    <span class="string">'Accept-Language'</span>:<span class="string">'zh-CN,zh;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Host'</span>:<span class="string">'www.purepen.com'</span>,</span><br><span class="line">    <span class="string">'Proxy-Connection'</span>:<span class="string">'keep-alive'</span>&#125;</span><br><span class="line">    page_text = requests.get(url=url,headers=headers)</span><br><span class="line">    <span class="comment"># 用于解决乱码</span></span><br><span class="line">    page_text.encoding = page_text.apparent_encoding</span><br><span class="line">    page_text = page_text.text</span><br><span class="line">    <span class="comment"># print(page_text.text)</span></span><br><span class="line">    <span class="comment"># 在首页中解析出章节的标题和详情页的url</span></span><br><span class="line">    <span class="comment"># 1 实例化BeautifulSoup的对象，需要将页面源码数据加载到该对象中</span></span><br><span class="line">    soup = BeautifulSoup(page_text,<span class="string">'lxml'</span>)</span><br><span class="line">    a_list = soup.select(<span class="string">'table a'</span>)</span><br><span class="line">    fp = open(<span class="string">'./爬虫实战/三国.txt'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># print(a_list)</span></span><br><span class="line">    <span class="comment">################debug#####################</span></span><br><span class="line">    <span class="comment"># detail_url = 'http://www.purepen.com/sgyy/'+a_list[113]['href']</span></span><br><span class="line">    <span class="comment"># # 对详情页发起请求，解析出章节内容</span></span><br><span class="line">    <span class="comment"># detail_page_text = requests.get(url=detail_url,headers=headers)</span></span><br><span class="line">    <span class="comment"># detail_page_text.encoding = detail_page_text.apparent_encoding</span></span><br><span class="line">    <span class="comment"># detail_page_text = detail_page_text.text</span></span><br><span class="line">    <span class="comment"># # print(detail_page_text)</span></span><br><span class="line">    <span class="comment"># detail_soup = BeautifulSoup(detail_page_text,'lxml')</span></span><br><span class="line">    <span class="comment"># print(detail_soup.find('font',face='宋体'))</span></span><br><span class="line">    <span class="comment"># text_tag = detail_soup.find('font',face='宋体')</span></span><br><span class="line">    <span class="comment"># detail_text = text_tag.get_text()</span></span><br><span class="line">    <span class="comment"># print(detail_text)</span></span><br><span class="line">    <span class="comment">################debug#####################</span></span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> a_list:</span><br><span class="line">        title = elem.string</span><br><span class="line">        detail_url = <span class="string">'http://www.purepen.com/sgyy/'</span>+elem[<span class="string">'href'</span>]</span><br><span class="line">        <span class="comment"># 对详情页发起请求，解析出章节内容</span></span><br><span class="line">        detail_page_text = requests.get(url=detail_url,headers=headers)</span><br><span class="line">        detail_page_text.encoding = detail_page_text.apparent_encoding</span><br><span class="line">        detail_page_text = detail_page_text.text</span><br><span class="line">        <span class="comment"># 解析出详情页中相关的章节内容</span></span><br><span class="line">        detail_soup = BeautifulSoup(detail_page_text,<span class="string">'lxml'</span>)</span><br><span class="line">        text_tag = detail_soup.find(<span class="string">'font'</span>,face=<span class="string">'宋体'</span>)</span><br><span class="line">        <span class="keyword">if</span> text_tag <span class="keyword">is</span> <span class="literal">None</span>:<span class="keyword">continue</span></span><br><span class="line">        detail_text = text_tag.get_text()</span><br><span class="line">        <span class="comment"># 持久化处理</span></span><br><span class="line">        fp.write(title+<span class="string">':'</span>+detail_text+<span class="string">'\n'</span>)</span><br><span class="line">        print(title,<span class="string">'爬取成功'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="23-xpath解析基础1"><a href="#23-xpath解析基础1" class="headerlink" title="23 xpath解析基础1"></a>23 xpath解析基础1</h2><p>xpath解析：最常用且便捷高效的一种解析方式，最为通用。</p>
<h3 id="23-1-xpath解析原理"><a href="#23-1-xpath解析原理" class="headerlink" title="23.1 xpath解析原理"></a>23.1 xpath解析原理</h3><p>1.实例化一个etree的对象，且需要被解析的页面源码加载到对象中。<br>2.调用etree对象中的xpath方法结合xpath表达式实现标签的定位和内容的捕获。</p>
<h3 id="23-2-实例化etree对象"><a href="#23-2-实例化etree对象" class="headerlink" title="23.2 实例化etree对象"></a>23.2 实例化etree对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br></pre></td></tr></table></figure>
<p>1.将本地的html文档中的源码数据加载到etree对象中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">etree.parse(filePath)</span><br></pre></td></tr></table></figure>
<p>2.可以将从互联网上获取的源码数据加载到该对象中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">etree.HTML(page_text)</span><br></pre></td></tr></table></figure>
<h2 id="24-xpath解析基础2"><a href="#24-xpath解析基础2" class="headerlink" title="24 xpath解析基础2"></a>24 xpath解析基础2</h2><h3 id="24-1-xpath-‘xpath’表达式"><a href="#24-1-xpath-‘xpath’表达式" class="headerlink" title="24.1 xpath(‘xpath’表达式)"></a>24.1 xpath(‘xpath’表达式)</h3><p>1.’/‘：表示的是从根节点开始的定位。同时在表达式中一个’/‘表示的是下一个层级。<br>2.’//‘表示多个层级。’//div’的功能是找到源码中所有的div标签。<br>3.属性定位的方法’//meta[@name=”description”]’。通用格式是’tag[@attrName=”attrValue”]’<br>4.索引定位方法’//head/meta[4]’，注意list的索引不是从0开始的，而是从1开始的。</p>
<h3 id="24-3-获取文本"><a href="#24-3-获取文本" class="headerlink" title="24.3 获取文本"></a>24.3 获取文本</h3><p>1.’//head/title/text()’:只能获取标签<strong>间</strong>的直系文本内容。<br>2.’//div[@class=”declare-wrap”]//text()’：或许标签<strong>间</strong>的所有文本内容，包括子标签间。</p>
<h3 id="24-4-获取属性"><a href="#24-4-获取属性" class="headerlink" title="24.4 获取属性"></a>24.4 获取属性</h3><p>‘//div[@class=”declare-wrap”]//a/@href’：获取a标签的href属性。格式为’/@attrName’。</p>
<h2 id="25-xpath实战-58二手房"><a href="#25-xpath实战-58二手房" class="headerlink" title="25 xpath实战-58二手房"></a>25 xpath实战-58二手房</h2><p>爬取58二手房中相关的房源信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 爬取页面源码数据</span></span><br><span class="line">    url = <span class="string">"https://bj.58.com/ershoufang/?PGTID=0d100000-0000-104c-1acb-491d6609ee01&amp;ClickID=5"</span></span><br><span class="line">    header = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>:<span class="string">'*/*'</span>,</span><br><span class="line">    <span class="string">'Accept-encoding'</span>:<span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">    <span class="string">'Accept-Language'</span>:<span class="string">'zh-CN,zh;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>:<span class="string">'id58=CocIjmM+yhVLn+gHLrwxAg==; city=bj; 58home=bj; 58tj_uuid=7af828cc-f973-4d2a-95cc-032f488f20ed; new_uv=1; utm_source=; spm=; init_refer=; wmda_uuid=9602a30531f6d88e561302899c8331c5; wmda_new_uuid=1; wmda_session_id_11187958619315=1665059392160-5e12be9b-c21d-fc2a; wmda_visited_projects=;11187958619315; als=0; aQQ_ajkguid=3ECFB47A-B884-4B18-A6FC-00AD1E6F0EBF; sessid=86E82895-80AB-499C-8785-9D16B1732620; ajk-appVersion=; ctid=1; fzq_h=73060fa7621b58b5bcf02b70bcd33cc8_1665059430571_b164ddb5b7b44a74a51e25c0a1aed939_1908977250; new_session=0; JSESSIONID=CE4EEE52A5CDDBC2C8B8C8AEF109F233; xxzl_cid=7e2fb22a84144e3096678912e56d6361; xxzl_deviceid=COpSCAAH8vpuxSiIZJzXkVfavvuoH9JGOt4O6xM2vDWU7FsK18PgIMjWZ65e6QyW'</span>&#125;</span><br><span class="line">    <span class="comment"># page_text = requests.get(url=url,headers=header).text</span></span><br><span class="line">    <span class="comment"># fp = open('./爬虫实战/58.html','w',encoding='utf-8')</span></span><br><span class="line">    <span class="comment"># fp.write(page_text)</span></span><br><span class="line">    <span class="comment"># 数据解析</span></span><br><span class="line">    filepath = <span class="string">'./爬虫实战/58.html'</span></span><br><span class="line">    tree = etree.parse(filepath,parser=etree.HTMLParser(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    title_list = tree.xpath(<span class="string">'//h3/@title'</span>)</span><br><span class="line">    fp = open(<span class="string">'./爬虫实战/58.txt'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> title_list:</span><br><span class="line">        fp.write(elem+<span class="string">'\n'</span>)</span><br><span class="line">    print(title_list)</span><br><span class="line">    <span class="comment"># for elem in title_list:</span></span><br><span class="line">    <span class="comment">#     elem.xpath('./@title') .表示从elem所在层级开始，而不是从根开始</span></span><br></pre></td></tr></table></figure>
<h2 id="26-xpath解析案例-4k图片解析下载"><a href="#26-xpath解析案例-4k图片解析下载" class="headerlink" title="26 xpath解析案例-4k图片解析下载"></a>26 xpath解析案例-4k图片解析下载</h2><p>视频中解决中文乱码的问题的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_name.encode(<span class="string">'iso-8859-1'</span>).decode(<span class="string">'gbk'</span>)</span><br></pre></td></tr></table></figure>
<p>实验源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url = <span class="string">'http://pic.netbian.com/4kmeinv/'</span></span><br><span class="line">    header = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>:<span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Accept-encoding'</span>:<span class="string">'gzip, deflate'</span>,</span><br><span class="line">    <span class="string">'Accept-language'</span>:<span class="string">'zh-CN,zh;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>:<span class="string">'__yjs_duid=1_60a75cb337fd0b67a092fac41e69956d1665137066790; zkhanecookieclassrecord=,54,; Hm_lvt_c59f2e992a863c2744e1ba985abaea6c=1665137069; Hm_lpvt_c59f2e992a863c2744e1ba985abaea6c=1665137069'</span>,</span><br><span class="line">    <span class="string">'Host'</span>:<span class="string">'pic.netbian.com'</span>&#125;</span><br><span class="line">    <span class="comment"># 这里需要解决一下乱码问题，也可以直接requests.get().content</span></span><br><span class="line">    response = requests.get(url=url,headers=header)</span><br><span class="line">        <span class="comment"># 设置编码格式建议用：response.encoding=response.apparent_encoding</span></span><br><span class="line">    response.encoding=response.apparent_encoding</span><br><span class="line">    page_text = response.text</span><br><span class="line">    <span class="comment"># print(page_text)</span></span><br><span class="line">    <span class="comment"># 数据解析 src的属性值和alt的属性值 </span></span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    li_list = tree.xpath(<span class="string">'//ul[@class="clearfix"]/li'</span>)</span><br><span class="line">    <span class="comment"># print(li_list)</span></span><br><span class="line">    <span class="comment"># 创建一个文件夹</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'./爬虫实战/美女'</span>):</span><br><span class="line">        os.mkdir(<span class="string">'./爬虫实战/美女'</span>)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">        img_src = <span class="string">'http://pic.netbian.com'</span>+li.xpath(<span class="string">'.//img/@src'</span>)[<span class="number">0</span>]</span><br><span class="line">        img_name = li.xpath(<span class="string">'.//img/@alt'</span>)[<span class="number">0</span>]+<span class="string">'.jpg'</span></span><br><span class="line">        <span class="comment"># print(img_name,img_src)</span></span><br><span class="line">        img_data = requests.get(url=img_src,headers=header).content</span><br><span class="line">        img_path = <span class="string">'./爬虫实战/美女/'</span>+img_name</span><br><span class="line">        <span class="keyword">with</span> open(img_path,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            fp.write(img_data)</span><br><span class="line">            print(img_name,<span class="string">'爬取成功'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="27-xpath解析案例-全国城市名称爬取"><a href="#27-xpath解析案例-全国城市名称爬取" class="headerlink" title="27 xpath解析案例-全国城市名称爬取"></a>27 xpath解析案例-全国城市名称爬取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># //div[@class="bottom"]/ul/li/a 热门城市a标签的层级关系</span></span><br><span class="line"><span class="comment"># //div[@class="bottom"]/ul/div[2]/li/a 全部城市a标签的层级关系</span></span><br><span class="line"><span class="comment"># 两个表达式用|连接</span></span><br><span class="line">tree.xpath(<span class="string">'//div[@class="bottom"]/ul/li/a|//div[@class="bottom"]/ul/div[2]/li/a'</span>)</span><br><span class="line">all_city_name = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> a_list:</span><br><span class="line">    city_name = a.xpath(<span class="string">'./text()'</span>)</span><br><span class="line">    all_city_names.append(city_name)</span><br></pre></td></tr></table></figure>
<p>完整源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url = <span class="string">'https://www.aqistudy.cn/historydata/'</span></span><br><span class="line">    header = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>:<span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Accept-encoding'</span>:<span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">    <span class="string">'Accept-language'</span>:<span class="string">'zh-CN,zh;q=0.9'</span>,</span><br><span class="line">    <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>:<span class="string">'Hm_lvt_6088e7f72f5a363447d4bafe03026db8=1665149440; Hm_lpvt_6088e7f72f5a363447d4bafe03026db8=1665149440'</span>,</span><br><span class="line">    <span class="string">'Host'</span>:<span class="string">'www.aqistudy.cn'</span>&#125;</span><br><span class="line">    page_text = requests.get(url=url,headers=header)</span><br><span class="line">    page_text.encoding = page_text.apparent_encoding</span><br><span class="line">    page_text = page_text.text</span><br><span class="line">    tree = etree.HTML(page_text,parser=etree.HTMLParser(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    hot_list = tree.xpath(<span class="string">'//ul[@class="unstyled"]//a/text()'</span>)</span><br><span class="line">    all_city_name = []</span><br><span class="line">    all_city_name.append(hot_list)</span><br><span class="line">    print(all_city_name)</span><br></pre></td></tr></table></figure>
<h2 id="28-xpath作业"><a href="#28-xpath作业" class="headerlink" title="28 xpath作业"></a>28 xpath作业</h2><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz?p=28&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">学习进度</a></p>
<h2 id="29-验证码识别简介"><a href="#29-验证码识别简介" class="headerlink" title="29 验证码识别简介"></a>29 验证码识别简介</h2><h2 id="30-云打码使用流程"><a href="#30-云打码使用流程" class="headerlink" title="30 云打码使用流程"></a>30 云打码使用流程</h2>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"># python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ACWing/" rel="prev" title="AcWing算法学习">
      <i class="fa fa-chevron-left"></i> AcWing算法学习
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-爬虫简介"><span class="nav-number">1.</span> <span class="nav-text">1 爬虫简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-爬虫合法性探究"><span class="nav-number">2.</span> <span class="nav-text">2 爬虫合法性探究</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-爬虫初始深入"><span class="nav-number">3.</span> <span class="nav-text">3 爬虫初始深入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-爬虫在使用场景中的分类"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 爬虫在使用场景中的分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-通用爬虫"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 通用爬虫</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-聚焦爬虫"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 聚焦爬虫</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-增量式爬虫"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 增量式爬虫</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-反爬机制"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 反爬机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-反反爬策略"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 反反爬策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-robots-txt协议"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 robots.txt协议</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-http和https协议"><span class="nav-number">4.</span> <span class="nav-text">4 http和https协议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-http协议"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 http协议</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-常用请求头信息"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1 常用请求头信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-常用响应头信息"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.2 常用响应头信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-https"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 https</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-加密方式"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 加密方式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-requests模块"><span class="nav-number">5.</span> <span class="nav-text">5 requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-python网络请求的相关模块"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 python网络请求的相关模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-如何使用requests模块"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 如何使用requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-requests模块的编码流程"><span class="nav-number">5.2.1.</span> <span class="nav-text">5.2.1 requests模块的编码流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-爬取搜狗首页的页面数据"><span class="nav-number">5.2.2.</span> <span class="nav-text">5.2.2 爬取搜狗首页的页面数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-requests巩固深入案例介绍"><span class="nav-number">6.</span> <span class="nav-text">6 requests巩固深入案例介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-requests模块巩固深入案例之简易网页采集器"><span class="nav-number">7.</span> <span class="nav-text">7 requests模块巩固深入案例之简易网页采集器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-UA检测和UA伪装"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 UA检测和UA伪装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-网页采集器源码"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 网页采集器源码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-requests模块巩固深入案例之破解百度翻译"><span class="nav-number">8.</span> <span class="nav-text">8 requests模块巩固深入案例之破解百度翻译</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-requests模块巩固深入案例之豆瓣电影"><span class="nav-number">9.</span> <span class="nav-text">9 requests模块巩固深入案例之豆瓣电影</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-实战练习"><span class="nav-number">10.</span> <span class="nav-text">10 实战练习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-综合练习之药监总局1"><span class="nav-number">11.</span> <span class="nav-text">11 综合练习之药监总局1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-综合练习之药监总局2"><span class="nav-number">12.</span> <span class="nav-text">12 综合练习之药监总局2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-综合练习之药监总局3-screenflow"><span class="nav-number">13.</span> <span class="nav-text">13 综合练习之药监总局3 screenflow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-综合练习之药监总局4-screenflow"><span class="nav-number">14.</span> <span class="nav-text">14 综合练习之药监总局4 screenflow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-总结回顾"><span class="nav-number">15.</span> <span class="nav-text">15 总结回顾</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-数据解析概述"><span class="nav-number">16.</span> <span class="nav-text">16 数据解析概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#16-1-聚焦爬虫"><span class="nav-number">16.1.</span> <span class="nav-text">16.1 聚焦爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-数据解析分类"><span class="nav-number">16.2.</span> <span class="nav-text">16.2 数据解析分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-3-数据解析原理"><span class="nav-number">16.3.</span> <span class="nav-text">16.3 数据解析原理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#17-图片数据爬取"><span class="nav-number">17.</span> <span class="nav-text">17 图片数据爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-正则表达式"><span class="nav-number">17.1.</span> <span class="nav-text">17.1 正则表达式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-正则解析案例1"><span class="nav-number">18.</span> <span class="nav-text">18 正则解析案例1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#19-正则解析案例2"><span class="nav-number">19.</span> <span class="nav-text">19 正则解析案例2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#20-bs4解析概述"><span class="nav-number">20.</span> <span class="nav-text">20 bs4解析概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#20-1-bs4解析原理"><span class="nav-number">20.1.</span> <span class="nav-text">20.1 bs4解析原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#20-2-beautifulSoup实例化"><span class="nav-number">20.2.</span> <span class="nav-text">20.2 beautifulSoup实例化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#21-bs4解析具体使用详解"><span class="nav-number">21.</span> <span class="nav-text">21 bs4解析具体使用详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#21-1-beautifulSoup提供的用于数据解析的方法和属性"><span class="nav-number">21.1.</span> <span class="nav-text">21.1 beautifulSoup提供的用于数据解析的方法和属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#21-2-获取标签之间的文本数据"><span class="nav-number">21.2.</span> <span class="nav-text">21.2 获取标签之间的文本数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#21-3-获取标签中属性值"><span class="nav-number">21.3.</span> <span class="nav-text">21.3 获取标签中属性值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#22-bs4解析案例实战"><span class="nav-number">22.</span> <span class="nav-text">22 bs4解析案例实战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#23-xpath解析基础1"><span class="nav-number">23.</span> <span class="nav-text">23 xpath解析基础1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#23-1-xpath解析原理"><span class="nav-number">23.1.</span> <span class="nav-text">23.1 xpath解析原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-2-实例化etree对象"><span class="nav-number">23.2.</span> <span class="nav-text">23.2 实例化etree对象</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#24-xpath解析基础2"><span class="nav-number">24.</span> <span class="nav-text">24 xpath解析基础2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#24-1-xpath-‘xpath’表达式"><span class="nav-number">24.1.</span> <span class="nav-text">24.1 xpath(‘xpath’表达式)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#24-3-获取文本"><span class="nav-number">24.2.</span> <span class="nav-text">24.3 获取文本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#24-4-获取属性"><span class="nav-number">24.3.</span> <span class="nav-text">24.4 获取属性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#25-xpath实战-58二手房"><span class="nav-number">25.</span> <span class="nav-text">25 xpath实战-58二手房</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#26-xpath解析案例-4k图片解析下载"><span class="nav-number">26.</span> <span class="nav-text">26 xpath解析案例-4k图片解析下载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#27-xpath解析案例-全国城市名称爬取"><span class="nav-number">27.</span> <span class="nav-text">27 xpath解析案例-全国城市名称爬取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#28-xpath作业"><span class="nav-number">28.</span> <span class="nav-text">28 xpath作业</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#29-验证码识别简介"><span class="nav-number">29.</span> <span class="nav-text">29 验证码识别简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#30-云打码使用流程"><span class="nav-number">30.</span> <span class="nav-text">30 云打码使用流程</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monica</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Monica</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
