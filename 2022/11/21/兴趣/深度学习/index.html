<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"monica-y.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="深度学习学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习">
<meta property="og:url" content="https://monica-y.github.io/2022/11/21/%E5%85%B4%E8%B6%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="深度学习学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%862.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%863.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%864.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%865.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%866.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%867.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%868.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%869.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8610.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8611.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8612.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8613.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8614.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8615.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8616.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8617.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8618.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8619.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8620.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8621.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8622.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8623.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%8624.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/textRNN%E5%AE%9E%E7%8E%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/textRNN%E5%AE%9E%E7%8E%B02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN%E5%AE%9E%E7%8E%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN%E5%AE%9E%E7%8E%B02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN%E5%AE%9E%E7%8E%B03.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN%E5%AE%9E%E7%8E%B04.png">
<meta property="article:published_time" content="2022-11-21T12:50:02.077Z">
<meta property="article:modified_time" content="2022-11-26T14:05:46.664Z">
<meta property="article:author" content="Monica">
<meta property="article:tag" content="数据挖掘">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://monica-y.github.io/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861.png">

<link rel="canonical" href="https://monica-y.github.io/2022/11/21/%E5%85%B4%E8%B6%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monica-y.github.io/2022/11/21/%E5%85%B4%E8%B6%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monica">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-21 20:50:02" itemprop="dateCreated datePublished" datetime="2022-11-21T20:50:02+08:00">2022-11-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-26 22:05:46" itemprop="dateModified" datetime="2022-11-26T22:05:46+08:00">2022-11-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" itemprop="url" rel="index"><span itemprop="name">数据挖掘</span></a>
                </span>
            </span>

          
            <div class="post-description">深度学习学习笔记</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="1-神经网络和深度学习"><a href="#1-神经网络和深度学习" class="headerlink" title="1 神经网络和深度学习"></a>1 神经网络和深度学习</h2><h3 id="1-1-什么是神经网络"><a href="#1-1-什么是神经网络" class="headerlink" title="1.1 什么是神经网络"></a>1.1 什么是神经网络</h3><p>“深度学习”指的是训练神经网络的规模很大。<br><a href="https://www.bilibili.com/video/BV1FT4y1E74V?p=2&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">学习进度 01:31</a></p>
<h2 id="2-改进深度神经网络：超参数调整、正则化和优化"><a href="#2-改进深度神经网络：超参数调整、正则化和优化" class="headerlink" title="2 改进深度神经网络：超参数调整、正则化和优化"></a>2 改进深度神经网络：超参数调整、正则化和优化</h2><h2 id="3-结构化机器学习工程"><a href="#3-结构化机器学习工程" class="headerlink" title="3 结构化机器学习工程"></a>3 结构化机器学习工程</h2><h2 id="4-卷积神经网络-Convolutional-Neural-Networks"><a href="#4-卷积神经网络-Convolutional-Neural-Networks" class="headerlink" title="4 卷积神经网络 Convolutional Neural Networks"></a>4 卷积神经网络 Convolutional Neural Networks</h2><h2 id="5-自然语言处理：建立序列模型"><a href="#5-自然语言处理：建立序列模型" class="headerlink" title="5 自然语言处理：建立序列模型"></a>5 自然语言处理：建立序列模型</h2><p><a href="https://github.com/graykode/nlp-tutorial" target="_blank" rel="noopener">学长给的代码库</a></p>
<h3 id="5-1-数学符号"><a href="#5-1-数学符号" class="headerlink" title="5.1 数学符号"></a>5.1 数学符号</h3><p>例如我们需要在一句话中定位人的名字，即命名实体识别问题，这一问题常用于搜索引擎。命名实体识别系统可以用来查找不同类型的文本中的人名、公司名、时间、地点、国家名、货币名等等。<br>给定一段文字输入x，我们希望模型输出的数据y能标识x中的每个单词是否是人名的一部分。下图中，我们将输入的文字索引为x^1,x^2…x^9,然后我们对输出数据y使用相同的索引方式，y^1,y^2,y^3…y^9。我们使用T_x来表示输入序列的长度，T_y来表示输出序列的长度。<br><img src="/images/深度学习/自然语言处理1.png" alt="自然语言处理1"><br>对于第i个训练集x^i,我们用x^(i)(t)表示训练样本i序列中第t个元素，T_x是序列的长度，训练集中不同的训练样本就会有不同的长度，T_x^i表示第i个训练样本的输入序列长度。同样地，y^(i)(t)代表第i个训练样本中第t个元素，T_y^i就是第i个训练样本的输出序列的长度。<br>对于输入的序列x，我们需要思考如何表示一个序列里单独的单词。<br><img src="/images/深度学习/自然语言处理2.png" alt="自然语言处理2"><br>首先我们需要做一张词表，也叫做词典，就是将我将要用到的单词做成一个列表。这样词典中的每一个单词都对应了一个索引。<br>接下来我们使用one-hot表示法来表示词典里的每个单词，例如Harry在我们词典中的索引是4075，那么我们使用一个长度为10000(词典长度)的向量来表示一个单词，对于Harry来说，其余行都是0，只有第4075行是1。同样上图中potter也使用类似的表示方法。最终，我们将会使用9个向量来表示输入的文本x。我们的训练数据将会是带有对应标签的x。我们使用UNK来表示没有在词典中出现的单词。</p>
<h3 id="5-2-循环神经网络"><a href="#5-2-循环神经网络" class="headerlink" title="5.2 循环神经网络"></a>5.2 循环神经网络</h3><p>我们考虑使用标准的神经网络来完成x到y的映射。如下图，神经网络的输入为经过one-hot编码得到的9个向量，输出为9个值为0/1的项，来表示输入的单词是否是人名的一部分。<br><img src="/images/深度学习/自然语言处理3.png" alt="自然语言处理3"><br>但结果表明这个方法并不好，主要存在两个问题：<br>1 输入数据x和输出结果y在不同例子中可以有不同的长度。不是所有的例子都有着同样输入长度T_x。即使每个句子都有最大长度，也许我们能够填充或0填充，使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。<br>2 标准神经网络并不共享从文本的不同位置学到的特征。例如在输入文本索引1出现的harry是人名的一部分，那么当Harry出现在其他位置时，我们希望它也能被识别为人名的一部分。<br>同时我们需要注意到，这个输入层非常庞大，它的大小为每条样本中最大单词数乘以10000，那么第一层的权重矩阵就会有巨量的参数。<br><img src="/images/深度学习/自然语言处理4.png" alt="自然语言处理4"><br>为了改善这些问题，我们有了循环神经网络。如上图，图中每一个块代表一个神经网络。如果我们从左到右读句子x，那么第一个单词就是x^1,我们要做的就是将x^输入到第一个神经网络的隐藏层，我们让这个神经网络尝试预测并输出结果。当神经网络读到句子x的第二个单词x^2时,它不是仅用x^2预测出y^2,它也会输入一些来自时间步1(也就是上一步)的信息，时间步1的激活值(激活项，是指由一个具体的神经元计算并输出的值)就会传递到时间步2，然后在下一个时间步。循环神经网络输入了单词x^3,然后输出了预测结果y^3,直到最后一个时间步，神经网络的输入为x^(T_x),神经网络的输出为y^(T_y),对于这个样本数据，T_x = T_y,如果不等，则这个结构会需要作出一些改变。<br>所以，在每一个时间步中，循环神经网络传递一个激活值到下一个步中用于计算。要开始整个流程，我们在零时刻，需要编造一个激活值(伪激活值)，它通常是一个0向量。<br>在上图中的右边位置，是有些论文中用来表示循环神经网络的方法，在课程中，我们一般使用左边这种表示方法。<br>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。W_ax管理着从x^1到隐藏层的连接的一系列参数。每一个时间步使用的都是相同的参数W_ax。而激活值，也就是网络中的水平联系是由参数W_aa决定的，同时每一个时间步都使用相同的参数W_aa。网络的输入由W_ya决定。<br>在这个循环神经网络中，上一时间步的信息通过水平连接传递给下一时间步来帮助当前时间步进行预测。这个循环神经网络的缺点就是它只使用了这个序列中之前的信息来做出预测，而没有办法使用这个序列之后的信息，例如我们在预测索引3位置的单词时，没有用到4、5、6等等位置的信息。但是在实际使用中，我们做出正确的判断也需要后面的信息，如上图中的Teddy，在第一句中是人名，但在第二句中却不是。<br>在后面，我们学习的双向循环神经网络(BRNN)的时候解决这个问题，现在我们以RNN为例进行讲解。<br><img src="/images/深度学习/自然语言处理5.png" alt="自然语言处理5"><br>如上图，我们先来了解一下前向传播的过程。一般开始先输入a^0,它是一个零向量。接着为了计算a^1,我们使用</p>
<script type="math/tex; mode=display">a^{<1>} = g(W_{aa}\times a^{<0>}+W_{ax}\times x^{<1>}+b_a),g = tanh/ReLU</script><p>然后计算y^1:</p>
<script type="math/tex; mode=display">y^{<1>} = g(W_{ya}\times a^{<1>} + b_y),g = sigmoid</script><p>这里解释一下参数下标的含义，例如W_ax,第二个下标的名字为x，表示它将乘上一个x类型的量，第一个下标的名字为a，表示它是用来计算a类型的量的。<br>循环神经网络用的激活函数经常是选用tanh，有时也会用ReLU，但是tanh是更通常的选择。选用哪个激活函数是取决于我们的输出y，如果它是一个二分问题，激活函数为sigmoid，如果是k分类问题，激活函数是softmax。对于命名实体识别来说，y只可能是0或者1，因此我们计算y使用的激活函数为sigmoid。<br>同理我们可以计算出t时刻的a^t，结果如上图。<br><img src="/images/深度学习/自然语言处理6.png" alt="自然语言处理6"><br>在上图中，我们将Waa和Wax矩阵水平放置在一起，形成一个矩阵W_a。例如，a的维度是100(a指的是单元数目)，x的维度是10000，那么W_aa就是100*100的矩阵，W_ax就是100*10000的矩阵，因此如果将这两个矩阵并排放置，W_a就会是个100*10100维的矩阵，具体细节如上图右边。<br>则有:</p>
<script type="math/tex; mode=display">a^{\left \langle t \right \rangle} = g(W_a[a^{\left \langle t-1 \right \rangle},x^{\left \langle t \right \rangle}]+b_a)</script><p>[]的意思是将这两个向量竖直放置在一起，如上图中紫色部分，最终得到10100维的向量。仔细观察绿色部分的两个矩阵相乘，实际上就等价于W_aa和a相乘，W_ax和x相乘。<br>这样做的好处是我们可以不使用两个参数矩阵W_aa和W_ax,而是将其压缩成一个参数矩阵W_a,这样能够简化我们要用到的符号。<br>同样对于y的计算，我们可以简化成图中左边蓝色部分，W_y表明它是计算类型y的权重矩阵，而上面的W_a和W_b则表示这些参数是用于计算a的值(激活值)</p>
<h3 id="5-3-通过时间的反向传播"><a href="#5-3-通过时间的反向传播" class="headerlink" title="5.3 通过时间的反向传播"></a>5.3 通过时间的反向传播</h3><p><img src="/images/深度学习/自然语言处理7.png" alt="自然语言处理7"><br>在上图中，蓝色表示前向传播的计算方向，红色表示反向传播的计算方向。<br><img src="/images/深度学习/自然语言处理8.png" alt="自然语言处理8"><br>在上图中，我们有一系列的输入x^1,x^2,…,x^T_x。然后我们用x^1和a^0计算出时间步1的激活项a^1;用x^2和a^1计算出时间步2的激活项a^2。在这个过程中我们需要的参数为W_a和b_a,这两个参数会被用于计算出所有的激活项。<br>然后我们通过a^1计算出预测输出y^1,在下一个时间步中，我们通过a^2计算出预测输出y^2。在这个过程中我们需要的参数为W_y和b_y,这两个参数会被用于计算出所有的输出y。<br>在进行反向传播之前，我们需要一个损失函数。我们先定义一个元素损失函数，它对应的是序列中一个具体的词，如果第t个数据是人名，那么y^t就是1，然后神经网络将输出第t个值是名字的概率y_pre^t,例如y_pre^t=0.1。我们将损失函数定义为标准逻辑回归损失函数，也叫交叉熵损失函数。具体公式见上图红框部分。这是关于单个位置上或者说某个时间步t上某个单词的预测值的损失函数。<br>现在我们来定义整个序列的损失函数。将L定义为从t=1开始一直到t=T_x/T_y(因为T_x=T_y),对每一个时间步的损失求和。<br>于是在上图中，我们可以通过y_pre^t和y^t计算出时间步t的损失函数。最后我们的反向传播就是沿着红色箭头的方向进行计算。然后我们就可以使用梯度下降法来更新参数。在反向传播过程中最重要的信息传递就是从右到左的运算，因此这个算法还有个名字叫做通过时间(穿过时间)的反向传播。这是因为在前向传播中，我们是从左到右进行计算，在反向传播的过程中，我们从右到左进行计算，就像是时间倒流。</p>
<h3 id="5-4-不同类型的循环神经网络"><a href="#5-4-不同类型的循环神经网络" class="headerlink" title="5.4 不同类型的循环神经网络"></a>5.4 不同类型的循环神经网络</h3><p>在前面我们谈论的RNN模型T_x=T_y,但是在实际问题中，T_x和T_y不一定相等。我们需要修改基本的RNN结构来处理这些问题。<br><img src="/images/深度学习/自然语言处理9.png" alt="自然语言处理9"><br>多对多结构：上图中左边的位置，输入序列有很多的输入，而输出序列也有很多输出。<br>多对一结构：输入x可能是一段文本，例如一个电影的评论，输出y是1-5或是0-1的数字。我们一次输入一个单词，如图中中部位置，我们不再在每个时间步上都有输出了，而是让这个RNN网络读入整个句子，然后在最后一个时间上得到输出。它的特点是输入序列有很多，然后只输出一个数字。<br>一对一结构：这就是一个小型的标准的神经网络。<br><img src="/images/深度学习/自然语言处理10.png" alt="自然语言处理10"><br>一对多结构：常用于音乐生成。我们的输出是一些音符，对应于一段音乐。输入x可以是一个整数，表示我们想要生成的音乐类型。首先我们输入x得到RNN输出的第一个值，在下一个时间步中，模型没有输入参数，输出第二个值。我们会在开头提供一个伪激活项a^0。这里有一个后面会涉及的技术细节，当我们生成序列时，通常会把第一个合成的输出也喂给下一层，所以实际的网络结构最终如上图左边所示。<br>对于多对多模型，我们考虑输入和输出长度不同的情况，例如机器翻译。首先读入这个句子。在输入结束时这个网络就会输出翻译的结果。因此整个网络分成了部分，首先是一个编码器，用于获取输入，第二部分是一个解码器，它会读取整个句子，然后输出翻译成其他语言的结果。<br>总结一下各种各样的RNN结构如下图;<br><img src="/images/深度学习/自然语言处理11.png" alt="自然语言处理11"><br>一对一：如果去掉了a^0，它就是一种标准类型的神经网络，不需要RNN。<br>一对多：用于音乐生成或者序列生成。<br>多对一：用于情感分类。<br>多对多：T_x=T_y,用于命名实体识别。<br>多对多：用于机器翻译，T_x不等于T_y。</p>
<h3 id="5-5-语言模型和序列生成"><a href="#5-5-语言模型和序列生成" class="headerlink" title="5.5 语言模型和序列生成"></a>5.5 语言模型和序列生成</h3><h4 id="5-5-1-语言模型的作用"><a href="#5-5-1-语言模型的作用" class="headerlink" title="5.5.1 语言模型的作用"></a>5.5.1 语言模型的作用</h4><p><img src="/images/深度学习/自然语言处理12.png" alt="自然语言处理12"><br>我们在语音识别时可能会将语音输入识别为上图中两种可能的情况，然而此时我们应该想要表达的意思是第二种，一个好的语音识别系统能正确输出识别结果为第二种而不是第一种，即使这两句话听起来是如此相似。为了达到这一目的，我们需要使用一个语言模型，来计算这两句话各自的可能性，通过比较这两个概率值，来决定最终的输出结果。<br>语言识别模型的工作就是输出某个特定的句子出现的概率是多少，它是语音识别系统和机器翻译系统的重要组成部分，目的是正确输出最接近的句子。</p>
<h4 id="5-5-2-建立一个语言模型"><a href="#5-5-2-建立一个语言模型" class="headerlink" title="5.5.2 建立一个语言模型"></a>5.5.2 建立一个语言模型</h4><h5 id="5-5-2-1-标记化"><a href="#5-5-2-1-标记化" class="headerlink" title="5.5.2.1 标记化"></a>5.5.2.1 标记化</h5><p><img src="/images/深度学习/自然语言处理13.png" alt="自然语言处理13"><br>训练集：包含一个很大的英文文本语料库。语料库是自然语言处理的一个专有名词，在这里就是数量众多的英文句子组成的文本。<br>假如说我们训练集中有一句话，如上图，猫一天睡15个小时。我们要做的第一件事就是将句子标记化。就像是之前我们曾经提到过的一样，建立一个字典，然后对每一个单词使用one-hot编码。同时我们需要定义句子的结尾，一般的做法是增加一个额外的标记EOS，这样我们可以知道一个句子什么时候结束。因此我们需要对训练集中的每一个句子的结尾增加这个标记。如果按照这样的规则，我们可以得到9个标记，y^1,y^2,…y^9。在这个过程中，我们可以决定要不要把标点看成是标记。这里我们忽略了标记。<br>在上图中第三行的样本数据中，出现了不在我们字典中的一些单词。此时我们可以把Mau替换成一个UNK标记，代表未知词。我们只针对UNK建立概率模型，而不针对这个具体的词Mau。</p>
<h5 id="5-5-2-2-构建RNN"><a href="#5-5-2-2-构建RNN" class="headerlink" title="5.5.2.2 构建RNN"></a>5.5.2.2 构建RNN</h5><p><img src="/images/深度学习/自然语言处理14.png" alt="自然语言处理14"><br>在这里，x^t=y^(t-1)。在第0个时间步，我们计算激活项a^1,它是由a^0和x^1共同决定的，a^0和x^1会被设为全0向量。然后a^1通过softmax预测第一个词可能是y_pre^1。这一步实际上就是通过softmax来预测字典中的任意单词会是第一个词的概率。例如第一个词是cats的概率是多少？所以softmax层可能输出10000+2种结果，这是因为字典中一共有10000个词，再加上句子结尾和UNK标志。<br>然后RNN进入下一个时间步，我们计算激活项a^2，它是由a^1和y^1(即在时间步0中正确的输出，在这里y^1=Cats),最终预测得到的第二个词会是y_pre^2。输出结果同样经过softmax层进行预测，RNN的工作就是预测这些词的概率，而不去管正确的结果是什么，它只会考虑之前的值y^1。<br>然后再进入到RNN的下一个时间步，它的输入是y^2=average,输出是y_pre^3,也就是字典中每一个词出现在这里的概率，通过之前得到的cats和average。<br>以此类推，最后停在第9个时间步，此时的输入为y^8=day,输出为y_pre^9,y^9=EOS,因此我们希望EOS标志能有很高的概率。<br>所以RNN中的每一个时间步，都会考虑前面得到的单词，比如给他前三个单词，让他给出下个词的分布。RNN的工作就是学习从左到右地预测一个词。</p>
<h5 id="5-5-2-3-定义代价函数"><a href="#5-5-2-3-定义代价函数" class="headerlink" title="5.5.2.3 定义代价函数"></a>5.5.2.3 定义代价函数</h5><p>在某一个时间步时，如果真正的词是y^t,softmax层预测的词是y_pre^t，那么softmax的损失函数定义如上图中红框部分。<br>而总体损失函数就是把所有单个预测的损失函数都相加起来。<br>如果我们用很大的训练集来训练这个RNN，我们就可以通过开头的一系列单词来预测之后的单词的概率。<br>现在有一个新的句子y^1,y^2,y^3，现在要计算出句子中各个单词的概率，方法就是第一个softmax层会输出p(y^1),然后第二个softmax层会输出在考虑y^1情况下的p(y^2),最后第三个softmax层会输出在考虑y^1和y^2情况下的p(y^3)。把这三个概率相乘，最后得到这个含3个单词的整个句子的概率。</p>
<h3 id="5-6-新序列采样"><a href="#5-6-新序列采样" class="headerlink" title="5.6 新序列采样"></a>5.6 新序列采样</h3><p>在我们训练一个序列模型之后，要想了解这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。<br><img src="/images/深度学习/自然语言处理15.png" alt="自然语言处理15"><br>一个序列模型模拟了任意特定单词序列的概率，我们的工作就是对概率分布进行采样来生成一个新的单词序列。这个网络已经被上图所展示的结构训练过了。而为了进行采样，我们需要做的是：<br>1 对我们想要模型生成的第一个词进行采样。该时间步的输入为a^0=0和x^1=0。我们得到的是所有可能的输出经过softmax层后得到的概率。<br>2 根据这个softmax的分布进行随机采样。softmax分布给我们的信息就是第一个词是x的概率是p(x)，x是词典中的每一个词。然后对这个y_pre向量使用numpy.random.choice，来根据向量中这些概率的分布进行采样。也就是根据p(x)选择最终输出的y_pre。<br>3 进入下一个时间步，第二个时间步的输入为y^1,注意在用于序列生成时，y^1=y_pre^1,然后softmax层就会预测y_pre^2。也就是说我们现在计算在第一个词是y^1的情况下，第二个词应该是什么？<br>4 进入到下一个时间步，用one-hot码表示y_pre^2选择的结果，并把它继续传递给下一个时间步。<br>5 表示句子的结束。如果EOS标志在字典中，我们可以一直进行采样直到得到EOS标志。另一种情况是，如果EOS标志不在字典中，我可以设定在固定的时间步结束。如果我们不希望生成的序列中有未知标志，那么我们可以设定当结果为未知标志时从剩余部分继续采样直到结果不是未知标志。<br>这就是RNN语言模型生成一个随机选择的句子的过程。<br>在上文中我们介绍的是基于单词的RNN，实际上我们也可以使用基于字符的RNN。在这种情况下，字典中仅包含从a到z的字母，可能还会有空格符，可能还会有0-9，还可以有大写字母。此时我的序列y^1,y^2,y^3,…在训练集中都将是单独的字符，而不是一个单词。<br>基于字符的RNN的优点是不用担心会出现UNK标识。它的缺点是我们最后会得到太多、太长的序列，大多数英语句子只有10到20个单词，但是却可能包含很多字符。所以基于字符的语言模型在捕捉句子中的依赖关系，也就是句子较前部分如何影响较后部分，不如基于单词的RNN那样可以捕捉长范围的关系。并且它的计算成本较高。</p>
<h3 id="5-7-带有神经网络的梯度消失"><a href="#5-7-带有神经网络的梯度消失" class="headerlink" title="5.7 带有神经网络的梯度消失"></a>5.7 带有神经网络的梯度消失</h3><h4 id="5-7-1-梯度消失"><a href="#5-7-1-梯度消失" class="headerlink" title="5.7.1 梯度消失"></a>5.7.1 梯度消失</h4><p><img src="/images/深度学习/自然语言处理16.png" alt="自然语言处理16"><br>在上图中的句子，cat-was，cats-were，它们有非常长距离的依赖，最前面的单词对句子后面的单词有影响。但是我们此前见到的基本RNN模型，不擅长捕获这种长期以来。这是因为梯度消失，即一个很深的神经网络，在进行反向传播的时候，输出y的梯度很难传播回去，很难影响靠前层的权重。在RNN中，从左到右进行前向传播，从右到左进行反向传播，因为存在梯度消失问题，所以反向传播会很困难，后面层的输出误差很难影响前面层的计算。反映到这个问题上就是很难让一个神经网络记住看到的是单数名词还是负数名词，然后在序列后面生成对应的单复数形式。<br>RNN网络中会有很多局部影响。也就是输出y^3主要受y^3附近的值的影响，很难受到序列靠前的输入的影响。这是因为这个区域的误差很难反向传播到序列的前面部分，也因此网络很难调整序列前面位置的计算，这是RNN模型的主要问题所在，不擅长处理长期依赖问题。</p>
<h4 id="5-7-2-梯度爆炸"><a href="#5-7-2-梯度爆炸" class="headerlink" title="5.7.2 梯度爆炸"></a>5.7.2 梯度爆炸</h4><p>梯度下降在训练RNN网络时是首要的问题。但也可能出现梯度爆炸。我们在进行反向传播时，随着层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。这是因为指数极大的梯度会让我们的参数变得极其大，以至于网路参数崩溃。所以梯度爆炸很容易发现，会出现NAN的结果。<br>我们解决梯度爆炸的一个方法就是梯度修剪。观察我们的梯度向量，如果它大于某个阈值，就需要缩放梯度向量，保证它不会太大。</p>
<h3 id="5-8-GRU单元-门控循环单元"><a href="#5-8-GRU单元-门控循环单元" class="headerlink" title="5.8 GRU单元(门控循环单元)"></a>5.8 GRU单元(门控循环单元)</h3><p>GRU单元改变了RNN的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。<br><img src="/images/深度学习/自然语言处理17.png" alt="自然语言处理17"><br>上图中，我们在5.2部分已经得到了这个公式，用于计算RNN的激活值，时间步t的激活值a^t等于W_a乘上时间步t-1的激活值和输入x^t,再加上偏差项。<br>我们把RNN的单元画一个图，输入是时间步t-1的激活值a^(t-1)和x^t，然后把这两项加上权重项计算，如果g是一个tanh激活函数，则在经过tanh计算之后，RNN单元会输出激活值a^t，同时将a^t传递给softmax层产生输出y_pre^t。这就是RNN隐藏层的单元的可视化呈现，如上图。<br><img src="/images/深度学习/自然语言处理18.png" alt="自然语言处理18"><br>当我们从左到右读入这个句子，GRU单元有一个新的变量c代表细胞，即记忆细胞。记忆细胞的作用是提供记忆的能力，比如说cat是单数还是复数。</p>
<h4 id="5-8-1-初始化c-t"><a href="#5-8-1-初始化c-t" class="headerlink" title="5.8.1 初始化c^t"></a>5.8.1 初始化c^t</h4><p>在时间步t，c^t=a^t,即使这两个值相同，我们也需要使用两个符号来表示，因为在后面LSTM中，这两个符号代表的是不同的值。</p>
<h4 id="5-8-2-计算c-t候选值c-pre-t"><a href="#5-8-2-计算c-t候选值c-pre-t" class="headerlink" title="5.8.2 计算c^t候选值c_pre^t"></a>5.8.2 计算c^t候选值c_pre^t</h4><p>在每一个时间步，我们将会用一个候选值重写记忆细胞，即c_pre^t的值，它就是一个候选值，代替了c^t的值。然后我们用tanh激活函数来计算W_c,具体如下式：</p>
<script type="math/tex; mode=display">c_{pre}^t = tanh(W_c[c^{(t-1)},x^{(t)}]+b_c)</script><p>即上一个记忆细胞的值和目前的输入x^t。</p>
<h4 id="5-8-3-确定门"><a href="#5-8-3-确定门" class="headerlink" title="5.8.3 确定门"></a>5.8.3 确定门</h4><p>在GRU中真正重要的思想是，我们有一个门Γu，u代表更新门，Γu表示一个0到1之间的值，它的具体计算方式是</p>
<script type="math/tex; mode=display">Γ_u = \sigma(W_u[c^{(t-1)},x^{(t)}]+b_u)</script><p>回忆sigmoid的函数图像，它的输出值总是在0到1之间。对于大多数可能的输入，sigmoid函数的输出值总是非常接近0或者非常接近1，那么Γ的取值也是大多数情况下非常接近0或1。<br>我们用c_pre来确定更新c的候选值，然后用门来决定是否要真的用这个候选值来更新c。<br>在这里，我们可以这样理解。记忆细胞c将被设定为0或1，这取决于我们考虑的单词在句子中是单数还是复数。在这里cat是单数情况，所以我们先假定c被设为了1，那么复数情况就会被设为0。然后GRU单元将会一直记住c^t的值，直到was的位置，因为记忆细胞告诉网络这里是单数，所以我们用was。<br>门的作用是决定我们什么时候更新c^t的值。例如当我们看到cat，这就是一个更新这个bit的好时机。然后当我们使用这个bit的时候，我们就可以更新bit的内容了，因为它已经发挥过作用了。</p>
<h4 id="5-8-4-更新c-t"><a href="#5-8-4-更新c-t" class="headerlink" title="5.8.4 更新c^t"></a>5.8.4 更新c^t</h4><p>c^t的更新方法如下：</p>
<script type="math/tex; mode=display">c^t = Γ_u\times c_{pre}^{(t)}+(1-Γ_u)\times c^{(t-1)}</script><p>即如果Γu=1，那么就是把c^t更新为候选值c_pre^t。在这个例子中，cat位置的Γu=1，证明应该更新c^t。在cat和was之间的位置，Γu=0，意识是不要去更新c^t的值，将当前时间步的c的值设置为上一时间步c的值。于是网络就会一直记得cat这里是单数。</p>
<h4 id="5-8-5-过程绘图"><a href="#5-8-5-过程绘图" class="headerlink" title="5.8.5 过程绘图"></a>5.8.5 过程绘图</h4><p>GRU单元输入c^(t-1)，我们可以先假设c^(t-1)=a^(t-1)，表示t-1时间步记忆细胞的激活值就是它要记忆的值。<br>GRU单元输入x^t。<br>把这两个输入用合适权重结合在一起，再用tanh计算出c_pre^t,也就是c^t的候选值。<br>两个输入以合适的权重通过sigmoid激活函数，计算Γ_u，即更新门。<br>通过Γ_u的值决定是否要用候选值来更新c^t。最终GRU单元输出a^t=c^t。表示记忆细胞的激活值就是它要记忆的值。同时a^t输入到softmax层可以用于预测y_pre^t的取值。<br>GRU的优点是通过门决定当我们从左到右扫描一个句子的时候，某一时刻是否需要更新记忆细胞。Γ_u很容易就能取到0，那么我们的更细就是c^t=c^(t-1),这非常有利于维持细胞的值。因为Γu很接近0，也避免了梯度消失的问题(就是y_pre的值只受到附近的值的影响，而没有办法解决长距离依赖问题)。即使经过很多的时间步，c^t的内容也会很好的维持。<br>c^t可以是一个向量。如果我们有100维的隐藏项的激活值，那么c^t也是100维的，c_pre^t也是相同的维度，Γu也是相同的维度。这样更新式中的×表示的是元素对应的乘积。如果Γu是一个100维的向量，那么里面的值几乎都是0或1，但在实际中可能也会有0到1之间的值。这个100维的记忆细胞就是我们每次更新的目标。门的功能就是告诉GRU单元哪个记忆细胞的向量维度在当前时间步需要更新。所以我们可以选择保持一些比特不变，而去更新其他的比特。<br>例如我们可能需要一个比特来记忆cat是单数还是复数，其他比特来理解另外的内容。</p>
<h4 id="5-8-6-完整的GRU单元"><a href="#5-8-6-完整的GRU单元" class="headerlink" title="5.8.6 完整的GRU单元"></a>5.8.6 完整的GRU单元</h4><p>在上图中我们展示的只是经过简化的GRU单元，下面我们来讨论完整的GRU单元。<br><img src="/images/深度学习/自然语言处理19.png" alt="自然语言处理19"><br>在我们计算的第一个式子中，给记忆细胞计算候选值，我们需要在c^(t-1)的前面增加一个门Γr，r代表相关性，Γr表示下一个c^t的候选值c_pre^t与c^(t-1)的相关性大小。Γr的计算方式如下：</p>
<script type="math/tex; mode=display">Γ_r = \sigma(W_r[c^{(t-1)},x^{(t)}]+b_r)</script><p>另一种捕捉长范围依赖，解决梯度消失的解决方案是LSTM。</p>
<h3 id="5-9-长短期记忆-LSTM"><a href="#5-9-长短期记忆-LSTM" class="headerlink" title="5.9 长短期记忆 LSTM"></a>5.9 长短期记忆 LSTM</h3><p><img src="/images/深度学习/自然语言处理20.png" alt="自然语言处理20"><br>上图中的左边部分是我们在5.8GRU部分得到的式子，在GRU中，我们有a^t=c^t,还有两个门，更新门和相关门。c_pre^t是记忆细胞的候选值，然后我们使用更新门Γu来决定是否要用候选值c_pre^t来更新c^t。</p>
<h4 id="5-9-1-c-pre-t计算"><a href="#5-9-1-c-pre-t计算" class="headerlink" title="5.9.1 c_pre^t计算"></a>5.9.1 c_pre^t计算</h4><p>LSTM是比GRU更强大和通用的版本，我们先了解记忆细胞。c_pre^t的计算方式如下：</p>
<script type="math/tex; mode=display">c_{pre}^t = tanh(W_c[a^{(t-1)},x^{(t)}]+b_c)</script><p>我们不再有a^t=c^t的情况，我们计算c_pre^t使用的是a^(t-1),而不是c^(t-1),我们也不用Γr。</p>
<h4 id="5-9-2-更新门Γu和遗忘门Γf"><a href="#5-9-2-更新门Γu和遗忘门Γf" class="headerlink" title="5.9.2 更新门Γu和遗忘门Γf"></a>5.9.2 更新门Γu和遗忘门Γf</h4><p>像以前那样我们有一个更新门Γu，具体计算方式如下：</p>
<script type="math/tex; mode=display">Γ_u = \sigma(W_u[a^{(t-1)},x^{(t)}]+b_u)</script><p>LSTM的一个特性就是不只有一个更新门控制c^t的更新(具体来说是c^t在更新时候选值的权值和c^(t-1)的权值)，即我们使用不同的门来取代左边式子中的Γu和1-Γu。我们将更新门作为候选值的权值，而c^(t-1)的权值为遗忘门Γf的输出，它的计算方式为：</p>
<script type="math/tex; mode=display">Γ_u = \sigma(W_f[a^{(t-1)},x^{(t)}]+b_f)</script><p>则c^t的计算方式为：</p>
<script type="math/tex; mode=display">c^t = Γ_u\times c_{pre}^{(t)} + Γ_f\times c^{(t-1)}</script><p>如果是矩阵，则×表示对应位置的元素乘积。这给了记忆细胞选择权去维持旧的值c^(t-1)或者就加上新的值c_pre^t。所以这里用了单独的更新门和遗忘门。</p>
<h4 id="5-9-3-输出门Γo"><a href="#5-9-3-输出门Γo" class="headerlink" title="5.9.3 输出门Γo"></a>5.9.3 输出门Γo</h4><p>然后我们还有输出门Γo：</p>
<script type="math/tex; mode=display">Γ_o = \sigma(W_o[a^{(t-1)},x^{(t)}]+b_o)</script><p>因此该时间步的激活值a^t的计算方式为</p>
<script type="math/tex; mode=display">a^t = Γ_o\times c^{(t)}</script><h4 id="5-9-4-过程绘图"><a href="#5-9-4-过程绘图" class="headerlink" title="5.9.4 过程绘图"></a>5.9.4 过程绘图</h4><p><img src="/images/深度学习/自然语言处理21.png" alt="自然语言处理21"><br>上图中，我们用a^(t-1),x^t计算了所有门值，从左到右依次是遗忘门、更新门和输出门。<br>同时a^(t-1),x^t也经过了tanh来计算y_pre^t。这些门值和c^(t-1)、c_pre^t一起决定了c^t。<br>然后我们将这些单元连接起来，我们发现红线位置只要我们正确地设置了遗忘门和更新门，LSTM很容易把c_pre^0的值一直向下一个时间步传递直到最右边，也就是c^3=c^0,这表明它非常擅长于长时间记忆某个值。最常用的LSTM版本门值不仅取决于a^(t-1)和x^t，有时也会参考一下c^(t-1)的值，这叫做窥视孔连接。<br>LSTM的主要区别在于这个技术细节：比如c是100维的记忆细胞单元，第50个c^(t-1)的元素，只会影响第50个元素对应的那三个门，所以对应关系是一对一的，c^(t-1)不可能影响所有的门元素。<br>GRU的优点是它是一个更加简单的模型，所以更容易创建一个更大的网络。而且它只有两个门，在计算性能上，也运行得更快。使用GRU我们可以扩大模型的规模。<br>LSTM比GRU更为强大和灵活，因为它有3个门。</p>
<h3 id="5-10-双向神经网络"><a href="#5-10-双向神经网络" class="headerlink" title="5.10 双向神经网络"></a>5.10 双向神经网络</h3><p>双向RNN模型可以让我们在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息。<br>深层RNN模型也可以帮助我们解决更复杂的问题。<br><img src="/images/深度学习/自然语言处理22.png" alt="自然语言处理22"><br>在上图中，我们判断Teddy是不是人名的一部分时，光看句子的前面部分是不够的。为了判断y_pre^3是0还是1，除了前3个单词，我们还需要更多信息。因为根据前3个单词无法判断这句话在谈论的是Teddy熊还是前美国总统。<br>上图中的网络是一个单向的RNN网络，这些单元可以是标准的RNN块、GRU单元、LSTM单元。单向网络的特点是都是前向的。那么一个双向的RNN，是如何工作的呢？<br><img src="/images/深度学习/自然语言处理23.png" alt="自然语言处理23"><br>上图展示的双向RNN的工作原理。我们的输入是一个只有四个单词的句子。首先这个网络有一个前向(从左到右)的循环单元，它们都有一个当前输入x，输出预测y_pre^t。<br>然后我们增加一个反向循环层(从右到左)，同样有一个当前输入x，输出预测。<br>这样网络就构成了一个无环图，给定一个输入序列x^1,…x^4,网络进行前向传播。这个序列首先计算前向a^1,a^2,…,a^4。而反向序列从计算a^4开始，反向进行，计算a^3直到a^1,这里也属于是神经网络的前向传播部分。<br>在计算完正向和反向的激活值之后，网络就可以计算预测结果了。y_pre的计算方式如下：</p>
<script type="math/tex; mode=display">y_{pre}^t = g(W_y[\mathop{a}\limits^{\rightarrow t},\mathop{a}\limits^{\leftarrow t}] + b_y)</script><p>此时如果要预测y_pre^3,则在正向循环中，x^1,x^2,x^3的信息都会考虑在内，在反向循环中，x^4的信息也会考虑在内。<br>双向RNN网络的缺点是我们需要完整的数据序列才能完成预测。</p>
<h3 id="5-11-深层循环神经网络"><a href="#5-11-深层循环神经网络" class="headerlink" title="5.11 深层循环神经网络"></a>5.11 深层循环神经网络</h3><p><img src="/images/深度学习/自然语言处理24.png" alt="自然语言处理24"><br>在深层RNN网络中，我们在每一个时间步中堆叠多个隐藏层。如上图，我们对激活值a的索引上增加了所在层的信息。对于a(2,3)单元来说，他有两个输入，一个是从下面过来的输入，还有一个从左边过来的输入。计算a(2,3)的时候用激活函数作用于权重矩阵，如下：</p>
<script type="math/tex; mode=display">a^{[2](3)} = g(W_a^{[2]}[a^{[2](2)},a^{[1](3)}]+b_a^{[2]})</script><p>Wa^2和ba^2在第二层的计算中都是相同的值。对于RNN网络来说，深度为3的网络就相当大了。但是我们可以将第三层后的输出去掉，直接在后面接上一个标准神经网络用于预测y_pre,而不再进行水平连接。</p>
<h2 id="6-textRNN实现"><a href="#6-textRNN实现" class="headerlink" title="6 textRNN实现"></a>6 textRNN实现</h2><p><a href="https://wmathor.com/index.php/archives/1446/" target="_blank" rel="noopener">参考博客1</a><br><a href="https://wmathor.com/index.php/archives/1392/" target="_blank" rel="noopener">参考博客2</a><br><a href="https://www.bilibili.com/video/BV1iK4y147ff/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">参考视频</a><br>问题背景：参考论文 Finding Structure in Time。我们有n句话，每句话都由且仅由3个单词组成。我们的工作是将前两个单词作为输入，最后一个单词作为输出，训练一个RNN模型。</p>
<h3 id="6-1-导库"><a href="#6-1-导库" class="headerlink" title="6.1 导库"></a>6.1 导库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.FloatTensor()默认生成32位浮点数，dtype 为 torch.float32 或 torch.float</span></span><br><span class="line">dtype=torch.FloatTensor</span><br></pre></td></tr></table></figure>
<h3 id="6-2-准备数据集"><a href="#6-2-准备数据集" class="headerlink" title="6.2 准备数据集"></a>6.2 准备数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">sentences = [<span class="string">"i like dog"</span>,<span class="string">"i love coffee"</span>,<span class="string">"i hate milk"</span>]</span><br><span class="line"><span class="comment"># 将数据集中的句子合并成字符串，不同的句子间以空格分开；然后以空格分开这一个字符串</span></span><br><span class="line"><span class="comment"># word_list中的内容 ['i', 'like', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk']</span></span><br><span class="line">word_list = <span class="string">" "</span>.join(sentences).split()</span><br><span class="line"><span class="comment"># set() 函数创建一个无序不重复元素集 &#123;'love', 'like', 'coffee', 'dog', 'hate', 'i', 'milk'&#125;</span></span><br><span class="line">vocab = list(set(word_list))</span><br><span class="line"><span class="comment"># word to index 把每个词映射到它的索引上</span></span><br><span class="line"><span class="comment"># enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</span></span><br><span class="line">word2idx = &#123;w:i <span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">idx2word = &#123;i:w <span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line"><span class="comment"># 所有可以用于预测的词的总数</span></span><br><span class="line">n_class = len(vocab)</span><br></pre></td></tr></table></figure>
<h3 id="6-3-预处理数据"><a href="#6-3-预处理数据" class="headerlink" title="6.3 预处理数据"></a>6.3 预处理数据</h3><p>n_step参数解释：<br><img src="/images/深度学习/textRNN实现1.png" alt="textRNN实现1"><br>TextRNN参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预处理数据，构建Dataset，定义DataLoader，输入数据one-hot编码</span></span><br><span class="line"><span class="comment"># TextRNN 参数</span></span><br><span class="line"><span class="comment"># batch_size 每次训练的样本数</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 一个样本数据中输入的单词数,一个样本数据共有3个单词，前2个作为输入，所以n_step=2</span></span><br><span class="line"><span class="comment"># 输入数据的单词数也是RNN网络中时间步的数目，也就是神经元单元的数目</span></span><br><span class="line">n_step = <span class="number">2</span></span><br><span class="line"><span class="comment"># 使用one-hot编码的词输入网络后我们用多少维的向量去编码，隐藏层神经元个数</span></span><br><span class="line">n_hidden = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>划分输入数据和标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_data</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    input_batch = []</span><br><span class="line">    target_batch = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="comment"># 将一个句子的单词提取出来</span></span><br><span class="line">        word = sen.split()</span><br><span class="line">        <span class="comment"># 输入数据的索引是0-n-1</span></span><br><span class="line">        input = [word2idx[n] <span class="keyword">for</span> n <span class="keyword">in</span> word[:<span class="number">-1</span>]]</span><br><span class="line">        <span class="comment"># 输入数据的标签是n</span></span><br><span class="line">        target = word2idx[word[<span class="number">-1</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># np.eye实现one-hot编码 构建n_class*n_class维的单位阵，然后取出input中数值所对应的行，例如input=[1,2]，那么就会返回E矩阵的第一行和第二行(下标从0开始)。</span></span><br><span class="line">        input_batch.append(np.eye(n_class)[input])</span><br><span class="line">        <span class="comment"># target不用one-hot编码</span></span><br><span class="line">        target_batch.append(target)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> input_batch,target_batch</span><br></pre></td></tr></table></figure>
<p>数据集封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">input_batch,target_batch = make_data(sentences)</span><br><span class="line"><span class="comment"># 与pytorch框架一致，使用tensor数据类型 torch.Tensor默认生成 torch.FloatTensor型 torch.LongTensor(2,3) #构建一个2*3 Long类型的张量</span></span><br><span class="line">input_batch,target_batch = torch.Tensor(input_batch),torch.LongTensor(target_batch)</span><br><span class="line"><span class="comment"># TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等。</span></span><br><span class="line">dataset = Data.TensorDataset(input_batch,target_batch)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">dataloader的作用是从dataset中取数据并加载到神经网络中</span></span><br><span class="line"><span class="string">dataset参数：我们要从哪一个数据集中取数据。</span></span><br><span class="line"><span class="string">batch_size参数：每次下载几个数组，默认为1。</span></span><br><span class="line"><span class="string">shuffle参数：在每次取数据的时候是否打断。为true则表示两次load数据的顺序是不一样的。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">loader = Data.DataLoader(dataset,batch_size,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [3, 2, 7],3句话，每句话2个词，每个词onehot编码为7*1的向量</span></span><br><span class="line">print(input_batch.shape)</span><br></pre></td></tr></table></figure>
<h3 id="6-4-构建网络结构"><a href="#6-4-构建网络结构" class="headerlink" title="6.4 构建网络结构"></a>6.4 构建网络结构</h3><p>out和ht在RNN网络中的对应关系：<br><img src="/images/深度学习/textRNN实现2.png" alt="textRNN实现2"><br><a href="https://zhuanlan.zhihu.com/p/159477597" target="_blank" rel="noopener">nn.CrossEntropyLoss()讲解</a>交叉熵损失就是把log_softmax和nll_loss结合起来。<br>Adam方法参数：<br>1 params (iterable) – 待优化参数的iterable或者是定义了参数组的dict。<br>2 lr (float, 可选) – 学习率（默认：1e-3）。<br>3 betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）。该超参数在稀疏梯度（如在 NLP 或计算机视觉任务中）中应该设置为接近 1 的数。<br>4 eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）。其为了防止在实现中除以零。<br>5 weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建网络结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(TextRNN,self).__init__()</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        input_size指的是每个单词我们用多少维的向量去编码</span></span><br><span class="line"><span class="string">        hidden_size指的是输出维度，输入RNN网络的是7维one-hot编码，网络输出的是hidden_size维度的向量</span></span><br><span class="line"><span class="string">        num_layers，指的是纵向的隐藏层个数，一般设置为 1~10，default=1</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.rnn = nn.RNN(input_size=n_class,hidden_size=n_hidden)</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        fully connected layer</span></span><br><span class="line"><span class="string">        将模型的输出维度从n_hidden映射到n_class </span></span><br><span class="line"><span class="string">        in_features:输入维度</span></span><br><span class="line"><span class="string">        out_features:输出维度</span></span><br><span class="line"><span class="string">        bias:true or false，是否增加偏置</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.fc = nn.Linear(n_hidden,n_class)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forword</span><span class="params">(self,hidden,X)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        out,ht = forward(x,h0)</span></span><br><span class="line"><span class="string">        ht表示的是上一时间步传递给下一时间步的信息,ht和h0的维度是[num_layers(纵向隐藏层个数),batch_size,hidden_size(网络输出的向量维度)]。ht指的是时间步t时全部num_layers层的记忆细胞的状态。</span></span><br><span class="line"><span class="string">        out.shape = [序列长度,batch_size,n_hidden](和ht的不同在于维度0) out指的是RNN输出层的输出，第0个维度是序列长度。它记录的是全部时间步上的直接决定输出层的记忆细胞的状态，即如果有2层记忆细胞，那么就是接近输出层的那一层记忆细胞的状态。</span></span><br><span class="line"><span class="string">        x是每一时间步的输入，pytorch的forward函数需要的x.shape = [序列长度(这里是2个),batch_size,word vector(即每一个单词向量的维度)]</span></span><br><span class="line"><span class="string">        实际传入的x.shape = [batch_size,n_step(也就是序列长度),n_class(也就是单词向量的维度)]</span></span><br><span class="line"><span class="string">        因此调用transpose方法交换0和1位置上的维度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        X = X.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        out,hidden = self.rnn(X,hidden)</span><br><span class="line">        <span class="comment"># 我们需要的输出为最后第一个时间步对应的输出 即out[-1]</span></span><br><span class="line">        out = out[<span class="number">-1</span>]</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        out.shape = [序列长度=2，batch_size=2,n_hidden=5]→out.shape = [batch_size=2,n_hidden=5]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># out 通过全连接层，进行一个维度变换</span></span><br><span class="line">        model = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> model</span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = TextRNN()</span><br><span class="line"><span class="comment"># 指定损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 指定优化函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;class 'torch.optim.adam.Adam'&gt;</span></span><br><span class="line">print(type(optimizer))</span><br></pre></td></tr></table></figure>
<h3 id="6-5-模型训练"><a href="#6-5-模型训练" class="headerlink" title="6.5 模型训练"></a>6.5 模型训练</h3><p>Epoch，Batch Size和Iteration的区别：<br>举个例子：将10kg的面粉使用面条加工机(每次只能处理2kg)，加工成10kg的面条。首先得把10kg面粉分成5份2kg的面粉，然后放入机器加工，经过5次，可以将这10kg面粉首次加工成面条，但是现在的面条肯定不好吃，因为不劲道，于是把10kg面条又放进机器再加工一遍，还是每次只处理2kg，处理5次，现在感觉还行，但是不够完美；于是又重复了一遍：将10kg上次加工好的面条又放进机器，每次2kg，加工5次，最终成型了，完美了，结束了。那么到底重复加工几次呢？只有有经验的师傅才知道。这就形象地说明：Epoch就是10斤面粉被加工的次数（上面的3次）；Batch Size就是每份的数量（上面的2kg），Iteration就是将10kg面粉加工完一次所使用的循环次数（上面的5次）。显然 1个epoch  = BatchSize * Iteration。<br>hidden就是伪激活值h0(也就是吴恩达理论课程中的a^0)，全0向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="comment"># hidden.shape = out.shape = [纵向隐藏层个数,batch_size,n_hidden]</span></span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>,x.shape[<span class="number">0</span>],n_hidden)</span><br><span class="line">        <span class="comment"># x.shape = [batch_size,n_step(也就是序列长度),n_class(也就是单词向量的维度)]</span></span><br><span class="line">        pred = model(hidden,x)</span><br><span class="line">        <span class="comment"># pred.shape=res_out.shape = [batch_size=2,n_class=3]</span></span><br><span class="line">        <span class="comment"># y.shape = [batch_size] 非one-hot编码，LongTensor类型</span></span><br><span class="line">        loss = criterion(pred,y)</span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>)%<span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>,<span class="string">'%04d'</span>%(epoch+<span class="number">1</span>),<span class="string">'cost = '</span>,<span class="string">'&#123;:.6f&#125;'</span>.format(loss))</span><br><span class="line">        <span class="comment"># zero_grad()方法是将上一步的梯度值清零，以防止在本步中的调整受到上一步梯度值的影响</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># TORCH.OPTIM部分，根据backward对模型进行优化。使用方法，在backward方法之后调用step方法。</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<p>输出为：<br>Epoch: 1000 cost =  0.056945<br>Epoch: 1000 cost =  0.066035<br>Epoch: 2000 cost =  0.010118<br>Epoch: 2000 cost =  0.010668<br>Epoch: 3000 cost =  0.002922<br>Epoch: 3000 cost =  0.003213<br>Epoch: 4000 cost =  0.000966<br>Epoch: 4000 cost =  0.001142<br>Epoch: 5000 cost =  0.000349<br>Epoch: 5000 cost =  0.000402</p>
<p>测试代码：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># 取出每一句话的前两个单词作为输入,[0,2)</span></span><br><span class="line">input = [sen.split()[:<span class="number">2</span>] <span class="keyword">for</span> sen <span class="keyword">in</span> sentences]</span><br><span class="line">hidden = torch.zeros(<span class="number">1</span>,len(input),n_hidden)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">dim=0表示计算每列的最大值，dim=1表示每行的最大值</span></span><br><span class="line"><span class="string">keepdim 表示是否需要保持输出的维度与输入一样，keepdim=True表示输出和输入的维度一样，keepdim=False表示输出的维度被压缩了，也就是输出会比输入低一个维度，变成了向量。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">predict = model(hidden,input_batch).max(<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch.return_types.max(</span></span><br><span class="line"><span class="string">values=tensor([[7.3859],</span></span><br><span class="line"><span class="string">        [6.7870],</span></span><br><span class="line"><span class="string">        [6.8292]]),</span></span><br><span class="line"><span class="string">indices=tensor([[5],</span></span><br><span class="line"><span class="string">        [6],</span></span><br><span class="line"><span class="string">        [4]]))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># print(predict)</span></span><br><span class="line"><span class="comment"># 因此我们需要的predict存索引</span></span><br><span class="line">predict = predict[<span class="number">1</span>]</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch.squeeze()函数主要对数据的维度进行压缩，去掉维数为1的的维度 predict.shape: 3*1 → 3</span></span><br><span class="line"><span class="string">a = torch.tensor(5)</span></span><br><span class="line"><span class="string">print(a) 输出tensor(5)</span></span><br><span class="line"><span class="string">print(a.item()) 输出 5</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">print([sen.split()[:<span class="number">2</span>] <span class="keyword">for</span> sen <span class="keyword">in</span> sentences], <span class="string">'-&gt;'</span>, [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">[['i', 'like'], ['i', 'love'], ['i', 'hate']] -&gt; ['dog', 'coffee', 'milk']</span><br></pre></td></tr></table></figure>
<h2 id="7-TextCNN实现"><a href="#7-TextCNN实现" class="headerlink" title="7 TextCNN实现"></a>7 TextCNN实现</h2><p>RNN网络有一个问题就是没有办法并行，因为时间步之间还会有水平信息传递，而CNN网络的优势在于没有时间维度的限制。<br><img src="/images/深度学习/TextCNN实现1.png" alt="TextCNN实现1"><br>如上图，一个含有9个词的句子，被转换成9*6的矩阵，即每个词用6维的向量编码。<br>第一个卷积核位于图中红框处，为6*2,它的宽度设置为2的好处是同时覆盖了wait和for两个词的向量。如果步长stride(偏移量，即一个大方块和一个小方块卷积的时候，小方块每次移动的量)为1的话，那么下一次卷积核覆盖的是for和the两个词的向量。在文本处理中，同时考虑一个词的上下文是很重要的一点。由上图可以看出，这里使用了四个不同尺寸的卷积核。<br><a href="https://wmathor.com/index.php/archives/1353/" target="_blank" rel="noopener">参考原理</a><br><a href="https://wmathor.com/index.php/archives/1445/" target="_blank" rel="noopener">参考实现</a><br><a href="https://www.bilibili.com/video/BV1ip4y1U735/?vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">参考视频</a><br>我们的代码是实现对一个句子的情感进行二分类。</p>
<h3 id="7-1-导库"><a href="#7-1-导库" class="headerlink" title="7.1 导库"></a>7.1 导库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="7-2-数据集处理和参数设定"><a href="#7-2-数据集处理和参数设定" class="headerlink" title="7.2 数据集处理和参数设定"></a>7.2 数据集处理和参数设定</h3><p>定义数据集:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line">sentences = [<span class="string">'i love you'</span>,<span class="string">'he loves me'</span>,<span class="string">'she likes baseball'</span>,<span class="string">'i hate you'</span>,<span class="string">'sorry for that'</span>,<span class="string">'this is awful'</span>]</span><br><span class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理数据集</span></span><br><span class="line">word_list = <span class="string">" "</span>.join(sentences).split()</span><br><span class="line">vocab = list(set(word_list))</span><br><span class="line">word2idx = &#123;w:i <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">vocab_size = len(vocab)</span><br></pre></td></tr></table></figure>
<p>textCNN参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TextCNN参数</span></span><br><span class="line"><span class="comment"># 每个单词所用的编码向量的维度</span></span><br><span class="line">embedding_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># 每一句话中的单词数=3</span></span><br><span class="line">sequence_length = len(sentences[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 要划分的类别数</span></span><br><span class="line">num_classes = len(set(labels))</span><br><span class="line">batch_size = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>封装数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建dataset，定义dataloader</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_data</span><span class="params">(sentences,labels)</span>:</span></span><br><span class="line">    inputs = []</span><br><span class="line">    targets = []</span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="comment"># 把句子中的单词映射成下标，其中split函数返回sen中单词的list</span></span><br><span class="line">        inputs.append([word2idx[n] <span class="keyword">for</span> n <span class="keyword">in</span> sen.split()])</span><br><span class="line">    <span class="keyword">for</span> lab <span class="keyword">in</span> labels:</span><br><span class="line">        targets.append(lab)</span><br><span class="line">    <span class="keyword">return</span> inputs,targets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装数据集</span></span><br><span class="line">input_batch,target_batch = make_data(sentences,labels)</span><br><span class="line">input_batch,target_batch = torch.Tensor(input_batch),torch.LongTensor(target_batch)</span><br><span class="line">dataset = Data.TensorDataset(input_batch,target_batch)</span><br><span class="line">loader = Data.Dataloader(dataset,batch_size,<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="7-3-构建模型"><a href="#7-3-构建模型" class="headerlink" title="7.3 构建模型"></a>7.3 构建模型</h3><p>embedding编码如下图所示：<br><img src="/images/深度学习/TextCNN实现2.png" alt="TextCNN实现2"><br>最终黄色边为样本数据的数目，紫色是一个样本数据中的单词个数，绿色是一个单词被编码的长度。<br>下面详细介绍一下数据在网络中流动的过程中维度的变化。输入数据是个矩阵，矩阵维度为 [batch_size, seqence_length]，输入矩阵的数字代表的是某个词在整个词库中的索引（下标）。</p>
<p>首先通过 Embedding 层，也就是查表，将每个索引转为一个向量，比方说 12 可能会变成 [0.3,0.6,0.12,…]，因此整个数据无形中就增加了一个维度，变成了 [batch_size, sequence_length, embedding_size]。<br>之后使用 unsqueeze(1) 函数使数据增加一个维度，变成 [batch_size, 1, sequence_length, embedding_size]。现在的数据才能做卷积，因为在传统 CNN 中，输入数据就应该是 [batch_size, in_channel, height, width] 这种维度。<br>[batch_size, 1, 3, 2] 的输入数据通过 nn.Conv2d(1, 3, (2, 2)) 的卷积之后，得到的就是 [batch_size, 3, 2, 1] 的数据，由于经过 ReLU 激活函数是不改变维度的，所以就没画出来。最后经过一个 nn.MaxPool2d((2, 1)) 池化，得到的数据维度就是 [batch_size, 3, 1, 1]。图示过程如下：<br><img src="/images/深度学习/TextCNN实现3.png" alt="TextCNN实现3"><br>需要注意经过卷积后的尺寸计算公式：<br><img src="/images/深度学习/TextCNN实现4.png" alt="TextCNN实现4"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(TextCNN,self).__init__()</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Embedding是用一个低维稠密向量来表示一个对象，使得这个向量能够表达相应对象的某些特征，同时向量之间的距离能反应对象之间的相似性。</span></span><br><span class="line"><span class="string">        它相对于one-hot的优势在于维度较低</span></span><br><span class="line"><span class="string">        第一个参数是词典中词的数量=16</span></span><br><span class="line"><span class="string">        第二个参数是编码一个词所用的向量维度=2。onehot就是vocab_size。</span></span><br><span class="line"><span class="string">        embedding的过程就是提供一个vocab_size*embedding_size的向量表，然后编码的过程就是根据词汇的index值查找表中第index个行向量，即为编码结果</span></span><br><span class="line"><span class="string">        这样一个index被编码为一个向量，增加了输入数据的维度 X.shape = [batch_size,sequence_length,embedding_size]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.W = nn.Embedding(vocab_size,embedding_size)</span><br><span class="line">        <span class="comment"># 卷积后的输出结果为3层，有3个卷积核参与卷积 可以自行设定</span></span><br><span class="line">        output_channel = <span class="number">3</span></span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            <span class="comment">#conv输入</span></span><br><span class="line">            <span class="comment">#input_channel为输入的通道数=1</span></span><br><span class="line">            <span class="comment">#output_channel为输出的通道数=3</span></span><br><span class="line">            <span class="comment">#kernel_size为卷积核的大小,(filter_height=2(即一次看两句话),filter_width=embedding_size=2(一次看一个编码维度,即一个单词))</span></span><br><span class="line">            <span class="comment">#stridestride为步径的大小=1</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>,output_channel,(<span class="number">2</span>,embedding_size)),</span><br><span class="line">            <span class="comment">#ReLU,激活函数的一种，功能是当input大于0的时候不改变，当小于0的时候将输入变为0。</span></span><br><span class="line">            <span class="comment">#该函数的作用是去除卷积结果中的负值，即保留特征比较好的值，将特征小于0的值舍去。</span></span><br><span class="line">            <span class="comment">#shape不变，只是改变数值</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            <span class="comment">#池化也称为欠采样或下采样。主要用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。</span></span><br><span class="line">            <span class="comment">#参数为池化核大小，(filter_height=2,filter_width=1)</span></span><br><span class="line">            nn.MaxPool2d((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># flatten.shape = [batch_size,output_channel,1,1],所以输入就是output_channel,输出是分类的类别数</span></span><br><span class="line">        self.fc = nn.Linear(output_channel,num_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        X.shape = [batch_size,sequence_length]</span></span><br><span class="line"><span class="string">        动态获取batch_size的好处是测试的时候如果数据少于batch_size也可以运行</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># embedding_X.shape = [batch_size=3,sequence_length=3,embedding_size=2]</span></span><br><span class="line">        embedding_X = self.W(X)</span><br><span class="line">        <span class="comment"># embedding_X.shape = [batch_size=3,channel=1,sequence_length=3,embedding_size=2]</span></span><br><span class="line">        <span class="comment"># 在embedding_X.shape[1]的位置添加一个channel维度=1</span></span><br><span class="line">        embedding_X = embedding_X.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># conved.shape = [batch_size,output_channel,1,1]</span></span><br><span class="line">        conved = self.conv(embedding_X)</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        如果是torch.view(参数a，-1)，则表示在参数b未知，参数a已知的情况下自动补齐列向量长度,注意会变成一个向量</span></span><br><span class="line"><span class="string">        flatten.shape = [batch_size,output_channel,1,1]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        flatten = conved.view(batch_size,<span class="number">-1</span>)</span><br><span class="line">        output = self.fc(flatten)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="7-4-模型训练"><a href="#7-4-模型训练" class="headerlink" title="7.4 模型训练"></a>7.4 模型训练</h3><p>模型训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">model = TextCNN().to(device)</span><br><span class="line"><span class="comment"># 分类问题的损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss().to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_x,batch_y <span class="keyword">in</span> loader:</span><br><span class="line">        batch_x,batch_y = batch_x.to(device),batch_y.to(device)</span><br><span class="line">        pred = model(batch_x)</span><br><span class="line">        loss = criterion(pred,batch_y)</span><br><span class="line">        <span class="keyword">if</span>(epoch+<span class="number">1</span>)%<span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch:'</span>,<span class="string">'%04d'</span>%(epoch+<span class="number">1</span>),<span class="string">'loss = '</span>,<span class="string">'&#123;:.6f&#125;'</span>.format(loss))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line">Epoch: <span class="number">1000</span> loss =  <span class="number">0.027163</span></span><br><span class="line">Epoch: <span class="number">1000</span> loss =  <span class="number">0.050510</span></span><br><span class="line">Epoch: <span class="number">2000</span> loss =  <span class="number">0.007027</span></span><br><span class="line">Epoch: <span class="number">2000</span> loss =  <span class="number">0.013381</span></span><br><span class="line">Epoch: <span class="number">3000</span> loss =  <span class="number">0.004606</span></span><br><span class="line">Epoch: <span class="number">3000</span> loss =  <span class="number">0.002388</span></span><br><span class="line">Epoch: <span class="number">4000</span> loss =  <span class="number">0.001722</span></span><br><span class="line">Epoch: <span class="number">4000</span> loss =  <span class="number">0.000885</span></span><br><span class="line">Epoch: <span class="number">5000</span> loss =  <span class="number">0.000340</span></span><br><span class="line">Epoch: <span class="number">5000</span> loss =  <span class="number">0.000662</span></span><br></pre></td></tr></table></figure>
<p>model.train()的意思并不是把模型设置为训练模式才能开始训练。只对一些特定的层有用，<br>并不一定需要写。<br>model.eval()的意思并不是把模型设置为eval状态才能开始测试。只对一些特定的层有用，<br>并不一定需要写。<br>模型测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型测试</span></span><br><span class="line">test_text = <span class="string">'i hate me'</span></span><br><span class="line"><span class="comment"># 映射成索引 [1, 9, 2]不可以，Expected 4-dimensional input for 4-dimensional weight [3, 1, 2, 2], but got 3-dimensional input of size [3, 1, 2] instead</span></span><br><span class="line"><span class="comment"># 所以需要二维列表</span></span><br><span class="line">tests = [[word2idx[n] <span class="keyword">for</span> n <span class="keyword">in</span> test_text.split()]]</span><br><span class="line"><span class="comment"># 封装成LongTensor类型</span></span><br><span class="line">test_batch = torch.LongTensor(tests).to(device)</span><br><span class="line"><span class="comment"># 模型切换到测试模式</span></span><br><span class="line">model = model.eval()</span><br><span class="line"><span class="comment"># 得到结果为两个1*2的矩阵(index0是values，index1是index)，然后按行取最大值</span></span><br><span class="line"><span class="comment"># 注意看是二维矩阵，两个中括号</span></span><br><span class="line"><span class="comment"># torch.return_types.max(</span></span><br><span class="line"><span class="comment"># values=tensor([[4.3949]], grad_fn=&lt;MaxBackward0&gt;),</span></span><br><span class="line"><span class="comment"># indices=tensor([[0]]))</span></span><br><span class="line">predict = model(test_batch).max(<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 我们需要的是index那个矩阵</span></span><br><span class="line">predict = predict[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> predict[<span class="number">0</span>][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">    print(test_text,<span class="string">'is Bad Mean...'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(test_text,<span class="string">'is Good Mean!!!'</span>)</span><br></pre></td></tr></table></figure>
<p>如果要实现使用不同的大小的卷积核，那么需要init部分定义一个卷积层的list，然后在forward卷积部分加上一个for循环。</p>
<h2 id="8-TextLSTM实现"><a href="#8-TextLSTM实现" class="headerlink" title="8 TextLSTM实现"></a>8 TextLSTM实现</h2><p><a href="https://wmathor.com/index.php/archives/1397/" target="_blank" rel="noopener">基本LSTM简介</a><br><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" target="_blank" rel="noopener">pytorch中LSTM接口</a><br><a href="https://blog.csdn.net/weixin_43632501/article/details/100086990" target="_blank" rel="noopener">LSTM实现文本分类</a><br>以简单数据集和网络结构实现TextLSTM，训练集中包括10个单词，用每个单词的前三个字符去预测最后一个字符，目的是便于读者更好的理解该网络的原理。</p>
<h3 id="8-1-导库和设置数据类型"><a href="#8-1-导库和设置数据类型" class="headerlink" title="8.1 导库和设置数据类型"></a>8.1 导库和设置数据类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据类型</span></span><br><span class="line">dtype=torch.FloatTensor</span><br></pre></td></tr></table></figure>
<h3 id="8-2-数据集处理和模型参数"><a href="#8-2-数据集处理和模型参数" class="headerlink" title="8.2 数据集处理和模型参数"></a>8.2 数据集处理和模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">seq_data = [<span class="string">'make'</span>,<span class="string">'need'</span>,<span class="string">'coal'</span>,<span class="string">'word'</span>,<span class="string">'love'</span>,<span class="string">'hate'</span>,<span class="string">'live'</span>,<span class="string">'home'</span>,<span class="string">'hash'</span>,<span class="string">'star'</span>]</span><br><span class="line">char_arr = [c <span class="keyword">for</span> c <span class="keyword">in</span> <span class="string">'abcdefghijklmnopqrstuvwxyz'</span>]</span><br><span class="line">word2idx = &#123;w:i <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(char_arr)&#125;</span><br><span class="line">idx2word = &#123;i:w <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(char_arr)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型参数设置</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">n_class = len(char_arr)</span><br><span class="line"><span class="comment"># 共计4个字符，用3个预测最后一个</span></span><br><span class="line">n_step = <span class="number">3</span></span><br><span class="line"><span class="comment"># 模型输出的维度</span></span><br><span class="line">n_hidden = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    input_batch = []</span><br><span class="line">    target_batch = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> data:</span><br><span class="line">        input = [word2idx[n] <span class="keyword">for</span> n <span class="keyword">in</span> word[:<span class="number">-1</span>]]</span><br><span class="line">        target = word2idx[word[<span class="number">-1</span>]]</span><br><span class="line"></span><br><span class="line">        input_batch.append(np.eye(n_class)[input])</span><br><span class="line">        target_batch.append(target)</span><br><span class="line">    <span class="keyword">return</span> input_batch,target_batch</span><br><span class="line">input_batch,target_batch = make_data(seq_data)</span><br><span class="line"><span class="comment"># 转换成tensor类型</span></span><br><span class="line">input_batch,target_batch = torch.Tensor(input_batch),torch.LongTensor(target_batch)</span><br><span class="line">dataset = Data.TensorDataset(input_batch,target_batch)</span><br><span class="line">loader = Data.DataLoader(dataset,batch_size,<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="8-3-建立网络"><a href="#8-3-建立网络" class="headerlink" title="8.3 建立网络"></a>8.3 建立网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextLSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(TextLSTM,self).__init__()</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        input_size – The number of expected features in the input x</span></span><br><span class="line"><span class="string">        hidden_size – The number of features in the hidden state h</span></span><br><span class="line"><span class="string">        num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</span></span><br><span class="line"><span class="string">        batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.lstm = nn.LSTM(input_size=n_class,hidden_size=n_hidden,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(n_hidden,n_class)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        <span class="comment"># X.shape = [batch_size,seq_len,feature] out.shape = [batch_size,seq_len,direation*hout=n_hidden]</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        input: tensor of shape (L, H_&#123;in&#125;)for unbatched input, (L, N, H_&#123;in&#125;) when batch_first=False or (N, L, H_&#123;in&#125;) when batch_first=True</span></span><br><span class="line"><span class="string">        h_0 default</span></span><br><span class="line"><span class="string">        c_0 default</span></span><br><span class="line"><span class="string">        output: tensor of shape (L, D * H_&#123;out&#125;) for unbatched input, (L, N, D * H_&#123;out&#125;)when batch_first=False or (N, L, D * H_&#123;out&#125;) when batch_first=True</span></span><br><span class="line"><span class="string">        h_n</span></span><br><span class="line"><span class="string">        c_n</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        out,h_nc_n = self.lstm(X)</span><br><span class="line">        out = out[:,<span class="number">-1</span>]</span><br><span class="line">        model = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = TextLSTM()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<h3 id="8-4-模型训练和验证"><a href="#8-4-模型训练和验证" class="headerlink" title="8.4 模型训练和验证"></a>8.4 模型训练和验证</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> loader:</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred,y)</span><br><span class="line">        <span class="keyword">if</span>(epoch+<span class="number">1</span>)%<span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch:'</span>,<span class="string">'%04d'</span>%(epoch+<span class="number">1</span>),<span class="string">'cost='</span>,<span class="string">'&#123;:.6f&#125;'</span>.format(loss))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">predict = model(input_batch).max(<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">predict = predict[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(predict)):</span><br><span class="line">    print(seq_data[i][:<span class="number">-1</span>],idx2word[predict[i].item()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">Epoch: <span class="number">1000</span> cost= <span class="number">0.000023</span></span><br><span class="line">Epoch: <span class="number">1000</span> cost= <span class="number">0.000198</span></span><br><span class="line">Epoch: <span class="number">1000</span> cost= <span class="number">0.000028</span></span><br><span class="line">Epoch: <span class="number">1000</span> cost= <span class="number">0.000232</span></span><br><span class="line">Epoch: <span class="number">1000</span> cost= <span class="number">0.000019</span></span><br><span class="line">Epoch: <span class="number">2000</span> cost= <span class="number">0.000013</span></span><br><span class="line">Epoch: <span class="number">2000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">2000</span> cost= <span class="number">0.000005</span></span><br><span class="line">Epoch: <span class="number">2000</span> cost= <span class="number">0.000003</span></span><br><span class="line">Epoch: <span class="number">2000</span> cost= <span class="number">0.000010</span></span><br><span class="line">Epoch: <span class="number">3000</span> cost= <span class="number">0.000001</span></span><br><span class="line">Epoch: <span class="number">3000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">3000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">3000</span> cost= <span class="number">0.000001</span></span><br><span class="line">Epoch: <span class="number">3000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">4000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">4000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">4000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">4000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">4000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">5000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">5000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">5000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">5000</span> cost= <span class="number">0.000000</span></span><br><span class="line">Epoch: <span class="number">5000</span> cost= <span class="number">0.000000</span></span><br><span class="line">mak e</span><br><span class="line">nee d</span><br><span class="line">coa l</span><br><span class="line">wor d</span><br><span class="line">lov e</span><br><span class="line">hat e</span><br><span class="line">liv e</span><br><span class="line">hom e</span><br><span class="line">has h</span><br><span class="line">sta r</span><br></pre></td></tr></table></figure>
<h2 id="9-BiLSTM实现"><a href="#9-BiLSTM实现" class="headerlink" title="9 BiLSTM实现"></a>9 BiLSTM实现</h2><p><a href="https://wmathor.com/index.php/archives/1447/" target="_blank" rel="noopener">pytorch实现</a><br><a href="https://www.bilibili.com/video/BV1tf4y117hA/?vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">参考讲解</a><br>我们的目的是使用一句话作为训练集训练模型，使得它可以预测某个单词的下一个单词是什么。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag"># 数据挖掘</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="prev" title="机器学习">
      <i class="fa fa-chevron-left"></i> 机器学习
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-神经网络和深度学习"><span class="nav-number">1.</span> <span class="nav-text">1 神经网络和深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-什么是神经网络"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 什么是神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-改进深度神经网络：超参数调整、正则化和优化"><span class="nav-number">2.</span> <span class="nav-text">2 改进深度神经网络：超参数调整、正则化和优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-结构化机器学习工程"><span class="nav-number">3.</span> <span class="nav-text">3 结构化机器学习工程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-卷积神经网络-Convolutional-Neural-Networks"><span class="nav-number">4.</span> <span class="nav-text">4 卷积神经网络 Convolutional Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-自然语言处理：建立序列模型"><span class="nav-number">5.</span> <span class="nav-text">5 自然语言处理：建立序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-数学符号"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 数学符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-循环神经网络"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 循环神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-通过时间的反向传播"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 通过时间的反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-不同类型的循环神经网络"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 不同类型的循环神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-语言模型和序列生成"><span class="nav-number">5.5.</span> <span class="nav-text">5.5 语言模型和序列生成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-1-语言模型的作用"><span class="nav-number">5.5.1.</span> <span class="nav-text">5.5.1 语言模型的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-2-建立一个语言模型"><span class="nav-number">5.5.2.</span> <span class="nav-text">5.5.2 建立一个语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-5-2-1-标记化"><span class="nav-number">5.5.2.1.</span> <span class="nav-text">5.5.2.1 标记化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-5-2-2-构建RNN"><span class="nav-number">5.5.2.2.</span> <span class="nav-text">5.5.2.2 构建RNN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-5-2-3-定义代价函数"><span class="nav-number">5.5.2.3.</span> <span class="nav-text">5.5.2.3 定义代价函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-新序列采样"><span class="nav-number">5.6.</span> <span class="nav-text">5.6 新序列采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-7-带有神经网络的梯度消失"><span class="nav-number">5.7.</span> <span class="nav-text">5.7 带有神经网络的梯度消失</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-1-梯度消失"><span class="nav-number">5.7.1.</span> <span class="nav-text">5.7.1 梯度消失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-2-梯度爆炸"><span class="nav-number">5.7.2.</span> <span class="nav-text">5.7.2 梯度爆炸</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-8-GRU单元-门控循环单元"><span class="nav-number">5.8.</span> <span class="nav-text">5.8 GRU单元(门控循环单元)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-1-初始化c-t"><span class="nav-number">5.8.1.</span> <span class="nav-text">5.8.1 初始化c^t</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-2-计算c-t候选值c-pre-t"><span class="nav-number">5.8.2.</span> <span class="nav-text">5.8.2 计算c^t候选值c_pre^t</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-3-确定门"><span class="nav-number">5.8.3.</span> <span class="nav-text">5.8.3 确定门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-4-更新c-t"><span class="nav-number">5.8.4.</span> <span class="nav-text">5.8.4 更新c^t</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-5-过程绘图"><span class="nav-number">5.8.5.</span> <span class="nav-text">5.8.5 过程绘图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-6-完整的GRU单元"><span class="nav-number">5.8.6.</span> <span class="nav-text">5.8.6 完整的GRU单元</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-9-长短期记忆-LSTM"><span class="nav-number">5.9.</span> <span class="nav-text">5.9 长短期记忆 LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-1-c-pre-t计算"><span class="nav-number">5.9.1.</span> <span class="nav-text">5.9.1 c_pre^t计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-2-更新门Γu和遗忘门Γf"><span class="nav-number">5.9.2.</span> <span class="nav-text">5.9.2 更新门Γu和遗忘门Γf</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-3-输出门Γo"><span class="nav-number">5.9.3.</span> <span class="nav-text">5.9.3 输出门Γo</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-4-过程绘图"><span class="nav-number">5.9.4.</span> <span class="nav-text">5.9.4 过程绘图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-10-双向神经网络"><span class="nav-number">5.10.</span> <span class="nav-text">5.10 双向神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-11-深层循环神经网络"><span class="nav-number">5.11.</span> <span class="nav-text">5.11 深层循环神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-textRNN实现"><span class="nav-number">6.</span> <span class="nav-text">6 textRNN实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-导库"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 导库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-准备数据集"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 准备数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-预处理数据"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 预处理数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-构建网络结构"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 构建网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-模型训练"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 模型训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-TextCNN实现"><span class="nav-number">7.</span> <span class="nav-text">7 TextCNN实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-导库"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 导库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-数据集处理和参数设定"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 数据集处理和参数设定</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-构建模型"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 构建模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-模型训练"><span class="nav-number">7.4.</span> <span class="nav-text">7.4 模型训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-TextLSTM实现"><span class="nav-number">8.</span> <span class="nav-text">8 TextLSTM实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-导库和设置数据类型"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 导库和设置数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-数据集处理和模型参数"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 数据集处理和模型参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-建立网络"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 建立网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-模型训练和验证"><span class="nav-number">8.4.</span> <span class="nav-text">8.4 模型训练和验证</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-BiLSTM实现"><span class="nav-number">9.</span> <span class="nav-text">9 BiLSTM实现</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monica</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Monica</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
