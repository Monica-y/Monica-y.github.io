<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"monica-y.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="机器学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://monica-y.github.io/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="机器学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A03.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E6%8F%8F%E8%BF%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B03.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B04.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B05.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D3.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D4.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D5.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D6.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%923.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%924.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%925.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%926.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%921.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%922.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B3.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%90%91%E9%87%8F%E5%8C%961.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB3.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB4.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB5.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB6.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB7.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB8.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB9.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB10.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB11.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB12.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB13.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB14.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB15.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB16.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%881.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%882.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%883.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%884.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%885.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%886.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%887.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%888.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%889.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%87%E6%8B%9F%E5%90%8810.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B03.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B04.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C4.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C5.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C6.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C7.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C8.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C9.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C10.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C11.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C12.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C13.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C14.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C15.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C16.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C17.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C18.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C19.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C20.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C21.png">
<meta property="article:published_time" content="2022-11-01T11:03:16.175Z">
<meta property="article:modified_time" content="2022-11-11T14:58:11.685Z">
<meta property="article:author" content="Monica">
<meta property="article:tag" content="数据挖掘">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A01.png">

<link rel="canonical" href="https://monica-y.github.io/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monica-y.github.io/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monica">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-01 19:03:16" itemprop="dateCreated datePublished" datetime="2022-11-01T19:03:16+08:00">2022-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-11 22:58:11" itemprop="dateModified" datetime="2022-11-11T22:58:11+08:00">2022-11-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" itemprop="url" rel="index"><span itemprop="name">数据挖掘</span></a>
                </span>
            </span>

          
            <div class="post-description">机器学习笔记</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="https://github.com/TheisTrue/MLofAndrew-Ng/tree/master/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B2%E4%B9%89" target="_blank" rel="noopener">讲义地址</a></p>
<h2 id="1-什么是机器学习"><a href="#1-什么是机器学习" class="headerlink" title="1 什么是机器学习"></a>1 什么是机器学习</h2><h3 id="1-1-机器学习的定义"><a href="#1-1-机器学习的定义" class="headerlink" title="1.1 机器学习的定义"></a>1.1 机器学习的定义</h3><p>Arthur Samuel:在没有明确编程(设置)的情况下，使计算机具有学习能力的研究领域。<br>Tom Mitchell：如果一个计算机程序可以通过经验E使得其在处理任务T时获得更好的效果(这个效果的好坏用性能度量P来衡量),即通过P测定在T上的表现因E而提高。<br>对于机器人跳棋程序来说，经验E就是程序与人下几万次跳棋，任务T就是玩跳棋，性能度量就是与新对手玩跳棋时赢的概率。<br>对于垃圾邮件拦截程序来说，经验E就是观察人对不同邮件的分类，任务T就是将邮件分类为垃圾邮件和非垃圾邮件，性能度量就是正确归类的邮件比例。</p>
<h3 id="1-2-学习算法"><a href="#1-2-学习算法" class="headerlink" title="1.2 学习算法"></a>1.2 学习算法</h3><p>1 监督学习(supervised learning),主要思想就是我们会教计算机做某件事情。<br>2 无监督学习(unsupervised learning)，主要思想是让计算机自己学习。<br>其他热词：强化学习(reinforcement)，推荐系统(recommender system)<br>要点：在应用学习算法的实际建议。如果我想要开发机器学习系统，如何让那些最佳实践操作指导我的决定，用什么方式建立自己的系统。</p>
<h2 id="2-监督学习"><a href="#2-监督学习" class="headerlink" title="2 监督学习"></a>2 监督学习</h2><h3 id="2-1-回归问题"><a href="#2-1-回归问题" class="headerlink" title="2.1 回归问题"></a>2.1 回归问题</h3><p>假设我要预测房价，我收集到了一些现有的房价数据。想要知道750平米的房子的价格是多少呢？学习算法能做的一件事情就是根据数据画一条直线，或者说用一条直线拟合数据(粉色直线)。然后可以估计出房子的价值约为150k。除了用一条直线拟合数据，可以用二次函数或二阶多项式(蓝色曲线)。然后可以估计出房子的价值约为200k。<br><img src="/images/机器学习/监督学习1.png" alt="监督学习1"><br>监督学习是指我们给算法一个数据集，其中包含了正确答案。也就是说我们给它一个房价数据集，在这个数据集中的每个样本，我们都给出正确的价格，即这个房子实际卖价。算法的目的就是给出更多的正确答案。例如得到面积为750的房子的价格。<br>用更专业的术语来定义，它也被称为回归问题。这里的回归问题指的是我们想要预测连续的数值输出，也就是房子的价格。在技术上来说，价格可以精确到分，因此实际上是一个离散值，但通常我们认为房价是一个实数，即连续值。回归这个术语是指我们设法预测连续值的属性。</p>
<h3 id="2-2-分类问题"><a href="#2-2-分类问题" class="headerlink" title="2.2 分类问题"></a>2.2 分类问题</h3><p>假设我想看医疗记录并设法预测乳腺癌是恶性的还是良性的。假设某人发现了一个乳腺肿瘤，即乳房上的肿块，恶性肿瘤就是有害且危险的，良性肿瘤就是无害的。<br>假设在我的数据集中，横轴是肿瘤的尺寸，纵轴表示我们看到的肿瘤样本是否是恶性的。然后有一个肿瘤样本的大小位于粉色箭头处，我们想要估计出肿瘤是良性还有恶性的概率。这就是一个分类问题。<br>分类问题就是我们设法预测一个离散值输出。对于这个问题，就是输出是0还是1。实际上在分类问题中，有时也可以有两个以上的可能的输出值。在实际例子中就是，可能有三种类型的乳腺癌，因此，可能的输出就有0(良性)，1(癌症类型1)，2(癌症类型2)和3(癌症类型3)。<br><img src="/images/机器学习/监督学习2.png" alt="监督学习2"><br>在分类问题中，有另一种方法来绘制这些数据。如果肿瘤的大小是预测恶性或良性的特征，我们可以在一个坐标轴上(表示肿瘤大小的坐标轴)，用不同的符号来表示良性或恶性(第二行的图)。在这个例子中，我们只使用了一个特征(属性)，即肿瘤得大小。在其他得机器学习问题中，我们会有多个特征(属性)。假设我们不仅知道肿瘤的大小，还知道病人的年纪，在这种情况下的数据集可以表示为下图。如果有一个患者，他的情况位于图中粉点处。因此在给定的数据集上，学习算法能做的，就是在数据上画出一条直线(黑色直线)，设法将恶性瘤和良性瘤分开，算法可以通过这条直线判断肿瘤的类型。在这个例子中，我们有两种特征，即病人的年龄和肿瘤大小。在其他的机器学习算法中，往往会有更多的特征，例如肿块的厚度，肿瘤细胞大小的均匀性，肿瘤细胞形状的均匀性。<br><img src="/images/机器学习/监督学习3.png" alt="监督学习3"><br>最有趣的机器学习算法，是一个不仅仅能处理两到三个或五个特征，而是能处理无穷多特征的算法，即如何在计算机中存储无穷维度的数据，而不会溢出呢？以支持向量机算法为例，就有一个灵活的数学技巧，允许计算机处理无穷多的特征。</p>
<h3 id="2-3-总结"><a href="#2-3-总结" class="headerlink" title="2.3 总结"></a>2.3 总结</h3><p>在这节课上我们讨论了监督学习，想法是在监督学习中，对于数据集中的每个样本，我们想要算法预测，并得出“正确答案”。像是房子的价格，或肿瘤是恶性还是良性的。<br>我们也讨论了回归问题，回归是指我们的目标是预测一个连续值输出。<br>我们还讨论了分类问题，其目的是预测离散值输出。</p>
<h2 id="3-无监督学习"><a href="#3-无监督学习" class="headerlink" title="3 无监督学习"></a>3 无监督学习</h2><h3 id="3-1-聚类"><a href="#3-1-聚类" class="headerlink" title="3.1 聚类"></a>3.1 聚类</h3><p>在上一节的监督学习中，我们的数据集中每个样本都被标明为阳性样本或阴性样本，即良性或恶性肿瘤。对于监督学习中的每个样本，我们已经被清楚地告知了什么是所谓的正确答案，即它们是良性还是恶性。在无监督学习中，我们所用的数据集中没有任何标签，我们试图在其中找到某种结构。无监督学习可能判定该数据集包含两个不同的簇。无监督学习算法，可以把这些数据分成两个不同的簇，这就是聚类算法。<br><img src="/images/机器学习/无监督学习1.png" alt="无监督学习1"><br>一个应用聚类算法的例子就是谷歌新闻。谷歌新闻所做的就是每天去网络上收集几十万条新闻，然后将它们组成一个个新闻专题。本质上就是将它们分簇，有关同一主题的新闻，被显示在一起。<br>其实聚类算法和无监督学习算法，也可以用于基因检测中。基因检测用于测定特定基因的表达程度，我们可以对测序结果进行聚类。<br>无监督学习或聚类算法可以用来组织大型的计算机集群。尝试找到那些机器趋向于协同工作，如果在实际使用中让这些机器共同工作，则数据中心可以更高效地工作。<br>聚类算法可以用于社交网络分析。如果可以得到我Email最频繁地联系人，机器就可以自动识别同属于一个圈子的朋友，判断哪些人互相认识。<br>聚类算法可以用于市场分析。许多公司利用庞大的客户信息数据库，对客户进行分类，从而能够自动高效地使用不同地策略进行销售。<br>聚类算法可以被用于天文数据分析，用于星系形成理论的研究。</p>
<h3 id="3-2-鸡尾酒会算法"><a href="#3-2-鸡尾酒会算法" class="headerlink" title="3.2 鸡尾酒会算法"></a>3.2 鸡尾酒会算法</h3><p>聚类算法只是无监督学习的一种。<br>鸡尾酒会问题。有一个宴会，有一屋子的人，大家都坐在一起说话，因为每个人都同时在说话，有许多声音混杂在一起，几乎很难听清楚面前的人说话。假设一个鸡尾酒会上只有两个人，也就是只有两个人在同时说话。我们放下了两个麦克风来记录来自两人声音的不同组合。两个人在两个麦克风的记录中声音大小不同，但是都是两个人话语重合的声音。<br>我们把这两个声音交给鸡尾酒会算法，让它帮忙找出数据的结构。鸡尾酒会算法会分离出这两个被叠加到一起的声音。或者是说话的声音和背景音。算法的主要代码为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[W,s,v] &#x3D; svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;);</span><br></pre></td></tr></table></figure>
<p>在课程中使用的代码为Octave。svd函数全称为奇异值分解函数。</p>
<h2 id="4-模型描述"><a href="#4-模型描述" class="headerlink" title="4 模型描述"></a>4 模型描述</h2><p>回想2中预测房子价格的例子，根据已有的数据集，我们需要预测面积为1250的房子的价值。这是一个监督学习——对于数据集中的房子我们知道它们的面积和价格，这是一个回归问题——我们预测一个具体的数值输出。另一种常见的监督学习问题——分类问题，我们用它来预测离散值输出。<br>在建模过程中的符号如下：<br>m:表示训练样本的数量。<br>x’s:输入变量/特征。<br>y’s:输出变量/标签变量。<br>(x,y)表示一个训练样本。</p>
<script type="math/tex; mode=display">(x^{(i)},y^{(i)})</script><p>表示第i个训练样本，上标i是训练集的一个索引。<br>在上图中h表示假设函数，它的作用是把房子的大小作为输入变量，输出为相应房子的预测y值。h是一个从x到y的映射。我们将h表示为</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1*x,缩写为 h(x)</script><p><img src="/images/机器学习/模型描述1.png" alt="模型描述1"><br>从表达式可以看出，y是一个关于x的线性函数(这是因为我们要从最基础的拟合开始讲起)。我们将从这个例子开始，先拟合线性函数，然后在此基础上，最终处理更加复杂的模型，以及学习更复杂的学习算法。这个模型被称为线性回归，这个例子为一元线性回归(单变量线性回归)。</p>
<h2 id="5-代价函数"><a href="#5-代价函数" class="headerlink" title="5 代价函数"></a>5 代价函数</h2><h3 id="5-1-代价函数的定义"><a href="#5-1-代价函数的定义" class="headerlink" title="5.1 代价函数的定义"></a>5.1 代价函数的定义</h3><p>在这一节中，我们将定义代价函数的概念，帮助我们弄清楚如何把最有可能的直线与我们的数据相拟合。</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1*x,缩写为 h(x)</script><script type="math/tex; mode=display">\theta_{i's}为模型参数</script><p>我们选择不同的θ值，就会得到不同的假设函数。<br><img src="/images/机器学习/代价函数1.png" alt="代价函数1"><br>我们要做的就是得出θ0和θ1这两个参数的值，来让假设函数表示的直线尽量地与这些数据点很好的拟合。那么我们如何得出θ0和θ1的值来使它很好地拟合数据呢？我们要去选择能使h(x)也就是输入x时我们预测的值最接近该样本对应的y值的参数θ0和θ1。<br>在线性回归中，我们要解决的是一个最小化问题，使得下式的值最小。</p>
<script type="math/tex; mode=display">minimize\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>我们使用(x^(i),y^(i))表示第i个样本。m指的是训练集的样本容量。1/m 表示m个样本方差的均值，2是为了方便后面求导，其实是可以取任意实数，最终都会得出相同的θ0值和相同的θ1值，使得上式取得最小值。<br>我们将上述值定义成一个代价函数(cost function)如下：</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>其中，</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1*x</script><p>代价函数也被称作平方误差函数(Squard error function)，它可能是解决回归问题最常用的手段了。</p>
<h3 id="5-2-代价函数的作用"><a href="#5-2-代价函数的作用" class="headerlink" title="5.2 代价函数的作用"></a>5.2 代价函数的作用</h3><p><img src="/images/机器学习/代价函数2.png" alt="代价函数2"><br>实际上这包含了两个关键函数：<br>第一个是假设函数hθ(x),对于给定的θ，这是一个关于x的函数。<br>第二个是代价函数J(θ1)，是关于参数θ1的函数，它控制着h中直线的斜率。<br>当我们假设θ1=1时，我们求J(θ1)的值。</p>
<script type="math/tex; mode=display">J(\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">J(\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(\theta_1x^{(i)}-y^{(i)})^2</script><script type="math/tex; mode=display">J(\theta_1)=\frac{1}{2m}0^{2}+0^{2}+0^{2}=0^2</script><p>上述结果是在下图数据集的基础上得到的。<br><img src="/images/机器学习/代价函数3.png" alt="代价函数3"><br>同理我们可以计算当θ1=0.5时，J(0.5)=7/12。当θ1=0时，J(0)=7/3。最终J(θ1)的函数图像如下：<br><img src="/images/机器学习/代价函数4.png" alt="代价函数4"><br>对于每个θ1，我们可以得到一个假设函数h，也可以得到损失函数J在θ1处的取值J(θ1)。优化算法的目标是我们通过选择不同的θ1，获得最小的J(θ1)，这就是线性回归的目标函数。在上图中，当θ1=1时，损失函数取得最小值。</p>
<h3 id="5-3-代价函数的作用-plus"><a href="#5-3-代价函数的作用-plus" class="headerlink" title="5.3 代价函数的作用-plus"></a>5.3 代价函数的作用-plus</h3><p>在上面的讨论中，为了简化问题，我们将θ0=0，现在我们保留全部参数θ0和θ1。则它的损失函数如下：<br><img src="/images/机器学习/代价函数5.png" alt="代价函数5"><br>代价函数最小的点对应着更好的假设函数。</p>
<h2 id="6-梯度下降"><a href="#6-梯度下降" class="headerlink" title="6 梯度下降"></a>6 梯度下降</h2><h3 id="6-1-梯度下降算法步骤"><a href="#6-1-梯度下降算法步骤" class="headerlink" title="6.1 梯度下降算法步骤"></a>6.1 梯度下降算法步骤</h3><p>梯度下降是很常用的算法，它不仅悲用在线性回归上，还被广泛用于机器学习的众多领域。我们可以使用梯度下降的方法去最小化线性回归的代价函数J，或是其他函数。<br>我们有一个函数J(θ0，θ1)，我们的目标是最小化J。<br>梯度下降的思路：<br>1)先给定θ0和θ1的初始值。初值到底是多少其实并不重要，但是一般将θ0设为0，θ1也设为0。<br>2)不停地一点点地改变θ0和θ1，来使得J变小。直到我们找到J的最小值，或者局部最小值。<br>梯度下降的一个有趣的地方在于，从不同的初始值出发，可能得到不同的局部最优解。<br>我们将会一直重复下面操作，直到收敛。</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)(for j=0 and j=1)</script><p>在上式中，:=表示赋值。在Pascal中，a=b表示我断言a和b相等。α被称为学习率，α用来控制梯度下降时，我们迈出多大的步子。如果α很大，梯度下降就很迅速，我们会用大步子下山(损失函数值)。如果α很小，那么我们会迈着很小的小碎步下山。<br>实现梯度下降算法的微妙之处是，对于上述更新方程，我们需要<strong>同时更新</strong>θ0和θ1。具体方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp0 = theta_0-alpha*dtheta0J</span><br><span class="line">temp1 = theta_1-alpha*dtheta1J</span><br></pre></td></tr></table></figure>
<p>同步更新是更自然的实现方法。</p>
<h3 id="6-2-梯度下降相关知识"><a href="#6-2-梯度下降相关知识" class="headerlink" title="6.2 梯度下降相关知识"></a>6.2 梯度下降相关知识</h3><h4 id="6-2-1-更新函数解释"><a href="#6-2-1-更新函数解释" class="headerlink" title="6.2.1 更新函数解释"></a>6.2.1 更新函数解释</h4><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)(for j=1 and j=1)</script><p>α控制我们以多大幅度更新这个参数θj。为了能更好地理解这个式子，我们假设想要最小化的函数只有一个参数的情形。假如我们有一个代价函数J，只有一个参数θ1(同5中简化情况)。<br>假设此时我们的损失函数J(θ1)如下图，<br><img src="/images/机器学习/梯度下降1.png" alt="梯度下降1"></p>
<h4 id="6-2-2-学习率解释"><a href="#6-2-2-学习率解释" class="headerlink" title="6.2.2 学习率解释"></a>6.2.2 学习率解释</h4><p>如果α过小(下图中上半部分情况)，就需要很多步才能到达最低点。<br>如果α过大(下图中下半部分情况)，那么梯度下降可能会越过最低点，甚至可能无法收敛。<br><img src="/images/机器学习/梯度下降2.png" alt="梯度下降2"><br>而如果θ1恰好初始化在局部最优点，那么梯度下降算法不会改变任何参数值。<br>梯度下降算法每次调整参数的大小不仅与规定的学习率α有关，也与当前θ点处的导数大小有关，越接近最低点，函数变化越平缓，导数越小，梯度下降算法调整参数的幅度就越小。所以实际上没有必要再另外减小α。<br><img src="/images/机器学习/梯度下降3.png" alt="梯度下降3"></p>
<h3 id="6-3-线性回归的梯度下降"><a href="#6-3-线性回归的梯度下降" class="headerlink" title="6.3 线性回归的梯度下降"></a>6.3 线性回归的梯度下降</h3><p>本节中我们将梯度下降和线性回归中的代价函数结合，得到线性回归的算法。<br><img src="/images/机器学习/梯度下降4.png" alt="梯度下降4"><br><img src="/images/机器学习/梯度下降5.png" alt="梯度下降5"><br>在求得偏导之后，我们带入上文中梯度下降的算法框架，得到<br><img src="/images/机器学习/梯度下降6.png" alt="梯度下降6"><br>梯度下降容易陷入局部最优。但是线性回归的代价函数是一个碗状函数(二元)，术语叫凸函数，这个函数没有局部最优解，只有一个全局最优。当计算这种代价函数的梯度下降时，只要使用线性回归，它总会收敛到全局最优。<br>上文中我们讨论的梯度下降算法，被称为Batch梯度下降，Batch指的是每一步梯度下降，我们都遍历了整个训练集的样本。在梯度下降计算偏导数时，我们计算总和。还会有其他不是Batch模式的梯度下降算法，它没有全览整个训练集，每次只关注了小子集。<br>求解代价函数的最小值，我们还有最小二乘法(正规方程组解法)，但是相对于此方法，梯度下降法适合更大的数据集。</p>
<h2 id="7-矩阵知识回顾"><a href="#7-矩阵知识回顾" class="headerlink" title="7 矩阵知识回顾"></a>7 矩阵知识回顾</h2><h3 id="7-1-矩阵和向量定义"><a href="#7-1-矩阵和向量定义" class="headerlink" title="7.1 矩阵和向量定义"></a>7.1 矩阵和向量定义</h3><p>矩阵是指由数字组成的矩形数组。矩阵的维数：行数×列数。</p>
<script type="math/tex; mode=display">R^{4\times 2}代表所有4\times 2的矩阵的集合</script><p>矩阵元素Aij表示矩阵中A中第i行第j列的元素。矩阵提供了一种很好的方式帮助我们快速整理、索引和访问大量数据。一般我们用大写字母表示矩阵，用小写字母表示数字/标量/向量。<br>一个向量时一种特殊的矩阵，向量是只有一列的矩阵，即维数为n×1，表示这是一个n维向量。R^4表示一个四维向量的集合。向量元素yi表示向量y的第i个元素。i可以从1开始(默认)，也可以从0开始。</p>
<h3 id="7-2-矩阵运算"><a href="#7-2-矩阵运算" class="headerlink" title="7.2 矩阵运算"></a>7.2 矩阵运算</h3><h4 id="7-2-1-矩阵加法"><a href="#7-2-1-矩阵加法" class="headerlink" title="7.2.1 矩阵加法"></a>7.2.1 矩阵加法</h4><p>只有相同维度的矩阵才可以相加。</p>
<h4 id="7-2-2-矩阵标量运算"><a href="#7-2-2-矩阵标量运算" class="headerlink" title="7.2.2 矩阵标量运算"></a>7.2.2 矩阵标量运算</h4><p>矩阵和标量的乘法运算。标量代表一个数字，或者实数。<br>3乘以矩阵A，等于将矩阵A中所有元素都逐一与3相乘。得到的结果是相同维度的矩阵。且乘法满足交换律。<br>矩阵A除以4，等同于0.25乘以矩阵A。</p>
<h4 id="7-3-3-矩阵之间的乘法"><a href="#7-3-3-矩阵之间的乘法" class="headerlink" title="7.3.3 矩阵之间的乘法"></a>7.3.3 矩阵之间的乘法</h4><p>矩阵相乘的前提是A矩阵的列数和B矩阵的行数相同。计算Aij的方法为让A的第i行元素分别乘以B中第j列的元素，并且相加起来。<br>我们可以利用矩阵乘法实现房价预测。<br><img src="/images/机器学习/矩阵知识回顾1.png" alt="矩阵知识回顾1"><br>我们可以利用矩阵乘法实现更为细致的房价预测。<br><img src="/images/机器学习/矩阵知识回顾2.png" alt="矩阵知识回顾2"><br>矩阵乘法的性质：<br>1)矩阵乘法不满足交换律。<br>2)矩阵乘法满足结合律。<br>3)幺元。在实数空间，1是一个乘法单位。在矩阵空间，单位矩阵是一个乘法单位，通常记作I(n×n)。对于A(m×n)矩阵来说，IA=AI=A，其中两个I的维度是不相同的，第一个是m×m，第二个是n×n。</p>
<h3 id="7-3-逆和转置"><a href="#7-3-逆和转置" class="headerlink" title="7.3 逆和转置"></a>7.3 逆和转置</h3><h4 id="7-3-1-矩阵的逆"><a href="#7-3-1-矩阵的逆" class="headerlink" title="7.3.1 矩阵的逆"></a>7.3.1 矩阵的逆</h4><p>在实数空间，每一个实数(除0外)都有一个倒数。在矩阵空间，如果矩阵A有逆矩阵A^(-1)，有</p>
<script type="math/tex; mode=display">AA^{-1}=A^{-1}A=I</script><p>如果一个矩阵的行数和列数相同，那么称这个矩阵为方阵。只有满秩的方阵才有逆矩阵。我们把没有逆矩阵的矩阵称为“奇异矩阵”或是“退化矩阵”。python求解逆矩阵的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a  = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])  <span class="comment"># 初始化一个非奇异矩阵(数组)</span></span><br><span class="line">print(np.linalg.inv(a))  <span class="comment"># 对应于MATLAB中 inv() 函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵对象可以通过 .I 更方便的求逆</span></span><br><span class="line">A = np.matrix(a)</span><br><span class="line">print(A.I)</span><br></pre></td></tr></table></figure>
<h4 id="7-3-2-矩阵的转置"><a href="#7-3-2-矩阵的转置" class="headerlink" title="7.3.2 矩阵的转置"></a>7.3.2 矩阵的转置</h4><p>A矩阵经过转置生成B矩阵，其特点是Aij=Bji。</p>
<h2 id="8-多元线性回归"><a href="#8-多元线性回归" class="headerlink" title="8 多元线性回归"></a>8 多元线性回归</h2><h3 id="8-1-多元线性回归定义"><a href="#8-1-多元线性回归定义" class="headerlink" title="8.1 多元线性回归定义"></a>8.1 多元线性回归定义</h3><p>在训练中我们会用到如下的符号：<br><img src="/images/机器学习/多元线性回归1.png" alt="多元线性回归1"><br>当我们有了多个特征量之后，我们的假设函数也需要相应变化。</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4</script><p>然后我们引入矩阵乘法：<br><img src="/images/机器学习/多元线性回归2.png" alt="多元线性回归2"></p>
<h3 id="8-2-多元梯度下降法"><a href="#8-2-多元梯度下降法" class="headerlink" title="8.2 多元梯度下降法"></a>8.2 多元梯度下降法</h3><h4 id="8-2-1-多元梯度下降法的算法流程"><a href="#8-2-1-多元梯度下降法的算法流程" class="headerlink" title="8.2.1 多元梯度下降法的算法流程"></a>8.2.1 多元梯度下降法的算法流程</h4><p>通过上文的描述，我们可以把多元线性回归的模型参数看作是一个n+1维的向量θ。多元梯度下降法的算法流程如下：<br><img src="/images/机器学习/多元线性回归3.png" alt="多元线性回归3"></p>
<h4 id="8-2-2-多元梯度下降法——特征缩放"><a href="#8-2-2-多元梯度下降法——特征缩放" class="headerlink" title="8.2.2 多元梯度下降法——特征缩放"></a>8.2.2 多元梯度下降法——特征缩放</h4><p>如果一个机器学习问题中有多个特征，且我能确保不同特征的取值都处在一个相近的范围内，这样梯度下降算法能更快地收敛。<br><img src="/images/机器学习/多元线性回归4.png" alt="多元线性回归4"><br>使用特征缩放方法将特征的取值约束到-1到+1之间，也不一定是-1到+1，只需要不同特征的取值范围相近，梯度下降算法就可以正常地工作。<br>除了上图中将特征除以最大值的方法外，在特征缩放中，有时我们也会对特征进行均值归一化的工作。具体方法如下：</p>
<script type="math/tex; mode=display">x_i = \frac{x_i-\mu_i}{max-min}</script><p>需要注意的是，我们不会对x0进行均值归一化的操作。上式的分母部分也可以是xi的标准差。</p>
<h4 id="8-2-3-多元梯度下降法——学习率"><a href="#8-2-3-多元梯度下降法——学习率" class="headerlink" title="8.2.3 多元梯度下降法——学习率"></a>8.2.3 多元梯度下降法——学习率</h4><p>为了保证梯度下降算法正确工作，我们可以绘制算法迭代次数-minJ(θ)的折线图，来展示梯度下降算法每次迭代之后，代价函数的值。如果算法正确运行，那么每一步迭代之后J(θ)都应该下降。当算法迭代到300-400次之间时，J(θ)的值已经下降的非常缓慢了。当算法迭代到400次时，从下图中可以看出已经差不多收敛了，因为代价函数值没有再继续下降了。<br><img src="/images/机器学习/多元线性回归5.png" alt="多元线性回归5"><br>也可以采用自动收敛测试的办法：如果在一次迭代中J(θ)减小的幅度小于10^(-3)则认为算法收敛。但是想要寻找到一个合适的阈值是困难的。一般情况下我们可以通过上图中的折线图判断。<br>在使用梯度下降算法时，如果迭代次数-minJ的变化趋势如上图，则可以说明梯度下降算法在正常工作，反之，随着迭代次数的增加，J在不断上升或是波动，则证明梯度下降算法没有正常工作。此时，通常意味着算法应该使用较小的学习率α。<br>只要α足够小，那么每次迭代之后代价函数J都会下降。但是学习率不可以国小，那样的话梯度下降算法可能收敛得很慢。<br><img src="/images/机器学习/多元线性回归6.png" alt="多元线性回归6"><br>总结一下，如果学习率α太小的话，算法收敛速度会较慢；如果学习率α太大的话，代价函数J可能不会在每次迭代都下降，甚至可能不收敛。在一些情况下，如果α值太大，也可能会出现收敛缓慢的问题，但并不常见。为了了解算法的运行情况，通常需要绘制代价函数J随迭代步数变化的曲线。<br>尝试一系列α值，0.001，0.003，0.01，0.03，0.1，0.3，1……然后通过绘制J随迭代次数变化的曲线，选择一个使得J快速下降的α值。</p>
<h2 id="9-特征和多项式回归"><a href="#9-特征和多项式回归" class="headerlink" title="9 特征和多项式回归"></a>9 特征和多项式回归</h2><p>在房价预测中，我们可以将房子的长和宽两个特征合并为房子的面积这一个特征。这样就将特征从两个减少到一个。根据数据趋势，我们打算使用三次函数对特征进行拟合，如下图。<br><img src="/images/机器学习/特征和多项式回归1.png" alt="特征和多项式回归1"><br>事实上，对于特征和假设函数，我们有更多的选择。<br><img src="/images/机器学习/特征和多项式回归2.png" alt="特征和多项式回归2"></p>
<h2 id="10-正规方程-区别于迭代方法的线性回归直接解法"><a href="#10-正规方程-区别于迭代方法的线性回归直接解法" class="headerlink" title="10 正规方程(区别于迭代方法的线性回归直接解法)"></a>10 正规方程(区别于迭代方法的线性回归直接解法)</h2><p>在上面我们了解了求minJ的梯度下降解法。正规方程提供了一种求θ的解析解法，使得我们不再需要运行迭代算法，而是可以直接一次性地求解θ的最优值。</p>
<h3 id="10-1-正规方程求解流程"><a href="#10-1-正规方程求解流程" class="headerlink" title="10.1 正规方程求解流程"></a>10.1 正规方程求解流程</h3><p><img src="/images/机器学习/正规方程求解流程1.png" alt="正规方程求解流程1"><br>下面是具体计算过程。<br><img src="/images/机器学习/正规方程求解流程2.png" alt="正规方程求解流程2"><br>设计矩阵X的构造方法如下图。<br><img src="/images/机器学习/正规方程求解流程3.png" alt="正规方程求解流程3"><br>求解θ的关键是</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>如果我们使用正规方程法求解θ，那么就不需要特征缩放。但特征缩放对于梯度下降法十分重要。<br>梯度下降法和标准方程法的适用范围：<br>梯度下降法(适用于特征较多，特征数为上万个)：<br>1 需要选择学习速率α。我们需要运行多次，尝试不同的α，找到运行效果最好的那个。<br>2 需要多次迭代。需要绘制迭代次数和损失函数J的曲线来检查收敛性。<br>3 在特征很多的情况下也能运行地相当好。<br>标准方程法(适用于特征较少，特征数为几千个)：<br>1 不需要选择学习速率α。<br>2 不需要迭代。<br>3 需要计算inv(X^T X),计算结果的维度为特征数(n+1)*(n+1),而逆矩阵的计算复杂度为O(n^3),当特征比较多时会比较慢。<br>当样本数据的特征数小于1万时，通常使用正规方程法进行线性回归，而不是用梯度下降法。</p>
<h3 id="10-2-正规方程在矩阵不可逆情况下的解决方法"><a href="#10-2-正规方程在矩阵不可逆情况下的解决方法" class="headerlink" title="10.2 正规方程在矩阵不可逆情况下的解决方法"></a>10.2 正规方程在矩阵不可逆情况下的解决方法</h3><p>在上文中，我们通过下式计算θ。</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>但是当XX^T不可逆时，也就是结果为奇异矩阵(或退化矩阵)时，正规方程解法可能会出现问题。在python中，我们可以通过计算矩阵的伪逆来解决这一问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个奇异阵 A 4×4</span></span><br><span class="line">A = np.zeros((<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># 第一行，倒数第一个元素为1</span></span><br><span class="line">A[<span class="number">0</span>, <span class="number">-1</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment"># 倒数第一行，第一个元素为-1</span></span><br><span class="line">A[<span class="number">-1</span>, <span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"><span class="comment"># 将A转化成矩阵</span></span><br><span class="line">A = np.matrix(A)</span><br><span class="line">print(A)</span><br><span class="line"><span class="comment"># print(A.I)  将报错，矩阵A为奇异矩阵，不可逆</span></span><br><span class="line">print(np.linalg.pinv(A))   <span class="comment"># 求矩阵A的伪逆（广义逆矩阵），对应于MATLAB中pinv()函数</span></span><br></pre></td></tr></table></figure>
<p>XX^T矩阵不可逆的原因：<br>1 数据集中包含了多余的特征(线性相关的特征)。此时我们需要删除掉一部分线性相关的特征。如area和length，area= length*length，我们可以删掉area这个特征。<br>2 数据集中包含了太多的特征。例如数据集中一共只有4个样本，但是却有40个特征。此时我们需要删除掉一些特征，或使用正则化的方法。正则化方法可以帮助我们使用很多的特征来配置很多参数，即使我们有一个相对较小的训练集。</p>
<h2 id="11-单变量线性回归实现"><a href="#11-单变量线性回归实现" class="headerlink" title="11 单变量线性回归实现"></a>11 单变量线性回归实现</h2><p>题目：假设你是一家餐厅的CEO，正在考虑开一家分店，根据该城市的人口数据预测其利润。<br>我们对梯度下降的算法进行向量化，可以得到每步更新的θ表达式为</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}X^T(X\theta-Y)</script><p>具体推到过程如下：<br><img src="/images/机器学习/单变量线性回归实现1.png" alt="单变量线性回归实现1"></p>
<h3 id="11-1-读取文件"><a href="#11-1-读取文件" class="headerlink" title="11.1 读取文件"></a>11.1 读取文件</h3><p>使用pandas读文件，得到一个类似于excel的表格。使用的函数为 pandas.read_csv()，关键参数如下：<br>1 filepath_or_buffer:字符串类型，要读取的文件存放路径，可以是http和文件<br>2 sep：字符串类型，分隔符，默认为’,’<br>3 header:整数，指定第几行作为列名(忽略注解行)，如果没有指定列名默认header=0,即文件中没有列名<br>4 names列表类型，指定列名<br>使用实例为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pandas是基于numpy的一种工具，该工具是为了解决数据分析任务而创建的</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(filepath_or_buffer=<span class="string">'ex1data1.txt'</span>,names=[<span class="string">'population'</span>,<span class="string">'profit'</span>])</span><br></pre></td></tr></table></figure>
<h3 id="11-2-数据集准备"><a href="#11-2-数据集准备" class="headerlink" title="11.2 数据集准备"></a>11.2 数据集准备</h3><p>绘制数据集的散点图，DataFrame使用data.plot.scatter()方法，关键参数如下：<br>1 横坐标<br>2 纵坐标<br>3 c：绘图使用的颜色<br>4 label:表示散点图的标签<br>5 s:表示绘制的散点的大小<br>使用实例为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.plot.scatter(<span class="string">'population'</span>,<span class="string">'profit'</span>,c=<span class="string">'b'</span>,label=<span class="string">'population'</span>,s=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<p>根据上图中的推导，X矩阵的第一列全为1，所以我们需要在population前面插入一列1。使用的方法为DataFrame.insert(loc,column,value,allow_duplicates=False),关键参数为：<br>1 loc:插入的列索引，如要插入最前面，那么col=0<br>2 column:插入列的标签，字符串<br>3 value:插入列的值<br>使用实例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 注意没有赋值，不可以写成data=data.insert()的形式</span></span><br></pre></td></tr></table></figure>
<p>在完成插入之后，我们需要将X矩阵的Y矩阵分离开。使用的方法为DataFrame.iloc(),使用的方法为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = data.iloc[:,<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># :表示所有的行都要参与切片</span></span><br><span class="line"><span class="comment"># 0:-1表示从第0列开始，到-1表示最后一列，但是取不到最后一列[0,-1)</span></span><br><span class="line">Y = data.iloc[:,<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># -1表示将最后一列切片出来</span></span><br></pre></td></tr></table></figure>
<p>因为后面我们需要涉及到矩阵运算，所以需要把DataFrame的类型变为数组型的，便于后续计算。DataFrame转换成ndarray的方法有：<br>1 df.values<br>2 df.as_matrix()<br>3 np.array(df)<br>使用实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = X.values</span><br><span class="line">Y = Y.values</span><br></pre></td></tr></table></figure>
<p>接下来我们查看一下数据的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure>
<h3 id="11-3-损失函数"><a href="#11-3-损失函数" class="headerlink" title="11.3 损失函数"></a>11.3 损失函数</h3><p>因为X是2维数组(shape有两个值)，Y是1维数组(shape只有一个值)为了后续运算方便，我们把Y也改成2维数组。方法为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = Y.reshape(<span class="number">97</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们需要定义代价函数。我们的依据是：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}SUM(X\theta-y)^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunction</span><span class="params">(X,Y,theta)</span>:</span></span><br><span class="line">    inner = np.power(X @ theta -Y,<span class="number">2</span>) <span class="comment"># @ 表示矩阵相乘</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(inner)/(<span class="number">2</span>*len(X))</span><br></pre></td></tr></table></figure>
<p>然后，我们对θ进行初始化。θ的维度为2×1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = np.zeros((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>最后我们就可以计算出损失函数的初始值了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost_init = costFunction(X,Y,theta)</span><br></pre></td></tr></table></figure>
<h3 id="11-4-梯度下降函数"><a href="#11-4-梯度下降函数" class="headerlink" title="11.4 梯度下降函数"></a>11.4 梯度下降函数</h3><p>我们更新θ的依据为：</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}X^T(X\theta-Y)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,Y,theta,alpha,iters)</span>:</span></span><br><span class="line">    costs = [] <span class="comment"># 用于存储迭代过程中的cost</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        theta = theta - (X.T @ (X @ theta - Y)) * alpha/len(X)</span><br><span class="line">        cost = costFunction(X,Y,theta)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        <span class="comment"># 打印一些cost</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(cost)</span><br><span class="line">    <span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure>
<p>我们设置的学习率和迭代次数如下,然后调用函数开始迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.02</span></span><br><span class="line">iters = <span class="number">2000</span></span><br><span class="line">theta,costs = gradientDescent(X,Y,theta,alpha,iters)</span><br></pre></td></tr></table></figure>
<h3 id="11-5-可视化损失函数"><a href="#11-5-可视化损失函数" class="headerlink" title="11.5 可视化损失函数"></a>11.5 可视化损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy是一个科学计算库，处理多维数组，进行数据分析</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># matplotlib是python的2D绘图库 matplotlib.pyplot提供一个类似matlab的绘图框架</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 通过该代码生成一幅空的图，ax是一个绘图的实例,后续可以使用ax继续往该图上添加元素</span></span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line"><span class="comment"># 如果是散点图，则为scatter</span></span><br><span class="line"><span class="comment"># 因为costs是list，所以我们需要把int型的iters变成list，方法为np.arrange()</span></span><br><span class="line">ax.plot(np.arange(iters),costs,<span class="string">'b'</span>)</span><br><span class="line"><span class="comment"># 通过ax这个实例设置x和y轴的label</span></span><br><span class="line">ax.set(xlabel=<span class="string">'iters'</span>,ylabel=<span class="string">'costs'</span>,title=<span class="string">'cost vs iters'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 更高级的subplots用法,绘制2*3个统计图</span></span><br><span class="line">fig,ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 第1行第一个图对应的实例名</span></span><br><span class="line">ax[<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="11-6-拟合函数可视化"><a href="#11-6-拟合函数可视化" class="headerlink" title="11.6 拟合函数可视化"></a>11.6 拟合函数可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(Y.min(),Y.max(),<span class="number">100</span>) <span class="comment"># 生成100个点，范围取决于Y,这是因为Y和X的范围差不多</span></span><br><span class="line">y_ = theta[<span class="number">0</span>,<span class="number">0</span>]+ theta[<span class="number">1</span>,<span class="number">0</span>]*x</span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line">ax.scatter(X[:,<span class="number">-1</span>],Y,label=<span class="string">'training data'</span>)</span><br><span class="line">ax.plot(x,y_,<span class="string">'r'</span>,label=<span class="string">'predict'</span>)</span><br><span class="line">ax.legend() <span class="comment"># 显示label</span></span><br><span class="line">ax.set(xlabel=<span class="string">'population'</span>,ylabel=<span class="string">'profit'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="11-7-多变量线性回归"><a href="#11-7-多变量线性回归" class="headerlink" title="11.7 多变量线性回归"></a>11.7 多变量线性回归</h3><p>因为不同特征取值范围相差巨大，所以首先我们需要对特征进行归一化。在实现中我们使用的是z-score normalization，量化后的特征服从标准正态分布：</p>
<script type="math/tex; mode=display">z = \frac{x_i-\mu}{\delta}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data-data.mean()/data.std()</span><br><span class="line">    <span class="comment"># mean为均值，std为标准差</span></span><br></pre></td></tr></table></figure>
<p>多α的cost-iters函数绘制方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig,ax = plt.subplots()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpha_list:</span><br><span class="line">    _,costs = gradientDescent(X,Y,theta,alpha,iters)</span><br><span class="line">    ax.plot(np.arange(iters),costs,label=alpha)</span><br><span class="line">    ax.legend()</span><br><span class="line">ax.set(xlabel=<span class="string">'iters'</span>,ylabel=<span class="string">'costs'</span>,title=<span class="string">'cost vs iters'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="11-8-正规方程解法"><a href="#11-8-正规方程解法" class="headerlink" title="11.8 正规方程解法"></a>11.8 正规方程解法</h3><p>我们求解的依据是</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><p>numpy.linalg模块包含线性代数的函数。使用这个模块，可以计算逆矩阵(inv)、求特征值、解线性方程以及求解行列式等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEquation</span><span class="params">(X,Y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T@X)@X.T@Y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<h2 id="12-向量化"><a href="#12-向量化" class="headerlink" title="12 向量化"></a>12 向量化</h2><h3 id="12-1-Octave基本操作"><a href="#12-1-Octave基本操作" class="headerlink" title="12.1 Octave基本操作"></a>12.1 Octave基本操作</h3><p>length A返回A矩阵中维度较大的维度。<br>分号意味着换行 C=[A B]将A放在B矩阵左边，行成矩阵C，C=[A； B]，将矩阵B放在矩阵A下面，行成矩阵C。<br>＊表示矩阵乘，.＊表示矩阵对应元素乘。通常情况下.表示对矩阵中每个元素进行处理。<br>对矩阵使用max，得到的结果是每一列的最大值。从第一维度操作，也就是按列操作取最大值，返回一行数据。第二维度操作，也就是按行取最大值，返回一列数据。<br>A＜3将会返回A矩阵中每个元素带入这个表达式得到的布尔值。<br>plt.savefig(“D:\machine learning\myplot.png”,dpi=600) 保存图片</p>
<h3 id="12-2-向量化操作"><a href="#12-2-向量化操作" class="headerlink" title="12 2 向量化操作"></a>12 2 向量化操作</h3><p><img src="/images/机器学习/向量化1.png" alt="向量化1"></p>
<h2 id="13-logistic回归-分类"><a href="#13-logistic回归-分类" class="headerlink" title="13 logistic回归-分类"></a>13 logistic回归-分类</h2><h3 id="13-1-logistic回归特点"><a href="#13-1-logistic回归特点" class="headerlink" title="13.1 logistic回归特点"></a>13.1 logistic回归特点</h3><p>我们的标签分为两种，我们用0来表示negative类(良性肿瘤，表示没有某样东西)，用1表示positive类(恶性肿瘤，表示具有某样东西)。<br>下图中，我们使用线性回归的思路对数据集拟合出了一个假设函数，并且根据实际情况给出了我们的判断依据。通过计算每个样本的h值，并将h值与0.5进行比较，我们可以得到预测的结果。<br><img src="/images/机器学习/logistic回归-分类1.png" alt="logistic回归-分类1"><br>但是当我们新增一个样本点位于右上角时，就会给我们的假设函数带来比较大的改变，即使是这个样本点并没有任何新增的信息(使用原来的h函数可以很好地预测这个样本点的标签)。<br>因此把线性回归的思想应用于分类问题并不是一个好主意，所以我们有了logistic回归算法，它的特点是0≤h≤1。logistic回归算法属于一种分类算法。</p>
<h3 id="13-2-假设陈述"><a href="#13-2-假设陈述" class="headerlink" title="13.2 假设陈述"></a>13.2 假设陈述</h3><p>在线性回归中，我们的假设函数h为：</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta^{T}x</script><p>对于逻辑回归，在此基础上修改，得到的假设函数h为：</p>
<script type="math/tex; mode=display">h_\theta(x)=g(\theta^{T}x),其中g(z)=\frac{1}{1+e^{-z}}</script><p>g(z)称为sigmoid function(激活函数)或是logistic function(逻辑函数)。它的函数图像如下：<br><img src="/images/机器学习/logistic回归-分类2.png" alt="logistic回归-分类2"><br>因此最终的假设函数为：</p>
<script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^{T}x}}</script><p>假设函数h(x)的输出为：对于一个输入x，y=1的概率估计。<br><img src="/images/机器学习/logistic回归-分类3.png" alt="logistic回归-分类3"><br>上图中，对于一个样本数据为x的患者，y=1的概率是0.7。这是一个后验概率。</p>
<h3 id="13-3-决策界限"><a href="#13-3-决策界限" class="headerlink" title="13.3 决策界限"></a>13.3 决策界限</h3><p>关于线性回归和逻辑回归的思考：<br>我们有一个二分类的样本，标签是0和1。首先根据样本做一个线性回归，得到一个假设函数h = θ^T<em>x,然后我们把这个假设函数h带入到激活函数中。激活函数输出的就是根据这个h函数的结果，这个标签y=1的概率是多少。阈值0.5，如果y=1的概率大于0.5，我们就预测这个输入的标签为1；小于0.5，就认为标签应该为0。所以激活函数的作用就是做一个概率评估，同时<br>添加非线性元素。<br><img src="/images/机器学习/logistic回归-分类4.png" alt="logistic回归-分类4"><br>在下图中，我们列出参数向量θ，然后应用上图结论：当θ^T</em>x≥0时，预测y=1。得到了一个不等式，也就是我们在分类问题中使用到的决策平面。<br><img src="/images/机器学习/logistic回归-分类5.png" alt="logistic回归-分类5"><br>决策边界是假设函数h的一个属性，即使我们改变了数据集，这条决策边界以及我们预测y=1和y=0的区域，他们都是假设函数的属性，取决于其参数θ，而不是数据集的属性。我们使用数据集来确定参数theta的取值，但是一旦我们有确定的参数取值，我们就将完全确定决策边界。我们不是用训练集来定义的决策边界，我们是用训练集来拟合参数θ(我理解就是决策边界的形状其实是取决于我们确定的拟合函数，而不是训练集)。<br>如下图，我们可以在h中添加额外的高阶多项式项。假如我们已经通过下节中的方法得到了θ向量的取值。然后我们使得h≥0，得到决策平面是一个以原点为圆心的圆。<br><img src="/images/机器学习/logistic回归-分类6.png" alt="logistic回归-分类6"></p>
<h3 id="13-4-代价函数"><a href="#13-4-代价函数" class="headerlink" title="13.4 代价函数"></a>13.4 代价函数</h3><p><img src="/images/机器学习/logistic回归-分类7.png" alt="logistic回归-分类7"><br>在线性回归模型中，我们定义了代价函数J，在逻辑回归中，我们的代价函数为：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})</script><script type="math/tex; mode=display">Cost(h_\theta(x^{(i)}),y^{(i)})=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>Cost函数的意义是在程序输出为h时，在实际标签为y的情况下，我们希望学习算法付出的代价。<br>但是如果我们按照这样的函数绘制出J-θ曲线，就会发现它是一个非凸函数，有很多局部最优值。如果我们对函数J使用梯度下降算法，不能保证他会收敛到全局最优值。我们希望代价函数是一个凸函数(单弓形函数)，只有一个局部最优值，这样我们可以保证梯度下降算法会收敛到该函数的全局最小值。<br>J(θ)函数之所以比较复杂的原因是因为在假设函数中引入了非线性的激活函数，且cost函数中还有平方操作，导致代价函数J有了很多个局部最优值。因此我们采用下图中的Cost函数带入到代价函数J中。<br><img src="/images/机器学习/logistic回归-分类8.png" alt="logistic回归-分类8"><br>1：h(x)，即1/(1+e^f(z))值域为(0,1)，(0,1)同时作为cost(h(x),y)的定义域<br>2.以拟合y=1举例，Cost(h(x),y)取-log(h(x))<br>3.当h(x)=1时,-log(1) = 0，h(x)=0,-log(0)趋向于正无穷<br>4.-log(x)在(0,1)内单调递减，不存在局部最小值，合理的用梯度下降一定能收敛到全局最小值。<br>下图是y=0时的Cost函数。<br><img src="/images/机器学习/logistic回归-分类9.png" alt="logistic回归-分类9"><br>当y=1时，如果h=1，也就是说算法认为P(y=1)=1,那么Cost函数趋近于正无穷。</p>
<h3 id="13-5-简化代价函数与梯度下降"><a href="#13-5-简化代价函数与梯度下降" class="headerlink" title="13.5 简化代价函数与梯度下降"></a>13.5 简化代价函数与梯度下降</h3><p>我们定义的代价函数J如下：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})</script><p>简化后的Cost函数如下,它来自于统计学中的极大似然估计，在统计学中用于为不同的模型快速寻找参数，是一个局部最优值等于最值的函数。</p>
<script type="math/tex; mode=display">Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)(1-h_\theta(x))</script><p><img src="/images/机器学习/logistic回归-分类10.png" alt="logistic回归-分类10"><br>接下来，我们的目标就是找出让损失函数J取得最小值的参数θ。我们使用梯度下降法。<br><img src="/images/机器学习/logistic回归-分类11.png" alt="logistic回归-分类11"><br><img src="/images/机器学习/logistic回归-分类12.png" alt="logistic回归-分类12"><br><img src="/images/机器学习/logistic回归-分类13.png" alt="logistic回归-分类13"><br>为了观察梯度下降算法是否正常运行，我们同样可以绘制J-θ曲线。同时对于范围相差比较大的特征，我们也可以使用特征缩放的方法。</p>
<h3 id="13-6-高级优化"><a href="#13-6-高级优化" class="headerlink" title="13.6 高级优化"></a>13.6 高级优化</h3><p>给定θ，我们计算损失函数J(θ)和对θj的偏导数(for j = 0,1,…,n),然后带入到θj的更新函数中。如下图：<br><img src="/images/机器学习/logistic回归-分类14.png" alt="logistic回归-分类14"><br>除了梯度下降法之外，我们还可以采用其他算法：<br>1 conjugate gradient(共轭梯度法)<br>2 BFGS<br>3 L-BFGS<br>这三种算法是在计算损失函数J(θ)和对θj的偏导数之后，使用比梯度下降法更复杂的算法来最小化代价函数。<br>他们的优点是：<br>1 不需要手动选择学习率α。我们可以认为他们有一个智能内循环(线搜索算法)，它可以尝试不同的学习速率并自动选择一个好的学习速率α。它甚至可以为每次迭代选择不同的学习速率。<br>2 收敛速度远远快于梯度下降。<br>缺点：比梯度下降法复杂。<br>在Octave中，我们可以使用内置的无约束最小化函数求解J(θ)的最小值。使用过程如下：<br><img src="/images/机器学习/logistic回归-分类15.png" alt="logistic回归-分类15"><br>因此，对于logistic回归算法，我们也可以使用内置的无约束最小化函数求解，前提是我们需要定义一个costFunction，它可以返回此时的J(θ)和θ的梯度向量。</p>
<h3 id="13-7-多元分类：一对多"><a href="#13-7-多元分类：一对多" class="headerlink" title="13.7 多元分类：一对多"></a>13.7 多元分类：一对多</h3><p>接下来我们谈到的是如何使用logistic回归来解决多元分类问题。我们会介绍一个“一对多”的分类算法。<br>多元分类问题是指我们算法输出的标签不是0和1两种，可能会有三个标签或是四个标签等等。多分类问题的思想是“一对其他”，即将某一类从其他类别数据集中划分出来。具体思路如下图：<br><img src="/images/机器学习/logistic回归-分类16.png" alt="logistic回归-分类16"><br>总之，对于类别i，我们训练一个逻辑回归分类器hi来预测y=i的概率。最终预测的结果是h最大的类别。</p>
<h2 id="14-过拟合"><a href="#14-过拟合" class="headerlink" title="14 过拟合"></a>14 过拟合</h2><h3 id="14-1-什么是过拟合"><a href="#14-1-什么是过拟合" class="headerlink" title="14.1 什么是过拟合"></a>14.1 什么是过拟合</h3><h4 id="14-1-1-线性回归中的拟合问题"><a href="#14-1-1-线性回归中的拟合问题" class="headerlink" title="14.1.1 线性回归中的拟合问题"></a>14.1.1 线性回归中的拟合问题</h4><p><img src="/images/机器学习/过拟合1.png" alt="过拟合1"><br>欠拟合-如果拟合一条直线，就好像算法有一个很强的偏见，或者说非常大的偏差，认为房子的价格与面积线性相关，而罔顾数据的不符，先入为主的拟合一条直线，最终导致拟合数据效果很差。<br>过拟合-拟合出来的曲线有很多的波动，即很高的方差。如果我们拟合一个高阶多项式，那么这个假设函数能拟合几乎所有的数据，这就可能面临假设函数太过庞大，变量太多的问题，我们没有足够的数据来约束它来获得一个很好的假设函数。<br>过拟合：如果我们有太多的特征，这时候训练得到的假设函数可以拟合训练集中的所有数据，但是它无法泛化(一个假设模型应用到新样本的能力)到新的样本中(没有出现在训练集中的样本)，那么就会有很高的方差(误差)。</p>
<h4 id="14-1-2-逻辑回归中的拟合问题"><a href="#14-1-2-逻辑回归中的拟合问题" class="headerlink" title="14.1.2 逻辑回归中的拟合问题"></a>14.1.2 逻辑回归中的拟合问题</h4><p><img src="/images/机器学习/过拟合2.png" alt="过拟合2"></p>
<h4 id="14-1-3-发现过拟合问题"><a href="#14-1-3-发现过拟合问题" class="headerlink" title="14.1.3 发现过拟合问题"></a>14.1.3 发现过拟合问题</h4><p>在单变量线性回归中，我们可以通过绘制假设模型曲线，观察曲线的形状，可以作为决定多项式阶次的一种方法。在多变量线性回归中，如果我们有过多的变量，而且只有非常少的训练数据，就会出现过度拟合的问题。且多变量问题我们的画图等数据可视化工作会比较困难。</p>
<h4 id="14-1-4-解决过拟合问题"><a href="#14-1-4-解决过拟合问题" class="headerlink" title="14.1.4 解决过拟合问题"></a>14.1.4 解决过拟合问题</h4><p>我们有两个办法来解决过拟合问题：<br>一 尽量减少选取变量的数量。<br>1.1具体而言，我们可以人工检查变量清单，并以此决定哪些变量更为重要，哪些特征变量应该保留，哪些应该舍弃。<br>1.2模型选择算法(在后续课程中会讲到)。这种算法可以自动选择哪些特征变量保留，哪些舍弃。这种减少特侦变量的方法，可以有效减少过拟合的发生。但它的缺点是舍弃一部分特征变量，同时也舍弃了关于问题的一些信息。例如，也许所有的特征变量，对于预测房价都是有用的，我们实际上并不想舍弃这些信息或者这些特征变量。<br>二 正则化。<br>我们将保留所有的特征变量，但是减少量级，也就是参数θj的大小。这种方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测y值产生一点影响。</p>
<h3 id="14-2-代价函数"><a href="#14-2-代价函数" class="headerlink" title="14.2 代价函数"></a>14.2 代价函数</h3><p>下图中，我们在过拟合的函数中加入惩罚项，使得参数θ3和θ4都非常小。具体方法为在损失函数J中加入和θ3、θ4大小有关的项，当我们最小化代价函数后，θ3、θ4都会趋近于0，就像我们直接去掉了这两项一样，最后我们的拟合结果就会是一个二次函数加上一个非常小的项，也就是一个适当的拟合结果。这就是正则化背后的思想。<br><img src="/images/机器学习/过拟合3.png" alt="过拟合3"><br>如果我们的参数值较小意味着：<br>1 一个更简单的假设函数(开头的例子)<br>2 如果参数的数值越小，我们得到的函数就会越平滑，也越简单。因此，也更不容易出现过拟合的问题。<br>例如我们在房价预测的过程中，可能有100个特征，我们就会有101个参数(多一个是因为我们加了一列全为1的特征)，但我们并不知道哪一个是高阶项，这使得我们很难预先挑选出其中的哪些参数来缩小它们的值。因此在正则化中，我们要做的就是修改线性回归的代价函数来缩小所有的参数(因为我们并不知道该选哪些参数去缩小)，我们将代价函数修改为下式，其中m是样本数，n是特征值个数，λ是正则化参数，控制两个不同目标之间的取舍(平衡关系)。我们的第一个目标就是想去训练一个假设函数能更好地拟合训练集(与损失函数的第一项有关)，第二个目标就是我们要保持参数尽量地小(与损失函数的第二项有关，与正则化目标有关)</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]</script><p>我们在代价函数中增加一个额外的正则化项来缩小每个参数的值。我们可以注意到θ是从θ1开始计算的，所以我们并没有给θ0增加惩罚项，这是约定俗成的。<br>我们使用正则化代价函数之后，假设函数的变化如下图：<br><img src="/images/机器学习/过拟合4.png" alt="过拟合4"><br>在正则化的线性回归中，如果正则化参数λ被设得太大的话，其结果就是我们对这些θ参数的惩罚程度过大，那么最后，这些参数都会接近于零，这样的话，就相当于把假设函数的全部项都忽略掉了，最后假设模型只剩一个θ0。于是我们最终的假设函数为h=θ0。这样就相当于我们直接用一条直线<br>去拟合数据，就会导致数据欠拟合。也就是这个假设模型的偏见性太强，与数据本身的趋势不符合，也与实际情况不符合。<br>为了让正则化起到应有的作用，我们应该去选择一个更合适的正则化参数λ。</p>
<h3 id="14-3-线性回归的正则化"><a href="#14-3-线性回归的正则化" class="headerlink" title="14.3 线性回归的正则化"></a>14.3 线性回归的正则化</h3><h4 id="14-3-1-使用正则化的梯度下降解法"><a href="#14-3-1-使用正则化的梯度下降解法" class="headerlink" title="14.3.1 使用正则化的梯度下降解法"></a>14.3.1 使用正则化的梯度下降解法</h4><p>我们之前推导了两种求解线性回归的方法，一种基于梯度下降，一种基于正规方程。在本节，我们将这两种方法推广到正则化线性回归中去。在上文中，我们得到了正则化代价函数，如下式：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]</script><p>在没有加入正则化参数时，梯度下降算法更新θ的方式如下图：<br><img src="/images/机器学习/过拟合5.png" alt="过拟合5"><br>在上图中，我们先单独更新了θ0，然后再更新了θj(j=1,n),这样做的目的是因为我们正则化线性回归的惩罚对象是参数θ1到θn，我们没有惩罚θ0，因此我们需要区别对待θ0。然后我们将θj的更新方式修改为图中最下面的式子。即加上了-αλθ_j/m，也就是在偏导数部分加上了λθ_j/m(Σθj^2/m对θj求偏导)，这实际上就是正则化代价函数J(θ)对θj的偏导数。<br>我们可以注意到，θj的系数(1-αλ/m)是一个只比1略小一点的数。这是因为通常学习率α很小，但m却很大，那么通常αλ/m会很小，所以(1-αλ/m)是一个只比1略小一点的数。所以θ更新的结果就是θj变成了θj*0.99，也就是把θj向0的方向缩小了一点点，这使得θj变小了一点点。然后θj更新中的第二项完全与我们在添加正则化项之前的梯度下降更新一样。<br>当我们在使用正则化线性回归时，我们要做的就是每次迭代时，都将θj乘以一个比1略小的数，我们每次都把参数缩小一点，然后进行和之前一样的更新操作。从数学角度来说，我们做的依然是对代价函数J(θ)使用梯度下降算法</p>
<h4 id="14-3-2-使用正则化的正规方程解法"><a href="#14-3-2-使用正则化的正规方程解法" class="headerlink" title="14.3.2 使用正则化的正规方程解法"></a>14.3.2 使用正则化的正规方程解法</h4><p>我们建立一个设计矩阵X，它的每一行都代表一个单独的训练样本。正规方程求解θ的关键是</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>现在我们在使用正规化的正规方程解法来得到我们想要的最小值。求解θ的式子如下图：<br><img src="/images/机器学习/过拟合6.png" alt="过拟合6"><br>在求解θ过程中可能会遇到不可逆的问题。如果我们的样本总数m≤样本的特征向量n，那么X^TX就是不可逆的，或者是一个奇异矩阵。但是我们可以求解它的伪逆矩阵，尽管我们可以得到最终的求解结果，但是这并不是一个很好的假设函数h。<br>参考同济大学线性代数矩阵的秩定理7，XTX的秩小于等于X的秩，X的秩&lt;=min(m,n),也就是说XTX的秩&lt;=n，所以XTX作为n+1阶行列式为0，矩阵不可逆。<br><img src="/images/机器学习/过拟合7.png" alt="过拟合7"><br>幸运的是，正则化的求解方法考虑到了这一情况，只要我们保证λ是大于0的，我们就可以确信上图中求逆矩阵的部分是可逆的。因此通过正则化这一操作，也可以解决一些X^TX出现不可逆的问题。这样我们就能很好地对一个很小的训练集但拥有大量的特征的训练集使用线性回归。</p>
<h3 id="14-4-Logistic回归的正则化"><a href="#14-4-Logistic回归的正则化" class="headerlink" title="14.4 Logistic回归的正则化"></a>14.4 Logistic回归的正则化</h3><p>对于逻辑回归，我们提到过两种优化方法，一种是使用梯度下降，另一种是更高级的优化方法。这些算法都需要你去想办法计算代价函数J(θ)，想办法去计算函数的导数。在本节中，我们将会展示如何改进这两种算法，使得它们都能够应用于正则化逻辑回归中去。</p>
<h4 id="14-4-1-使用正则化的梯度下降算法"><a href="#14-4-1-使用正则化的梯度下降算法" class="headerlink" title="14.4.1 使用正则化的梯度下降算法"></a>14.4.1 使用正则化的梯度下降算法</h4><p>前面13我们提到，逻辑回归也会遇到过拟合的情况，尤其是当我们使用高阶多项式去拟合数据的时候。通常情况下，如果逻辑回归中有很多特征，有无关紧要的多项式。这些大量的特征，最终会导致过拟合的现象。<br>在逻辑回归中，我们定义的代价函数如下：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})</script><script type="math/tex; mode=display">Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)(1-h_\theta(x))</script><p>为了使用正则化，我们需要对上式进行一些修改，拟合函数如下图：<br><img src="/images/机器学习/过拟合8.png" alt="过拟合8"><br>这样做的话，产生的效果就是，即使当你拟合阶数很高，且参数很多，只要添加了这个正则化项，保持参数较小，就仍然可以得到一条比较平滑的决策边界。使用正则化的效果就是即使我们有很多特征，正则化都可以帮助我们避免过拟合的现象。<br>我们添加正则化项之后的梯度下降算法如下图：<br><img src="/images/机器学习/过拟合9.png" alt="过拟合9"></p>
<h4 id="14-4-2-使用正则化的高级优化算法"><a href="#14-4-2-使用正则化的高级优化算法" class="headerlink" title="14.4.2 使用正则化的高级优化算法"></a>14.4.2 使用正则化的高级优化算法</h4><p>对于这些高级优化算法(13中)，我们需要自己定义一个costFunction函数，这个函数以参数向量θ作为输入。然后将他赋值给fminunc函数中的参数，fminunc函数的功能是在无约束条件下求最小值，它会将costFunction最小化。<br>costFunction会返回两个值，第一个是jVal，因此我们需要一部分代码来计算代价函数J(θ)，在下图中，表明了应用正则化方法代价函数J(θ)需要进行的改变。cost Function函数的另一个返回值是梯度向量。<br><img src="/images/机器学习/过拟合10.png" alt="过拟合10"><br>我们对costFunction函数调用fminunc函数或者其他类似的高级优化函数，这将最小化新的正则代价函数J(θ)，而函数的返回参数代表的就是正则化逻辑回归的解。<br>逻辑回归是线性分类器，接下来将会了解到一些非线性的分类器。<br><a href="https://www.bilibili.com/video/BV164411b7dx?p=43&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">学习进度</a></p>
<h2 id="15-逻辑回归实现"><a href="#15-逻辑回归实现" class="headerlink" title="15 逻辑回归实现"></a>15 逻辑回归实现</h2><h3 id="15-1-线性可分知识点回顾"><a href="#15-1-线性可分知识点回顾" class="headerlink" title="15.1 线性可分知识点回顾"></a>15.1 线性可分知识点回顾</h3><p>我们使用逻辑回归实现二分类。二分类问题又可以分为线性可分与线性不可分。线性可分是指我们可以大致使用一条直线对数据集进行划分。我们的第一个例子是线性可分的，第二个例子是线性不可分的。<br>现在我们先看第一个例子，我们的假设函数如下：</p>
<script type="math/tex; mode=display">h_\theta(x)=g(X\theta)</script><script type="math/tex; mode=display">h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)</script><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>激活函数g的作用是把假设函数h的值域从负无穷到正无穷压缩到了0到1之间。然后如果h≥0.5，那么输出y=1；如果h＜0.5，输出y=0。我们很容易发现，其实这与Xθ是大于0还是小于0有关。<br>我们将逻辑回归中的代价函数进行向量化，如下图：<br><img src="/images/机器学习/逻辑回归实现1.png" alt="逻辑回归实现1"><br>我们的梯度更新原理为：<br><img src="/images/机器学习/逻辑回归实现2.png" alt="逻辑回归实现2"></p>
<h3 id="15-2-逻辑回归-线性可分-数据可视化"><a href="#15-2-逻辑回归-线性可分-数据可视化" class="headerlink" title="15.2 逻辑回归-线性可分 数据可视化"></a>15.2 逻辑回归-线性可分 数据可视化</h3><p>根据学生的两门学习成绩，预测该学生是否会被大学录取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig,ax = plt.subplots()</span><br><span class="line"><span class="comment"># 过滤出所有被接收的点</span></span><br><span class="line">ax.scatter(data[data[<span class="string">'Accepted'</span>]==<span class="number">0</span>][<span class="string">'Exam 1'</span>],data[data[<span class="string">'Accepted'</span>]==<span class="number">0</span>][<span class="string">'Exam 2'</span>],c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>,label=<span class="string">'y=0'</span>)</span><br><span class="line">ax.scatter(data[data[<span class="string">'Accepted'</span>]==<span class="number">1</span>][<span class="string">'Exam 1'</span>],data[data[<span class="string">'Accepted'</span>]==<span class="number">1</span>][<span class="string">'Exam 2'</span>],c=<span class="string">'b'</span>,marker=<span class="string">'o'</span>,label=<span class="string">'y=1'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set(xlabel=<span class="string">'exam1'</span>,ylabel=<span class="string">'exam2'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="15-3-损失函数"><a href="#15-3-损失函数" class="headerlink" title="15.3 损失函数"></a>15.3 损失函数</h3><p>先定义激活函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br></pre></td></tr></table></figure>
<p>再定义损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunction</span><span class="params">(X,Y,theta)</span>:</span></span><br><span class="line">    inner = sigmoid(X@theta)</span><br><span class="line">    first = Y*np.log(inner)</span><br><span class="line">    second = (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-inner)</span><br><span class="line">    <span class="comment"># 累加每个样本点的误差</span></span><br><span class="line">    <span class="keyword">return</span> -np.sum(first+second)/len(X)</span><br></pre></td></tr></table></figure>
<h3 id="15-4-梯度下降、预测"><a href="#15-4-梯度下降、预测" class="headerlink" title="15.4 梯度下降、预测"></a>15.4 梯度下降、预测</h3><p>梯度更新函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,Y,theta,iters,alpha)</span>:</span></span><br><span class="line">    size = len(X)</span><br><span class="line">    costs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        theta = theta-X.T@(sigmoid(X@theta)-Y)*alpha/size</span><br><span class="line">        cost = costFunction(X,Y,theta)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">    <span class="keyword">return</span> costs,theta</span><br></pre></td></tr></table></figure>
<p>预测函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X,theta)</span>:</span></span><br><span class="line">    prob = sigmoid(X@theta)</span><br><span class="line">    <span class="comment"># numpy语法，list迭代器</span></span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x&gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> prob]</span><br></pre></td></tr></table></figure>
<p>计算精度的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_ = np.array(predict(X,theta_final))</span><br><span class="line">y_pre = y_.reshape(len(y_),<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 规定参与计算均值的元素的条件</span></span><br><span class="line">acc = np.mean(y_pre == Y)</span><br><span class="line">print(acc)</span><br></pre></td></tr></table></figure>
<h3 id="15-5-决策边界"><a href="#15-5-决策边界" class="headerlink" title="15.5 决策边界"></a>15.5 决策边界</h3><p>我们的决策边界是：</p>
<script type="math/tex; mode=display">\theta_0+\theta_1x_1+\theta_2x_2 = 0</script><p>在绘图时，我们把x1当成横轴，把x2当成纵轴，那么我们的决策边界函数为：</p>
<script type="math/tex; mode=display">x_2 = -\frac{\theta_0}{\theta_2}-\frac{\theta_1}{\theta_2}x_1</script><p>绘制决策边界的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">coef1 = -theta_final[<span class="number">0</span>,<span class="number">0</span>]/theta_final[<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">coef2 = -theta_final[<span class="number">1</span>,<span class="number">0</span>]/theta_final[<span class="number">2</span>,<span class="number">0</span>]</span><br><span class="line">x = np.linspace(<span class="number">20</span>,<span class="number">100</span>,<span class="number">100</span>) <span class="comment"># 在20到100之间，生成100个点</span></span><br><span class="line">f = coef1+coef2*x</span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line"><span class="comment"># 过滤出所有被接收的点</span></span><br><span class="line">ax.scatter(data[data[<span class="string">'Accepted'</span>]==<span class="number">0</span>][<span class="string">'Exam 1'</span>],data[data[<span class="string">'Accepted'</span>]==<span class="number">0</span>][<span class="string">'Exam 2'</span>],c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>,label=<span class="string">'y=0'</span>)</span><br><span class="line">ax.scatter(data[data[<span class="string">'Accepted'</span>]==<span class="number">1</span>][<span class="string">'Exam 1'</span>],data[data[<span class="string">'Accepted'</span>]==<span class="number">1</span>][<span class="string">'Exam 2'</span>],c=<span class="string">'b'</span>,marker=<span class="string">'o'</span>,label=<span class="string">'y=1'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set(xlabel=<span class="string">'exam1'</span>,ylabel=<span class="string">'exam2'</span>)</span><br><span class="line">ax.plot(x,f,c=<span class="string">'g'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="15-6-逻辑回归-线性不可分-特征映射"><a href="#15-6-逻辑回归-线性不可分-特征映射" class="headerlink" title="15.6 逻辑回归-线性不可分 特征映射"></a>15.6 逻辑回归-线性不可分 特征映射</h3><p>我们解决线性不可分问题的方法是特征映射。见数据挖掘理论算法学习24。<br>我们需要决定根据芯片在两次测试中的测试结果，决定芯片是要被接受还是抛弃。<br>特征映射代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_mapping</span><span class="params">(x1,x2,power)</span>:</span></span><br><span class="line">    <span class="comment"># 新建一个字典存放映射结果</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    <span class="comment"># 传入两个特征和要映射的阶次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(power+<span class="number">1</span>): <span class="comment"># 从0一直循环到power</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> np.arange(i+<span class="number">1</span>): <span class="comment"># 从0到i</span></span><br><span class="line">            data[<span class="string">'F&#123;&#125;&#123;&#125;'</span>.format(i-j,j)] = np.power(x1,i-j)*np.power(x2,j)</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(data) <span class="comment"># 将字典型数据转换成dataFrame结构</span></span><br><span class="line"></span><br><span class="line">x1 = data[<span class="string">'Test 1'</span>]</span><br><span class="line">x2 = data[<span class="string">'Test 2'</span>]</span><br><span class="line">data2 = feature_mapping(x1,x2,<span class="number">6</span>)</span><br><span class="line">data2.head()</span><br></pre></td></tr></table></figure>
<h3 id="15-7-加入正则化项的损失函数"><a href="#15-7-加入正则化项的损失函数" class="headerlink" title="15.7 加入正则化项的损失函数"></a>15.7 加入正则化项的损失函数</h3><p>我们构造数据集X时直接从data2中取出所需的特征即可，不需要对原数据集data进行切分，但对Y需要切片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = data2.values</span><br><span class="line">X.shape</span><br><span class="line">Y = data.iloc[:,<span class="number">-1</span>].values</span><br><span class="line">Y = Y.reshape(len(Y),<span class="number">1</span>)</span><br><span class="line">Y.shape</span><br></pre></td></tr></table></figure>
<p>在加入正则化项之后的损失函数表达式为：<br><img src="/images/机器学习/逻辑回归实现3.png" alt="逻辑回归实现3"><br>代码实现为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunction</span><span class="params">(X,Y,theta,lamda)</span>:</span></span><br><span class="line">    inner = sigmoid(X@theta)</span><br><span class="line">    first = Y*np.log(inner)</span><br><span class="line">    second = (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-inner)</span><br><span class="line">    <span class="comment"># 添加正则化项 对从1到后面所有的θ进行平方求和</span></span><br><span class="line">    reg = np.sum(np.power(theta[<span class="number">1</span>:],<span class="number">2</span>))*(lamda/(<span class="number">2</span>*len(X)))</span><br><span class="line">    <span class="keyword">return</span> -np.sum(first+second)/len(X)+reg</span><br><span class="line">theta = np.zeros((<span class="number">28</span>,<span class="number">1</span>))</span><br><span class="line">lamda = <span class="number">1</span></span><br><span class="line">cost_init = costFunction(X,Y,theta,lamda)</span><br><span class="line">print(cost_init)</span><br></pre></td></tr></table></figure>
<h3 id="15-8-加入正则化项的梯度下降、准确率"><a href="#15-8-加入正则化项的梯度下降、准确率" class="headerlink" title="15.8 加入正则化项的梯度下降、准确率"></a>15.8 加入正则化项的梯度下降、准确率</h3><p>在加入正则化项之后的梯度下降更新方式为：<br><img src="/images/机器学习/逻辑回归实现4.png" alt="逻辑回归实现4"><br>代码实现为;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,Y,theta,alpha,iters,lamda)</span>:</span></span><br><span class="line">    costs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        reg = theta[<span class="number">1</span>:]*lamda/len(X)</span><br><span class="line">        <span class="comment"># 因为我们不对θ0进行任何操作，所以我们在reg第一行之前插入一行0</span></span><br><span class="line">        <span class="comment"># 因为我们是对全部列操作，所以axis = 0</span></span><br><span class="line">        reg = np.insert(reg,<span class="number">0</span>,values=<span class="number">0</span>,axis = <span class="number">0</span>)</span><br><span class="line">        theta = theta-(X.T@(sigmoid(X@theta)-Y))*alpha/len(X)-alpha*reg</span><br><span class="line">        cost = costFunction(X,Y,theta,lamda)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> i% <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(cost)</span><br><span class="line">    <span class="keyword">return</span> theta,costs</span><br><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">iters = <span class="number">200000</span></span><br><span class="line">lamda = <span class="number">0.001</span></span><br><span class="line">theta_final,costs = gradientDescent(X,Y,theta,alpha,iters,lamda)</span><br></pre></td></tr></table></figure>
<p>准确率计算方式与线性可分的例子类似。</p>
<h3 id="15-9-决策平面"><a href="#15-9-决策平面" class="headerlink" title="15.9 决策平面"></a>15.9 决策平面</h3><p>绘制代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集本身的绘制</span></span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line">ax.scatter(data[data[<span class="string">'Accepted'</span>]==<span class="number">0</span>][<span class="string">'Test 1'</span>],data[data[<span class="string">'Accepted'</span>]==<span class="number">0</span>][<span class="string">'Test 2'</span>],c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>,label=<span class="string">'y=0'</span>)</span><br><span class="line">ax.scatter(data[data[<span class="string">'Accepted'</span>]==<span class="number">1</span>][<span class="string">'Test 1'</span>],data[data[<span class="string">'Accepted'</span>]==<span class="number">1</span>][<span class="string">'Test 2'</span>],c=<span class="string">'b'</span>,marker=<span class="string">'o'</span>,label=<span class="string">'y=1'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set(xlabel=<span class="string">'Test1'</span>,ylabel=<span class="string">'Test2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.contour(xx,yy,zz,0) 等高线是三维的，最后一个数据表示的是绘制的是数据为多少的等高线</span></span><br><span class="line">x = np.linspace(<span class="number">-1.2</span>,<span class="number">1.2</span>,<span class="number">200</span>) <span class="comment"># 生成范围为-1.2到1.2的200个数据</span></span><br><span class="line">xx,yy = np.meshgrid(x,x) <span class="comment"># np中生成网格的函数,生成200*200的矩阵 就是图中x1轴和x2轴的数据</span></span><br><span class="line">z = feature_mapping(xx.ravel(),yy.ravel(),<span class="number">6</span>).values<span class="comment"># 对xx和yy进行一个特征映射 ravel使得xx变成400*1的向量 最后加上values从dataframe变成array</span></span><br><span class="line"><span class="comment"># 我们需要的是zz=0的点，就是我们的分界面</span></span><br><span class="line">zz = z@theta_final</span><br><span class="line"><span class="comment"># 此时zz是40000*1维的，我们需要把他变成200*200方便画图</span></span><br><span class="line">zz = zz.reshape(xx.shape)</span><br><span class="line">plt.contour(xx,yy,zz,<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="16-神经网络"><a href="#16-神经网络" class="headerlink" title="16 神经网络"></a>16 神经网络</h2><h3 id="16-1-非线性假设"><a href="#16-1-非线性假设" class="headerlink" title="16.1 非线性假设"></a>16.1 非线性假设</h3><p>神经网络是一个相对古老的算法。与线性回归和逻辑回归相比，神经网络具有复杂的非线性假设。对于很多机器学习问题，特征个数n是很大的，那么我们就会得到更多的交叉项。因此只是包括平方项或者立方项特征的简单逻辑回归算法，并不是一个在n很大时学习复杂的非线性假设的好办法，因为有过多的特征参与。神经网络在学习复杂的非线性假设上被证明是一种好得多的算法，即使输入特征空间或n很大。</p>
<h3 id="16-2-模型展示-如何表示模型"><a href="#16-2-模型展示-如何表示模型" class="headerlink" title="16.2 模型展示-如何表示模型"></a>16.2 模型展示-如何表示模型</h3><p>神经网络的起源是人们尝试设计出模仿大脑的算法。神经元是一个计算单元，他从输入通道接收一定数目的信息并做一些计算，然后将结果通过它的输出通道传送到其他节点。<br>我们将神经元模拟成一个逻辑单元。如下图：<br><img src="/images/机器学习/神经网络1.png" alt="神经网络1"><br>神经网络实质上就是一组神经元连接在一起的集合。如下图：<br><img src="/images/机器学习/神经网络2.png" alt="神经网络2"><br>下面我们解释神经网络具体的计算步骤，其中：<br>ai^j表示第j层第i个神经元的激活项。激活项是指由一个具体的神经元计算并输出的值。<br>θj是一个权重矩阵，它控制从第j层到第j+1层的映射。θ上标表示第i层的特征矩阵，θ下标是矩阵row和column。第一层到第二层为θ1，第二层到第三层为θ2。θ1具体情况如下图：(请注意没有第一行，偏置单元没有权重)<br><img src="/images/机器学习/神经网络3.png" alt="神经网络3"><br>具体计算过程见下图：<br><img src="/images/机器学习/神经网络4.png" alt="神经网络4"><br>如果神经网络在j层有s(j)个单元，在j+1层有s(j+1)个单元,则θj矩阵的维度为s(j+1)×(s(j)+1)，因为j层还有一个x0。<br>一个人工神经网络定义了函数h，从输入x到输出y的映射。这些假设函数被参数化，我们将参数记为Θ，这样一来改变Θ，就能得到不同的假设函数，也就有了输入x到输出y的多种映射方式。</p>
<h3 id="16-3-神经网络模型向量化实现"><a href="#16-3-神经网络模型向量化实现" class="headerlink" title="16.3 神经网络模型向量化实现"></a>16.3 神经网络模型向量化实现</h3><p><img src="/images/机器学习/神经网络5.png" alt="神经网络5"><br>上图中的z，都是该层输入x0，x1，x2，x3的加权线性组合。z值可以通过矩阵向量乘法来得到。根据这个，我们就可以将神经网络的计算向量化了。为了便于统一表示，我们将输入层的接收到的输入看成是第一层的激活项(即第一层的计算结果)，例如我们定义a^1 = x,所以a^1就是一个向量了，所以有z^2 = Θ^1 a^1。<br>接下来我们额外加上一个a^2_0 = 1,注意偏置单元没有权重θ，因此θ2的维度为3*4，a^2的维度为4*1。<br>上图中右边为计算h(x)的过程，也被称为前向传播，这是因为我们从输入单元的激活项开始，然后进行前向传播给隐藏层，计算隐藏层的激活项，然后我们继续前向传播并计算输出层的激活项。这个依次计算激活项，从输入层到隐藏层再到输出层的过程叫前向传播，我们谈到的是这一过程的向量化实现方法。<br>这种前向传播的方法也能帮助我们理解神经网络的作用和它为什么能够帮助我们学习有趣的非线性假设函数。在下图所示的神经网络中，如果我们暂时遮盖住它的输入层，我们会发现图中剩下的部分看起来很像逻辑回归。也就是以汇点为逻辑回归单元来预测h(x)的值(即把左边三个神经元看成x，中间那个圈堪称是sigmoid函数)，我们的假设函数就是图中的h(x)。θ_{10}可以理解成上一级a0的计算结果在下一级a1中的权重。<br><img src="/images/机器学习/神经网络6.png" alt="神经网络6"><br>这实际上就是逻辑回归，但是逻辑回归模型中输入的特征，是通过隐藏层计算的这些数值。这个神经网络所做的事情，就像是逻辑回归，但是它不是使用原本的x1，x2，x3作为特征，而是使用a1，a2，a3作为新的特征。所以这本质上是一个特征工程。从原始数据X挖掘出一层比一层有用的特征A1, A2, …  最后使用逻辑回归线性拟合得出预测。而挖掘特征时使用的方法也是逻辑回归。<br>神经网络中神经元的连接方式，又称为神经网络的架构。架构是指不同的神经元的连接方式。如下图：<br><img src="/images/机器学习/神经网络7.png" alt="神经网络7"><br>第二层有三个隐藏神经元，它们会计算一些输入层的复杂功能，然后第三层可以将第二层训练出的特征项作为输入，然后在第三层计算出更复杂的特征。因此在第四层时就可以利用在第三层训练出的更复杂的特征作为输入，以此得到非常有趣的非线性假设函数。第一层被称为输入层，第四层被称为输出层，这个网络有两个隐藏层。</p>
<h3 id="16-4-含有一个神经元的网络"><a href="#16-4-含有一个神经元的网络" class="headerlink" title="16.4 含有一个神经元的网络"></a>16.4 含有一个神经元的网络</h3><p>在本节中，我们通过一个详细的例子来说明神经网络是怎样计算复杂非线性函数的输入的，为什么神经网络可以用来学习复杂的非线性假设模型。<br><img src="/images/机器学习/神经网络8.png" alt="神经网络8"><br>上图中，x1，x2的取值范围为0，1。左图可以看成是右图复杂机器学习问题的一个简化版本，我们的目标是去学习一个非线性的判断边界来区分这些正样本和负样本。</p>
<h4 id="16-4-1-拟合AND"><a href="#16-4-1-拟合AND" class="headerlink" title="16.4.1 拟合AND"></a>16.4.1 拟合AND</h4><p>为了建立能够拟合XNOR运算的神经网络，我们先从一个比较简单的能够拟合AND运算的网络入手。<br>假设有两个二进制的输入x1和x2，只能取0或1，现在目标函数y = x1 AND x2，and的意思是逻辑与。<br><img src="/images/机器学习/神经网络9.png" alt="神经网络9"><br>现在我们对上图网络中的权重或者说参数进行赋值，权重标在图中对应边上。由此我们可以写出图中底部位置的假设函数。把图中θ都想成是网络边界中连接着这些输入参数的权值。然后我们带入具体的x1和x2的值观察神经元的输出结果，见上图右侧部分。</p>
<h4 id="16-4-1-拟合OR"><a href="#16-4-1-拟合OR" class="headerlink" title="16.4.1 拟合OR"></a>16.4.1 拟合OR</h4><p>下图中的神经网络可以实现OR的功能。<br><img src="/images/机器学习/神经网络10.png" alt="神经网络10"><br>通过上面的例子我们可以明白神经网络中的单个神经元是如何被用来计算逻辑函数，事实上线性可分的问题可以用单个神经元完成，也就是一个逻辑回归。</p>
<h3 id="16-5-含有多个神经元的网络"><a href="#16-5-含有多个神经元的网络" class="headerlink" title="16.5 含有多个神经元的网络"></a>16.5 含有多个神经元的网络</h3><h4 id="16-5-1-not函数-非"><a href="#16-5-1-not函数-非" class="headerlink" title="16.5.1 not函数(非)"></a>16.5.1 not函数(非)</h4><p><img src="/images/机器学习/神经网络11.png" alt="神经网络11"><br>实现非逻辑的思想就是在预期得到非结果的变量前面放一个很大的负权重，如上图中的-20。</p>
<h4 id="16-5-2-xnor函数-异或非"><a href="#16-5-2-xnor函数-异或非" class="headerlink" title="16.5.2 xnor函数(异或非)"></a>16.5.2 xnor函数(异或非)</h4><p>在实现了与、或、与非的基础上，我们就可以实现xnor逻辑，具体实现如下图。很显然我们需要一个非线性的决策边界。<br><img src="/images/机器学习/神经网络12.png" alt="神经网络12"><br>我们的输入都放在输入层，然后在中间放一个隐藏层，用来计算一些关于输入的略微复杂的功能，然后再继续增加一层，用于计算一个更复杂的非线性函数，这就是为什么拥有很多层的神经网络可以计算这种复杂的函数。</p>
<h3 id="16-6-使用神经网络的多元分类"><a href="#16-6-使用神经网络的多元分类" class="headerlink" title="16.6 使用神经网络的多元分类"></a>16.6 使用神经网络的多元分类</h3><p>要在神经网络中实现多分类，采用的方法本质上是逻辑回归中一对多法的拓展，我们需要用到4个逻辑回归分类器。例如我们想要实现一个四分类模型，实现对行人、小汽车、摩托、卡车的分类，我们需要建立一个有四个输出单元的神经网络，即神经网络的输出为一个四维向量，我们要做的是利用第一个输出单元判断图中是否是一个行人，再用第二个输出单元来判断图片中是否是一辆汽车，再用第三个输出单元来判断是否是一辆摩托，最后用第四个输出单元来判断图中国是否是一辆货车。如果输入的是一个行人的照片，理想的输出是1 0 0 0，其他类别以此类推。<br><img src="/images/机器学习/神经网络13.png" alt="神经网络13"></p>
<h3 id="16-7-代价函数"><a href="#16-7-代价函数" class="headerlink" title="16.7 代价函数"></a>16.7 代价函数</h3><p>接下来我们来了解一个学习算法，它能在给定训练集时，为神经网络拟合参数。我们将从拟合神经网络参数的代价函数开始谈起。<br>我们问题的背景是神经网络在分类问题中的应用。<br>L表示神经网络中的总层数，对于上图的神经网络，L = 4。<br>s_l表示第l层的单元数，也就是神经元的数量，这其中不包括第l层的偏置神经元。例如，s_1 = 3,s_2 = 5。<br>我们考虑两种情况的分类问题，第一种是二元分类，这里y只能为0或是1，在这种情况下我们会有一个输出单元，也就是计算出来的h(x),它将会是一个。在这种情况下s_l是输出单元的个数，其中L代表最后一层的序号，s_l = 1,二分类问题的k值为1，也就等于输出层的单元数目。<br>第二种分类问题是多分类问题，也就是说会有k个不同的类。之前的例子中，如果我们有四类的话，我们就使用一个四维向量来表示y。那么k个输出单元(s_l = k,k≥3)，我们的假设函数就会输出k维的向量。只有k≥3时我们才需要使用这种一对多的方法，如果只有2个类别，我们只需要一个输出单元就可以了。<br>在神经网络中我们使用的代价函数是逻辑回归中使用的代价函数的一般形式。即y由一个数变成了一个向量，这是因为神经网络现在的输出变成了一个k维向量。代价函数见下图：<br><img src="/images/机器学习/神经网络14.png" alt="神经网络14"></p>
<h3 id="16-8-反向传播算法"><a href="#16-8-反向传播算法" class="headerlink" title="16.8 反向传播算法"></a>16.8 反向传播算法</h3><p>上一节中我们了解了神经网络的代价函数J(θ)，接下来我们寻找使得J(θ)最小化的方法。为了使用梯度下降算法或者其他某种高级优化算法，我们需要做的是写代码获得输入参数θ，并计算J(θ)和对θij的偏导项。J(θ)可以直接通过定义式计算得到。因此我们关注的地方是偏导数的计算。<br>首先我们再次解释一下神经网络前向传播的过程，如下图：<br><img src="/images/机器学习/神经网络15.png" alt="神经网络15"><br>向量化使得我们可以计算神经网络结构里的每一个神经元的激活值。接下来，为了计算导数项，我们将采用一种叫做反向传播(backpropagation)的算法。<br>反向传播算法从直观上说就是对每一个节点我们计算δ^l_j,就用这种方式代表了第l层的第j个节点的误差。前面我们提到，a^l_j表示的是第l层第j个单元的激活项。所以这个δ值，在某种程度上就捕捉了我们在这个神经节点的激活值的误差，所以我们可能希望这个节点的激活值稍微不一样。<br>具体地讲，我们用右边这个有四层的神经网络结构做例子。所以第4层第j个单元的δ^4_j = a^4_j - y_j,即该单元的激活值减去训练样本里的真实值，其中a^4_j也可以写成hθ(x)_j,这里的下标j是输出向量中第j个元素的值。如果我们把上式中的δ^4_j = a^4_j - y_j这三项都看成向量，式子就变成了δ^4 = a^4 - y，并且向量维度等于输出单元的数目。所以现在我们得到了第四层的误差项δ^4。δ^3的向量化计算方式如下：</p>
<script type="math/tex; mode=display">\delta^{(3)} = (\theta^{(3)})^{T}\delta^{(4)}.*g'(z^{(3)})</script><p>其中.*是点乘。g’(z^3)其实是对激活函数g在输入值为z(3)的时候所求的导数。因为激活函数的导数为</p>
<script type="math/tex; mode=display">\frac{\partial g(z)}{\partial z} = g(z)(1-g(z))</script><p>所以有：</p>
<script type="math/tex; mode=display">g'(z^{(3)}) = g(z^{(3)}).*(1-g(z^{(3)})) = a^{(3)}(1-a^{(3)})</script><p>其余δ的计算方式见下图：<br><img src="/images/机器学习/神经网络16.png" alt="神经网络16"><br>返校传播这个名字源于我们从输出从开始计算δ项，然后我们返回到上一层计算第三隐藏层的δ项，接着我们再往前一步来计算δ2，所以说我们是类似于把输出层的误差反向传播给了第3层，然后是再传到第2层，这就是反向传播的意思。<br>最终我们可以得到偏导数项的计算式为：</p>
<script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_{ij}} = a^{(l)}_j\delta^{(l+1)}_i</script><p>在上式中我们忽略了λ或者说标准化项，也就是λ项=0。我们将在之后完善正则项细节。<br><img src="/images/机器学习/神经网络17.png" alt="神经网络17"><br>在上图中，我们首先初始化偏导数数组Δij，将它的初值都设为0。我们打算使用这个数组来计算偏导数。所以这些δij被作为累加项慢慢地增加，以计算出所有的偏导数项。<br>解析来我们将遍历我们的训练集。对于第i个循环而言，我们将取第i个训练样本x^i和y^i。<br>于是我们要做的第一件事是设定a^1向量(即输入层的激活函数)，设定它等于x^i。<br>接下来我们运用正向传播，来计算第二层、第三层一直到第L层的激活值。<br>接下来，我们将用我们这个样本的标签值y^i来计算模型输出值对应的误差项δij，所以δ^L就是假设的输出值a^L减去模型的目标输出值y^i。<br>接下来，我们打算运用反向传播算法来计算δ(L-1)、δ(L-2)一直到δ(2),再强调一下，这里没有δ(1),因为我们不需要对输入层考虑误差项。<br>最后我们将用这些Δij来累加我们在前面计算好的篇导数项δij。我们也可以把这个过程向量化，具体来说，如果把δij看作一个矩阵，ij表示矩阵中的位置。如果δ(L)是一个矩阵，我们就可以把求累加和的操作写成图中红框部分的式子，实质上就是使得第i行δ乘上第j列a。其中δ(l+1)的维度是(第l+1层神经元的个数)<em>(第l+2层(或是要求输出的类)神经元的个数)，a(l)的维度是(第l层的神经元的个数)\</em>(第l+1层神经元的个数)，所以是满足矩阵乘法规则的。<br>最后我们跳出这个for循环，计算最下面的两个式子得到Dij，当j=0时，对应偏差项，当j≠0时，我们加入正则化项。Dij就是J(θ)关于每个参数的偏导数，然后我们可以使用梯度下降或者其他高级优化算法。</p>
<h3 id="16-9-理解反向传播"><a href="#16-9-理解反向传播" class="headerlink" title="16.9 理解反向传播"></a>16.9 理解反向传播</h3><p>在本小节中，我们更多地关注一下反向传播的一些固定步骤，说明这些固定步骤在做什么。<br>我们从前向传播开始，如下图：<br><img src="/images/机器学习/神经网络18.png" alt="神经网络18"><br>神经网络的前向传播过程如下图：<br><img src="/images/机器学习/神经网络19.png" alt="神经网络19"><br>事实上，下文中我们会发现，反向传播的过程和前向传播非常相似，只是这两个算法计算的方向不一样而已。在反向传播中，是从右往左进行运算，并且计算方法和前向传播非常相似。<br>为了更好地理解反向传播算法的过程，让我们先看看代价函数，我们关注于一个样本x^i,y^i,且神经网络只有一个输出单元并且忽略正则化项(λ=0)的情况。如下图：<br><img src="/images/机器学习/神经网络20.png" alt="神经网络20"><br>首先我们观察代价函数J(θ)对应于第i个样本的值，即中括号内的求和项。这个代价函数扮演一个类似方差的角色，我们可以把cost(i)近似地当成神经网络输出值h(x)与实际值y^i的方差。就像逻辑回归实际中会偏向于选择比较复杂的，带对数形式的代价函数，但为了方便理解，可以把这个代价函数看作是某种方差函数。因此cost(i)表示了神经网络预测样本的准确程度，也就是网络的输出值和实际观测值y^i的接近程度。</p>
<h4 id="16-9-1-δ项的意义"><a href="#16-9-1-δ项的意义" class="headerlink" title="16.9.1 δ项的意义"></a>16.9.1 δ项的意义</h4><p>如下图，反向传播算法就是在计算这些δ^l_j项，我们可以把它看作是我们在第l层中第j个单元中得到的激活项和目标值的误差。更深入地来说，δ项实际上是代价函数cost(i)关于z^l_j的偏导数，也就是计算出的上一层a项的加权和，或者说代价函数关于z项的偏导数。<br><img src="/images/机器学习/神经网络21.png" alt="神经网络21"><br>具体来说，代价函数是一个关于标签y和神经网络中h(x)的输出值的函数，如果分析网络的内部，稍微把z^l_j改一下，就会影响到神经网络的h(x),最终将改变代价函数的值。<br>综上，δ项实际上是代价函数关于这些计算得出的中间项的导数，他们衡量的是为了影响这些中间值，我们需要改变神经网络中的权重的修改程度，进而影响整个神经网络的输出h(x),并影响所有的代价函数。<br>举个例子：f(x,y)=5x+3y,则对x求偏导就是5，它得到的是x的系数，即这里的权重参数。</p>
<h4 id="16-9-2-δ项的计算方式"><a href="#16-9-2-δ项的计算方式" class="headerlink" title="16.9.2 δ项的计算方式"></a>16.9.2 δ项的计算方式</h4><p>如上图，对于输出层，如果我们设δ比如δ^4_1,则它的计算方式是y^i-a^4_1,然后我们进行反向传播，计算过程稍后再解释，最后可以计算出前一层的δ项，也就是δ^3_1,δ^3_2,然后继续进行传播，最后计算出δ^2_1,δ^2_2,反向传播的计算过程和前向传播非常地相似，只是方向反了过来。我们以计算δ^2_2为例，与前向传播相似，我们要先标出一对权重，δ^3_1的权重是θ^2(12)(在前向传播时用到了，即第2层<strong>第2个神经元</strong>到第3层<strong>第1个神经元</strong>的权重),δ^3_2的权重是θ^2(22)。然后计算δ^2_2我们要做的是用δ^3_1<em>θ^2(12)，δ^3_2\</em>θ^2(22),实际上就是后一层δ的加权和，加权是由对应边的强度来进行加权。<br>再来一个例子，我们要计算δ^3_2 = θ^3(12)*δ^4_1。<br>我们最后计算偏置单元的δ值，但是我们会忽略掉它们，并不使用，因为他们最后并不影响偏导数的计算。</p>
<h3 id="16-10-实现-展开参数"><a href="#16-10-实现-展开参数" class="headerlink" title="16.10 实现-展开参数"></a>16.10 实现-展开参数</h3><p><a href="https://www.bilibili.com/video/BV164411b7dx?p=53&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">学习进度</a></p>
<h2 id="17-逻辑回归多分类-amp-神经网络前向传播"><a href="#17-逻辑回归多分类-amp-神经网络前向传播" class="headerlink" title="17 逻辑回归多分类&amp;神经网络前向传播"></a>17 逻辑回归多分类&amp;神经网络前向传播</h2><p><a href="https://www.bilibili.com/video/BV1mt411p7kG/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">学习进度</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag"># 数据挖掘</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/01/%E5%85%B4%E8%B6%A3/%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="prev" title="生物数据挖掘学习">
      <i class="fa fa-chevron-left"></i> 生物数据挖掘学习
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-什么是机器学习"><span class="nav-number">1.</span> <span class="nav-text">1 什么是机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-机器学习的定义"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 机器学习的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-学习算法"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 学习算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-监督学习"><span class="nav-number">2.</span> <span class="nav-text">2 监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-回归问题"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 回归问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-分类问题"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-总结"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-无监督学习"><span class="nav-number">3.</span> <span class="nav-text">3 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-聚类"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-鸡尾酒会算法"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 鸡尾酒会算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-模型描述"><span class="nav-number">4.</span> <span class="nav-text">4 模型描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-代价函数"><span class="nav-number">5.</span> <span class="nav-text">5 代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-代价函数的定义"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 代价函数的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-代价函数的作用"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 代价函数的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-代价函数的作用-plus"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 代价函数的作用-plus</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-梯度下降"><span class="nav-number">6.</span> <span class="nav-text">6 梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-梯度下降算法步骤"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 梯度下降算法步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-梯度下降相关知识"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 梯度下降相关知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-更新函数解释"><span class="nav-number">6.2.1.</span> <span class="nav-text">6.2.1 更新函数解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-学习率解释"><span class="nav-number">6.2.2.</span> <span class="nav-text">6.2.2 学习率解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-线性回归的梯度下降"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 线性回归的梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-矩阵知识回顾"><span class="nav-number">7.</span> <span class="nav-text">7 矩阵知识回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-矩阵和向量定义"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 矩阵和向量定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-矩阵运算"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 矩阵运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-矩阵加法"><span class="nav-number">7.2.1.</span> <span class="nav-text">7.2.1 矩阵加法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-矩阵标量运算"><span class="nav-number">7.2.2.</span> <span class="nav-text">7.2.2 矩阵标量运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-3-矩阵之间的乘法"><span class="nav-number">7.2.3.</span> <span class="nav-text">7.3.3 矩阵之间的乘法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-逆和转置"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 逆和转置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-1-矩阵的逆"><span class="nav-number">7.3.1.</span> <span class="nav-text">7.3.1 矩阵的逆</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-2-矩阵的转置"><span class="nav-number">7.3.2.</span> <span class="nav-text">7.3.2 矩阵的转置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-多元线性回归"><span class="nav-number">8.</span> <span class="nav-text">8 多元线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-多元线性回归定义"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 多元线性回归定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-多元梯度下降法"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 多元梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-1-多元梯度下降法的算法流程"><span class="nav-number">8.2.1.</span> <span class="nav-text">8.2.1 多元梯度下降法的算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-2-多元梯度下降法——特征缩放"><span class="nav-number">8.2.2.</span> <span class="nav-text">8.2.2 多元梯度下降法——特征缩放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-3-多元梯度下降法——学习率"><span class="nav-number">8.2.3.</span> <span class="nav-text">8.2.3 多元梯度下降法——学习率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-特征和多项式回归"><span class="nav-number">9.</span> <span class="nav-text">9 特征和多项式回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-正规方程-区别于迭代方法的线性回归直接解法"><span class="nav-number">10.</span> <span class="nav-text">10 正规方程(区别于迭代方法的线性回归直接解法)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-正规方程求解流程"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 正规方程求解流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-正规方程在矩阵不可逆情况下的解决方法"><span class="nav-number">10.2.</span> <span class="nav-text">10.2 正规方程在矩阵不可逆情况下的解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-单变量线性回归实现"><span class="nav-number">11.</span> <span class="nav-text">11 单变量线性回归实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-读取文件"><span class="nav-number">11.1.</span> <span class="nav-text">11.1 读取文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-数据集准备"><span class="nav-number">11.2.</span> <span class="nav-text">11.2 数据集准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-损失函数"><span class="nav-number">11.3.</span> <span class="nav-text">11.3 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-4-梯度下降函数"><span class="nav-number">11.4.</span> <span class="nav-text">11.4 梯度下降函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-5-可视化损失函数"><span class="nav-number">11.5.</span> <span class="nav-text">11.5 可视化损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-6-拟合函数可视化"><span class="nav-number">11.6.</span> <span class="nav-text">11.6 拟合函数可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-7-多变量线性回归"><span class="nav-number">11.7.</span> <span class="nav-text">11.7 多变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-8-正规方程解法"><span class="nav-number">11.8.</span> <span class="nav-text">11.8 正规方程解法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-向量化"><span class="nav-number">12.</span> <span class="nav-text">12 向量化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-Octave基本操作"><span class="nav-number">12.1.</span> <span class="nav-text">12.1 Octave基本操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-向量化操作"><span class="nav-number">12.2.</span> <span class="nav-text">12 2 向量化操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-logistic回归-分类"><span class="nav-number">13.</span> <span class="nav-text">13 logistic回归-分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-logistic回归特点"><span class="nav-number">13.1.</span> <span class="nav-text">13.1 logistic回归特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-假设陈述"><span class="nav-number">13.2.</span> <span class="nav-text">13.2 假设陈述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-3-决策界限"><span class="nav-number">13.3.</span> <span class="nav-text">13.3 决策界限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-4-代价函数"><span class="nav-number">13.4.</span> <span class="nav-text">13.4 代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-5-简化代价函数与梯度下降"><span class="nav-number">13.5.</span> <span class="nav-text">13.5 简化代价函数与梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-6-高级优化"><span class="nav-number">13.6.</span> <span class="nav-text">13.6 高级优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-7-多元分类：一对多"><span class="nav-number">13.7.</span> <span class="nav-text">13.7 多元分类：一对多</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-过拟合"><span class="nav-number">14.</span> <span class="nav-text">14 过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-1-什么是过拟合"><span class="nav-number">14.1.</span> <span class="nav-text">14.1 什么是过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-1-线性回归中的拟合问题"><span class="nav-number">14.1.1.</span> <span class="nav-text">14.1.1 线性回归中的拟合问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-2-逻辑回归中的拟合问题"><span class="nav-number">14.1.2.</span> <span class="nav-text">14.1.2 逻辑回归中的拟合问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-3-发现过拟合问题"><span class="nav-number">14.1.3.</span> <span class="nav-text">14.1.3 发现过拟合问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-4-解决过拟合问题"><span class="nav-number">14.1.4.</span> <span class="nav-text">14.1.4 解决过拟合问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-代价函数"><span class="nav-number">14.2.</span> <span class="nav-text">14.2 代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-线性回归的正则化"><span class="nav-number">14.3.</span> <span class="nav-text">14.3 线性回归的正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-1-使用正则化的梯度下降解法"><span class="nav-number">14.3.1.</span> <span class="nav-text">14.3.1 使用正则化的梯度下降解法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-2-使用正则化的正规方程解法"><span class="nav-number">14.3.2.</span> <span class="nav-text">14.3.2 使用正则化的正规方程解法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-Logistic回归的正则化"><span class="nav-number">14.4.</span> <span class="nav-text">14.4 Logistic回归的正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-4-1-使用正则化的梯度下降算法"><span class="nav-number">14.4.1.</span> <span class="nav-text">14.4.1 使用正则化的梯度下降算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-4-2-使用正则化的高级优化算法"><span class="nav-number">14.4.2.</span> <span class="nav-text">14.4.2 使用正则化的高级优化算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-逻辑回归实现"><span class="nav-number">15.</span> <span class="nav-text">15 逻辑回归实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-线性可分知识点回顾"><span class="nav-number">15.1.</span> <span class="nav-text">15.1 线性可分知识点回顾</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-2-逻辑回归-线性可分-数据可视化"><span class="nav-number">15.2.</span> <span class="nav-text">15.2 逻辑回归-线性可分 数据可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-3-损失函数"><span class="nav-number">15.3.</span> <span class="nav-text">15.3 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-梯度下降、预测"><span class="nav-number">15.4.</span> <span class="nav-text">15.4 梯度下降、预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-5-决策边界"><span class="nav-number">15.5.</span> <span class="nav-text">15.5 决策边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-6-逻辑回归-线性不可分-特征映射"><span class="nav-number">15.6.</span> <span class="nav-text">15.6 逻辑回归-线性不可分 特征映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-7-加入正则化项的损失函数"><span class="nav-number">15.7.</span> <span class="nav-text">15.7 加入正则化项的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-8-加入正则化项的梯度下降、准确率"><span class="nav-number">15.8.</span> <span class="nav-text">15.8 加入正则化项的梯度下降、准确率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-9-决策平面"><span class="nav-number">15.9.</span> <span class="nav-text">15.9 决策平面</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-神经网络"><span class="nav-number">16.</span> <span class="nav-text">16 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#16-1-非线性假设"><span class="nav-number">16.1.</span> <span class="nav-text">16.1 非线性假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-模型展示-如何表示模型"><span class="nav-number">16.2.</span> <span class="nav-text">16.2 模型展示-如何表示模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-3-神经网络模型向量化实现"><span class="nav-number">16.3.</span> <span class="nav-text">16.3 神经网络模型向量化实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-4-含有一个神经元的网络"><span class="nav-number">16.4.</span> <span class="nav-text">16.4 含有一个神经元的网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-4-1-拟合AND"><span class="nav-number">16.4.1.</span> <span class="nav-text">16.4.1 拟合AND</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-4-1-拟合OR"><span class="nav-number">16.4.2.</span> <span class="nav-text">16.4.1 拟合OR</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-5-含有多个神经元的网络"><span class="nav-number">16.5.</span> <span class="nav-text">16.5 含有多个神经元的网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-5-1-not函数-非"><span class="nav-number">16.5.1.</span> <span class="nav-text">16.5.1 not函数(非)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-5-2-xnor函数-异或非"><span class="nav-number">16.5.2.</span> <span class="nav-text">16.5.2 xnor函数(异或非)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-6-使用神经网络的多元分类"><span class="nav-number">16.6.</span> <span class="nav-text">16.6 使用神经网络的多元分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-7-代价函数"><span class="nav-number">16.7.</span> <span class="nav-text">16.7 代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-8-反向传播算法"><span class="nav-number">16.8.</span> <span class="nav-text">16.8 反向传播算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-9-理解反向传播"><span class="nav-number">16.9.</span> <span class="nav-text">16.9 理解反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-9-1-δ项的意义"><span class="nav-number">16.9.1.</span> <span class="nav-text">16.9.1 δ项的意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-9-2-δ项的计算方式"><span class="nav-number">16.9.2.</span> <span class="nav-text">16.9.2 δ项的计算方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-10-实现-展开参数"><span class="nav-number">16.10.</span> <span class="nav-text">16.10 实现-展开参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#17-逻辑回归多分类-amp-神经网络前向传播"><span class="nav-number">17.</span> <span class="nav-text">17 逻辑回归多分类&amp;神经网络前向传播</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monica</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Monica</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
