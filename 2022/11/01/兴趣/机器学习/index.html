<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"monica-y.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="机器学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://monica-y.github.io/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="机器学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A03.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E6%8F%8F%E8%BF%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B01.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B02.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B03.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B04.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B05.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D3.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D4.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D5.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D6.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%923.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%924.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%925.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%926.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%921.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%922.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B1.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B2.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B3.png">
<meta property="og:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B01.png">
<meta property="article:published_time" content="2022-11-01T11:03:16.175Z">
<meta property="article:modified_time" content="2022-11-06T14:44:05.520Z">
<meta property="article:author" content="Monica">
<meta property="article:tag" content="数据挖掘">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://monica-y.github.io/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A01.png">

<link rel="canonical" href="https://monica-y.github.io/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monica-y.github.io/2022/11/01/%E5%85%B4%E8%B6%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monica">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-01 19:03:16" itemprop="dateCreated datePublished" datetime="2022-11-01T19:03:16+08:00">2022-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-06 22:44:05" itemprop="dateModified" datetime="2022-11-06T22:44:05+08:00">2022-11-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" itemprop="url" rel="index"><span itemprop="name">数据挖掘</span></a>
                </span>
            </span>

          
            <div class="post-description">机器学习笔记</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="https://github.com/TheisTrue/MLofAndrew-Ng/tree/master/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B2%E4%B9%89" target="_blank" rel="noopener">讲义地址</a></p>
<h2 id="1-什么是机器学习"><a href="#1-什么是机器学习" class="headerlink" title="1 什么是机器学习"></a>1 什么是机器学习</h2><h3 id="1-1-机器学习的定义"><a href="#1-1-机器学习的定义" class="headerlink" title="1.1 机器学习的定义"></a>1.1 机器学习的定义</h3><p>Arthur Samuel:在没有明确编程(设置)的情况下，使计算机具有学习能力的研究领域。<br>Tom Mitchell：如果一个计算机程序可以通过经验E使得其在处理任务T时获得更好的效果(这个效果的好坏用性能度量P来衡量),即通过P测定在T上的表现因E而提高。<br>对于机器人跳棋程序来说，经验E就是程序与人下几万次跳棋，任务T就是玩跳棋，性能度量就是与新对手玩跳棋时赢的概率。<br>对于垃圾邮件拦截程序来说，经验E就是观察人对不同邮件的分类，任务T就是将邮件分类为垃圾邮件和非垃圾邮件，性能度量就是正确归类的邮件比例。</p>
<h3 id="1-2-学习算法"><a href="#1-2-学习算法" class="headerlink" title="1.2 学习算法"></a>1.2 学习算法</h3><p>1 监督学习(supervised learning),主要思想就是我们会教计算机做某件事情。<br>2 无监督学习(unsupervised learning)，主要思想是让计算机自己学习。<br>其他热词：强化学习(reinforcement)，推荐系统(recommender system)<br>要点：在应用学习算法的实际建议。如果我想要开发机器学习系统，如何让那些最佳实践操作指导我的决定，用什么方式建立自己的系统。</p>
<h2 id="2-监督学习"><a href="#2-监督学习" class="headerlink" title="2 监督学习"></a>2 监督学习</h2><h3 id="2-1-回归问题"><a href="#2-1-回归问题" class="headerlink" title="2.1 回归问题"></a>2.1 回归问题</h3><p>假设我要预测房价，我收集到了一些现有的房价数据。想要知道750平米的房子的价格是多少呢？学习算法能做的一件事情就是根据数据画一条直线，或者说用一条直线拟合数据(粉色直线)。然后可以估计出房子的价值约为150k。除了用一条直线拟合数据，可以用二次函数或二阶多项式(蓝色曲线)。然后可以估计出房子的价值约为200k。<br><img src="/images/机器学习/监督学习1.png" alt="监督学习1"><br>监督学习是指我们给算法一个数据集，其中包含了正确答案。也就是说我们给它一个房价数据集，在这个数据集中的每个样本，我们都给出正确的价格，即这个房子实际卖价。算法的目的就是给出更多的正确答案。例如得到面积为750的房子的价格。<br>用更专业的术语来定义，它也被称为回归问题。这里的回归问题指的是我们想要预测连续的数值输出，也就是房子的价格。在技术上来说，价格可以精确到分，因此实际上是一个离散值，但通常我们认为房价是一个实数，即连续值。回归这个术语是指我们设法预测连续值的属性。</p>
<h3 id="2-2-分类问题"><a href="#2-2-分类问题" class="headerlink" title="2.2 分类问题"></a>2.2 分类问题</h3><p>假设我想看医疗记录并设法预测乳腺癌是恶性的还是良性的。假设某人发现了一个乳腺肿瘤，即乳房上的肿块，恶性肿瘤就是有害且危险的，良性肿瘤就是无害的。<br>假设在我的数据集中，横轴是肿瘤的尺寸，纵轴表示我们看到的肿瘤样本是否是恶性的。然后有一个肿瘤样本的大小位于粉色箭头处，我们想要估计出肿瘤是良性还有恶性的概率。这就是一个分类问题。<br>分类问题就是我们设法预测一个离散值输出。对于这个问题，就是输出是0还是1。实际上在分类问题中，有时也可以有两个以上的可能的输出值。在实际例子中就是，可能有三种类型的乳腺癌，因此，可能的输出就有0(良性)，1(癌症类型1)，2(癌症类型2)和3(癌症类型3)。<br><img src="/images/机器学习/监督学习2.png" alt="监督学习2"><br>在分类问题中，有另一种方法来绘制这些数据。如果肿瘤的大小是预测恶性或良性的特征，我们可以在一个坐标轴上(表示肿瘤大小的坐标轴)，用不同的符号来表示良性或恶性(第二行的图)。在这个例子中，我们只使用了一个特征(属性)，即肿瘤得大小。在其他得机器学习问题中，我们会有多个特征(属性)。假设我们不仅知道肿瘤的大小，还知道病人的年纪，在这种情况下的数据集可以表示为下图。如果有一个患者，他的情况位于图中粉点处。因此在给定的数据集上，学习算法能做的，就是在数据上画出一条直线(黑色直线)，设法将恶性瘤和良性瘤分开，算法可以通过这条直线判断肿瘤的类型。在这个例子中，我们有两种特征，即病人的年龄和肿瘤大小。在其他的机器学习算法中，往往会有更多的特征，例如肿块的厚度，肿瘤细胞大小的均匀性，肿瘤细胞形状的均匀性。<br><img src="/images/机器学习/监督学习3.png" alt="监督学习3"><br>最有趣的机器学习算法，是一个不仅仅能处理两到三个或五个特征，而是能处理无穷多特征的算法，即如何在计算机中存储无穷维度的数据，而不会溢出呢？以支持向量机算法为例，就有一个灵活的数学技巧，允许计算机处理无穷多的特征。</p>
<h3 id="2-3-总结"><a href="#2-3-总结" class="headerlink" title="2.3 总结"></a>2.3 总结</h3><p>在这节课上我们讨论了监督学习，想法是在监督学习中，对于数据集中的每个样本，我们想要算法预测，并得出“正确答案”。像是房子的价格，或肿瘤是恶性还是良性的。<br>我们也讨论了回归问题，回归是指我们的目标是预测一个连续值输出。<br>我们还讨论了分类问题，其目的是预测离散值输出。</p>
<h2 id="3-无监督学习"><a href="#3-无监督学习" class="headerlink" title="3 无监督学习"></a>3 无监督学习</h2><h3 id="3-1-聚类"><a href="#3-1-聚类" class="headerlink" title="3.1 聚类"></a>3.1 聚类</h3><p>在上一节的监督学习中，我们的数据集中每个样本都被标明为阳性样本或阴性样本，即良性或恶性肿瘤。对于监督学习中的每个样本，我们已经被清楚地告知了什么是所谓的正确答案，即它们是良性还是恶性。在无监督学习中，我们所用的数据集中没有任何标签，我们试图在其中找到某种结构。无监督学习可能判定该数据集包含两个不同的簇。无监督学习算法，可以把这些数据分成两个不同的簇，这就是聚类算法。<br><img src="/images/机器学习/无监督学习1.png" alt="无监督学习1"><br>一个应用聚类算法的例子就是谷歌新闻。谷歌新闻所做的就是每天去网络上收集几十万条新闻，然后将它们组成一个个新闻专题。本质上就是将它们分簇，有关同一主题的新闻，被显示在一起。<br>其实聚类算法和无监督学习算法，也可以用于基因检测中。基因检测用于测定特定基因的表达程度，我们可以对测序结果进行聚类。<br>无监督学习或聚类算法可以用来组织大型的计算机集群。尝试找到那些机器趋向于协同工作，如果在实际使用中让这些机器共同工作，则数据中心可以更高效地工作。<br>聚类算法可以用于社交网络分析。如果可以得到我Email最频繁地联系人，机器就可以自动识别同属于一个圈子的朋友，判断哪些人互相认识。<br>聚类算法可以用于市场分析。许多公司利用庞大的客户信息数据库，对客户进行分类，从而能够自动高效地使用不同地策略进行销售。<br>聚类算法可以被用于天文数据分析，用于星系形成理论的研究。</p>
<h3 id="3-2-鸡尾酒会算法"><a href="#3-2-鸡尾酒会算法" class="headerlink" title="3.2 鸡尾酒会算法"></a>3.2 鸡尾酒会算法</h3><p>聚类算法只是无监督学习的一种。<br>鸡尾酒会问题。有一个宴会，有一屋子的人，大家都坐在一起说话，因为每个人都同时在说话，有许多声音混杂在一起，几乎很难听清楚面前的人说话。假设一个鸡尾酒会上只有两个人，也就是只有两个人在同时说话。我们放下了两个麦克风来记录来自两人声音的不同组合。两个人在两个麦克风的记录中声音大小不同，但是都是两个人话语重合的声音。<br>我们把这两个声音交给鸡尾酒会算法，让它帮忙找出数据的结构。鸡尾酒会算法会分离出这两个被叠加到一起的声音。或者是说话的声音和背景音。算法的主要代码为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[W,s,v] &#x3D; svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;);</span><br></pre></td></tr></table></figure>
<p>在课程中使用的代码为Octave。svd函数全称为奇异值分解函数。</p>
<h2 id="4-模型描述"><a href="#4-模型描述" class="headerlink" title="4 模型描述"></a>4 模型描述</h2><p>回想2中预测房子价格的例子，根据已有的数据集，我们需要预测面积为1250的房子的价值。这是一个监督学习——对于数据集中的房子我们知道它们的面积和价格，这是一个回归问题——我们预测一个具体的数值输出。另一种常见的监督学习问题——分类问题，我们用它来预测离散值输出。<br>在建模过程中的符号如下：<br>m:表示训练样本的数量。<br>x’s:输入变量/特征。<br>y’s:输出变量/标签变量。<br>(x,y)表示一个训练样本。</p>
<script type="math/tex; mode=display">(x^{(i)},y^{(i)})</script><p>表示第i个训练样本，上标i是训练集的一个索引。<br>在上图中h表示假设函数，它的作用是把房子的大小作为输入变量，输出为相应房子的预测y值。h是一个从x到y的映射。我们将h表示为</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1*x,缩写为 h(x)</script><p><img src="/images/机器学习/模型描述1.png" alt="模型描述1"><br>从表达式可以看出，y是一个关于x的线性函数(这是因为我们要从最基础的拟合开始讲起)。我们将从这个例子开始，先拟合线性函数，然后在此基础上，最终处理更加复杂的模型，以及学习更复杂的学习算法。这个模型被称为线性回归，这个例子为一元线性回归(单变量线性回归)。</p>
<h2 id="5-代价函数"><a href="#5-代价函数" class="headerlink" title="5 代价函数"></a>5 代价函数</h2><h3 id="5-1-代价函数的定义"><a href="#5-1-代价函数的定义" class="headerlink" title="5.1 代价函数的定义"></a>5.1 代价函数的定义</h3><p>在这一节中，我们将定义代价函数的概念，帮助我们弄清楚如何把最有可能的直线与我们的数据相拟合。</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1*x,缩写为 h(x)</script><script type="math/tex; mode=display">\theta_{i's}为模型参数</script><p>我们选择不同的θ值，就会得到不同的假设函数。<br><img src="/images/机器学习/代价函数1.png" alt="代价函数1"><br>我们要做的就是得出θ0和θ1这两个参数的值，来让假设函数表示的直线尽量地与这些数据点很好的拟合。那么我们如何得出θ0和θ1的值来使它很好地拟合数据呢？我们要去选择能使h(x)也就是输入x时我们预测的值最接近该样本对应的y值的参数θ0和θ1。<br>在线性回归中，我们要解决的是一个最小化问题，使得下式的值最小。</p>
<script type="math/tex; mode=display">minimize\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>我们使用(x^(i),y^(i))表示第i个样本。m指的是训练集的样本容量。1/m 表示m个样本方差的均值，2是为了方便后面求导，其实是可以取任意实数，最终都会得出相同的θ0值和相同的θ1值，使得上式取得最小值。<br>我们将上述值定义成一个代价函数(cost function)如下：</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>其中，</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1*x</script><p>代价函数也被称作平方误差函数(Squard error function)，它可能是解决回归问题最常用的手段了。</p>
<h3 id="5-2-代价函数的作用"><a href="#5-2-代价函数的作用" class="headerlink" title="5.2 代价函数的作用"></a>5.2 代价函数的作用</h3><p><img src="/images/机器学习/代价函数2.png" alt="代价函数2"><br>实际上这包含了两个关键函数：<br>第一个是假设函数hθ(x),对于给定的θ，这是一个关于x的函数。<br>第二个是代价函数J(θ1)，是关于参数θ1的函数，它控制着h中直线的斜率。<br>当我们假设θ1=1时，我们求J(θ1)的值。</p>
<script type="math/tex; mode=display">J(\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">J(\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(\theta_1x^{(i)}-y^{(i)})^2</script><script type="math/tex; mode=display">J(\theta_1)=\frac{1}{2m}0^{2}+0^{2}+0^{2}=0^2</script><p>上述结果是在下图数据集的基础上得到的。<br><img src="/images/机器学习/代价函数3.png" alt="代价函数3"><br>同理我们可以计算当θ1=0.5时，J(0.5)=7/12。当θ1=0时，J(0)=7/3。最终J(θ1)的函数图像如下：<br><img src="/images/机器学习/代价函数4.png" alt="代价函数4"><br>对于每个θ1，我们可以得到一个假设函数h，也可以得到损失函数J在θ1处的取值J(θ1)。优化算法的目标是我们通过选择不同的θ1，获得最小的J(θ1)，这就是线性回归的目标函数。在上图中，当θ1=1时，损失函数取得最小值。</p>
<h3 id="5-3-代价函数的作用-plus"><a href="#5-3-代价函数的作用-plus" class="headerlink" title="5.3 代价函数的作用-plus"></a>5.3 代价函数的作用-plus</h3><p>在上面的讨论中，为了简化问题，我们将θ0=0，现在我们保留全部参数θ0和θ1。则它的损失函数如下：<br><img src="/images/机器学习/代价函数5.png" alt="代价函数5"><br>代价函数最小的点对应着更好的假设函数。</p>
<h2 id="6-梯度下降"><a href="#6-梯度下降" class="headerlink" title="6 梯度下降"></a>6 梯度下降</h2><h3 id="6-1-梯度下降算法步骤"><a href="#6-1-梯度下降算法步骤" class="headerlink" title="6.1 梯度下降算法步骤"></a>6.1 梯度下降算法步骤</h3><p>梯度下降是很常用的算法，它不仅悲用在线性回归上，还被广泛用于机器学习的众多领域。我们可以使用梯度下降的方法去最小化线性回归的代价函数J，或是其他函数。<br>我们有一个函数J(θ0，θ1)，我们的目标是最小化J。<br>梯度下降的思路：<br>1)先给定θ0和θ1的初始值。初值到底是多少其实并不重要，但是一般将θ0设为0，θ1也设为0。<br>2)不停地一点点地改变θ0和θ1，来使得J变小。直到我们找到J的最小值，或者局部最小值。<br>梯度下降的一个有趣的地方在于，从不同的初始值出发，可能得到不同的局部最优解。<br>我们将会一直重复下面操作，直到收敛。</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)(for j=1 and j=1)</script><p>在上式中，:=表示赋值。在Pascal中，a=b表示我断言a和b相等。α被称为学习率，α用来控制梯度下降时，我们迈出多大的步子。如果α很大，梯度下降就很迅速，我们会用大步子下山(损失函数值)。如果α很小，那么我们会迈着很小的小碎步下山。<br>实现梯度下降算法的微妙之处是，对于上述更新方程，我们需要<strong>同时更新</strong>θ0和θ1。具体方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp0 = theta_0-alpha*dtheta0J</span><br><span class="line">temp1 = theta_1-alpha*dtheta1J</span><br></pre></td></tr></table></figure>
<p>同步更新是更自然的实现方法。</p>
<h3 id="6-2-梯度下降相关知识"><a href="#6-2-梯度下降相关知识" class="headerlink" title="6.2 梯度下降相关知识"></a>6.2 梯度下降相关知识</h3><h4 id="6-2-1-更新函数解释"><a href="#6-2-1-更新函数解释" class="headerlink" title="6.2.1 更新函数解释"></a>6.2.1 更新函数解释</h4><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)(for j=1 and j=1)</script><p>α控制我们以多大幅度更新这个参数θj。为了能更好地理解这个式子，我们假设想要最小化的函数只有一个参数的情形。假如我们有一个代价函数J，只有一个参数θ1(同5中简化情况)。<br>假设此时我们的损失函数J(θ1)如下图，<br><img src="/images/机器学习/梯度下降1.png" alt="梯度下降1"></p>
<h4 id="6-2-2-学习率解释"><a href="#6-2-2-学习率解释" class="headerlink" title="6.2.2 学习率解释"></a>6.2.2 学习率解释</h4><p>如果α过小(下图中上半部分情况)，就需要很多步才能到达最低点。<br>如果α过大(下图中下半部分情况)，那么梯度下降可能会越过最低点，甚至可能无法收敛。<br><img src="/images/机器学习/梯度下降2.png" alt="梯度下降2"><br>而如果θ1恰好初始化在局部最优点，那么梯度下降算法不会改变任何参数值。<br>梯度下降算法每次调整参数的大小不仅与规定的学习率α有关，也与当前θ点处的导数大小有关，越接近最低点，函数变化越平缓，导数越小，梯度下降算法调整参数的幅度就越小。所以实际上没有必要再另外减小α。<br><img src="/images/机器学习/梯度下降3.png" alt="梯度下降3"></p>
<h3 id="6-3-线性回归的梯度下降"><a href="#6-3-线性回归的梯度下降" class="headerlink" title="6.3 线性回归的梯度下降"></a>6.3 线性回归的梯度下降</h3><p>本节中我们将梯度下降和线性回归中的代价函数结合，得到线性回归的算法。<br><img src="/images/机器学习/梯度下降4.png" alt="梯度下降4"><br><img src="/images/机器学习/梯度下降5.png" alt="梯度下降5"><br>在求得偏导之后，我们带入上文中梯度下降的算法框架，得到<br><img src="/images/机器学习/梯度下降6.png" alt="梯度下降6"><br>梯度下降容易陷入局部最优。但是线性回归的代价函数是一个碗状函数(二元)，术语叫凸函数，这个函数没有局部最优解，只有一个全局最优。当计算这种代价函数的梯度下降时，只要使用线性回归，它总会收敛到全局最优。<br>上文中我们讨论的梯度下降算法，被称为Batch梯度下降，Batch指的是每一步梯度下降，我们都遍历了整个训练集的样本。在梯度下降计算偏导数时，我们计算总和。还会有其他不是Batch模式的梯度下降算法，它没有全览整个训练集，每次只关注了小子集。<br>求解代价函数的最小值，我们还有最小二乘法(正规方程组解法)，但是相对于此方法，梯度下降法适合更大的数据集。</p>
<h2 id="7-矩阵知识回顾"><a href="#7-矩阵知识回顾" class="headerlink" title="7 矩阵知识回顾"></a>7 矩阵知识回顾</h2><h3 id="7-1-矩阵和向量定义"><a href="#7-1-矩阵和向量定义" class="headerlink" title="7.1 矩阵和向量定义"></a>7.1 矩阵和向量定义</h3><p>矩阵是指由数字组成的矩形数组。矩阵的维数：行数×列数。</p>
<script type="math/tex; mode=display">R^{4\times 2}代表所有4\times 2的矩阵的集合</script><p>矩阵元素Aij表示矩阵中A中第i行第j列的元素。矩阵提供了一种很好的方式帮助我们快速整理、索引和访问大量数据。一般我们用大写字母表示矩阵，用小写字母表示数字/标量/向量。<br>一个向量时一种特殊的矩阵，向量是只有一列的矩阵，即维数为n×1，表示这是一个n维向量。R^4表示一个四维向量的集合。向量元素yi表示向量y的第i个元素。i可以从1开始(默认)，也可以从0开始。</p>
<h3 id="7-2-矩阵运算"><a href="#7-2-矩阵运算" class="headerlink" title="7.2 矩阵运算"></a>7.2 矩阵运算</h3><h4 id="7-2-1-矩阵加法"><a href="#7-2-1-矩阵加法" class="headerlink" title="7.2.1 矩阵加法"></a>7.2.1 矩阵加法</h4><p>只有相同维度的矩阵才可以相加。</p>
<h4 id="7-2-2-矩阵标量运算"><a href="#7-2-2-矩阵标量运算" class="headerlink" title="7.2.2 矩阵标量运算"></a>7.2.2 矩阵标量运算</h4><p>矩阵和标量的乘法运算。标量代表一个数字，或者实数。<br>3乘以矩阵A，等于将矩阵A中所有元素都逐一与3相乘。得到的结果是相同维度的矩阵。且乘法满足交换律。<br>矩阵A除以4，等同于0.25乘以矩阵A。</p>
<h4 id="7-3-3-矩阵之间的乘法"><a href="#7-3-3-矩阵之间的乘法" class="headerlink" title="7.3.3 矩阵之间的乘法"></a>7.3.3 矩阵之间的乘法</h4><p>矩阵相乘的前提是A矩阵的列数和B矩阵的行数相同。计算Aij的方法为让A的第i行元素分别乘以B中第j列的元素，并且相加起来。<br>我们可以利用矩阵乘法实现房价预测。<br><img src="/images/机器学习/矩阵知识回顾1.png" alt="矩阵知识回顾1"><br>我们可以利用矩阵乘法实现更为细致的房价预测。<br><img src="/images/机器学习/矩阵知识回顾2.png" alt="矩阵知识回顾2"><br>矩阵乘法的性质：<br>1)矩阵乘法不满足交换律。<br>2)矩阵乘法满足结合律。<br>3)幺元。在实数空间，1是一个乘法单位。在矩阵空间，单位矩阵是一个乘法单位，通常记作I(n×n)。对于A(m×n)矩阵来说，IA=AI=A，其中两个I的维度是不相同的，第一个是m×m，第二个是n×n。</p>
<h3 id="7-3-逆和转置"><a href="#7-3-逆和转置" class="headerlink" title="7.3 逆和转置"></a>7.3 逆和转置</h3><h4 id="7-3-1-矩阵的逆"><a href="#7-3-1-矩阵的逆" class="headerlink" title="7.3.1 矩阵的逆"></a>7.3.1 矩阵的逆</h4><p>在实数空间，每一个实数(除0外)都有一个倒数。在矩阵空间，如果矩阵A有逆矩阵A^(-1)，有</p>
<script type="math/tex; mode=display">AA^{-1}=A^{-1}A=I</script><p>如果一个矩阵的行数和列数相同，那么称这个矩阵为方阵。只有满秩的方阵才有逆矩阵。我们把没有逆矩阵的矩阵称为“奇异矩阵”或是“退化矩阵”。python求解逆矩阵的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a  = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])  <span class="comment"># 初始化一个非奇异矩阵(数组)</span></span><br><span class="line">print(np.linalg.inv(a))  <span class="comment"># 对应于MATLAB中 inv() 函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵对象可以通过 .I 更方便的求逆</span></span><br><span class="line">A = np.matrix(a)</span><br><span class="line">print(A.I)</span><br></pre></td></tr></table></figure>
<h4 id="7-3-2-矩阵的转置"><a href="#7-3-2-矩阵的转置" class="headerlink" title="7.3.2 矩阵的转置"></a>7.3.2 矩阵的转置</h4><p>A矩阵经过转置生成B矩阵，其特点是Aij=Bji。</p>
<h2 id="8-多元线性回归"><a href="#8-多元线性回归" class="headerlink" title="8 多元线性回归"></a>8 多元线性回归</h2><h3 id="8-1-多元线性回归定义"><a href="#8-1-多元线性回归定义" class="headerlink" title="8.1 多元线性回归定义"></a>8.1 多元线性回归定义</h3><p>在训练中我们会用到如下的符号：<br><img src="/images/机器学习/多元线性回归1.png" alt="多元线性回归1"><br>当我们有了多个特征量之后，我们的假设函数也需要相应变化。</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4</script><p>然后我们引入矩阵乘法：<br><img src="/images/机器学习/多元线性回归2.png" alt="多元线性回归2"></p>
<h3 id="8-2-多元梯度下降法"><a href="#8-2-多元梯度下降法" class="headerlink" title="8.2 多元梯度下降法"></a>8.2 多元梯度下降法</h3><h4 id="8-2-1-多元梯度下降法的算法流程"><a href="#8-2-1-多元梯度下降法的算法流程" class="headerlink" title="8.2.1 多元梯度下降法的算法流程"></a>8.2.1 多元梯度下降法的算法流程</h4><p>通过上文的描述，我们可以把多元线性回归的模型参数看作是一个n+1维的向量θ。多元梯度下降法的算法流程如下：<br><img src="/images/机器学习/多元线性回归3.png" alt="多元线性回归3"></p>
<h4 id="8-2-2-多元梯度下降法——特征缩放"><a href="#8-2-2-多元梯度下降法——特征缩放" class="headerlink" title="8.2.2 多元梯度下降法——特征缩放"></a>8.2.2 多元梯度下降法——特征缩放</h4><p>如果一个机器学习问题中有多个特征，且我能确保不同特征的取值都处在一个相近的范围内，这样梯度下降算法能更快地收敛。<br><img src="/images/机器学习/多元线性回归4.png" alt="多元线性回归4"><br>使用特征缩放方法将特征的取值约束到-1到+1之间，也不一定是-1到+1，只需要不同特征的取值范围相近，梯度下降算法就可以正常地工作。<br>除了上图中将特征除以最大值的方法外，在特征缩放中，有时我们也会对特征进行均值归一化的工作。具体方法如下：</p>
<script type="math/tex; mode=display">x_i = \frac{x_i-\mu_i}{max-min}</script><p>需要注意的是，我们不会对x0进行均值归一化的操作。上式的分母部分也可以是xi的标准差。</p>
<h4 id="8-2-3-多元梯度下降法——学习率"><a href="#8-2-3-多元梯度下降法——学习率" class="headerlink" title="8.2.3 多元梯度下降法——学习率"></a>8.2.3 多元梯度下降法——学习率</h4><p>为了保证梯度下降算法正确工作，我们可以绘制算法迭代次数-minJ(θ)的折线图，来展示梯度下降算法每次迭代之后，代价函数的值。如果算法正确运行，那么每一步迭代之后J(θ)都应该下降。当算法迭代到300-400次之间时，J(θ)的值已经下降的非常缓慢了。当算法迭代到400次时，从下图中可以看出已经差不多收敛了，因为代价函数值没有再继续下降了。<br><img src="/images/机器学习/多元线性回归5.png" alt="多元线性回归5"><br>也可以采用自动收敛测试的办法：如果在一次迭代中J(θ)减小的幅度小于10^(-3)则认为算法收敛。但是想要寻找到一个合适的阈值是困难的。一般情况下我们可以通过上图中的折线图判断。<br>在使用梯度下降算法时，如果迭代次数-minJ的变化趋势如上图，则可以说明梯度下降算法在正常工作，反之，随着迭代次数的增加，J在不断上升或是波动，则证明梯度下降算法没有正常工作。此时，通常意味着算法应该使用较小的学习率α。<br>只要α足够小，那么每次迭代之后代价函数J都会下降。但是学习率不可以国小，那样的话梯度下降算法可能收敛得很慢。<br><img src="/images/机器学习/多元线性回归6.png" alt="多元线性回归6"><br>总结一下，如果学习率α太小的话，算法收敛速度会较慢；如果学习率α太大的话，代价函数J可能不会在每次迭代都下降，甚至可能不收敛。在一些情况下，如果α值太大，也可能会出现收敛缓慢的问题，但并不常见。为了了解算法的运行情况，通常需要绘制代价函数J随迭代步数变化的曲线。<br>尝试一系列α值，0.001，0.003，0.01，0.03，0.1，0.3，1……然后通过绘制J随迭代次数变化的曲线，选择一个使得J快速下降的α值。</p>
<h2 id="9-特征和多项式回归"><a href="#9-特征和多项式回归" class="headerlink" title="9 特征和多项式回归"></a>9 特征和多项式回归</h2><p>在房价预测中，我们可以将房子的长和宽两个特征合并为房子的面积这一个特征。这样就将特征从两个减少到一个。根据数据趋势，我们打算使用三次函数对特征进行拟合，如下图。<br><img src="/images/机器学习/特征和多项式回归1.png" alt="特征和多项式回归1"><br>事实上，对于特征和假设函数，我们有更多的选择。<br><img src="/images/机器学习/特征和多项式回归2.png" alt="特征和多项式回归2"></p>
<h2 id="10-正规方程-区别于迭代方法的线性回归直接解法"><a href="#10-正规方程-区别于迭代方法的线性回归直接解法" class="headerlink" title="10 正规方程(区别于迭代方法的线性回归直接解法)"></a>10 正规方程(区别于迭代方法的线性回归直接解法)</h2><p>在上面我们了解了求minJ的梯度下降解法。正规方程提供了一种求θ的解析解法，使得我们不再需要运行迭代算法，而是可以直接一次性地求解θ的最优值。</p>
<h3 id="10-1-正规方程求解流程"><a href="#10-1-正规方程求解流程" class="headerlink" title="10.1 正规方程求解流程"></a>10.1 正规方程求解流程</h3><p><img src="/images/机器学习/正规方程求解流程1.png" alt="正规方程求解流程1"><br>下面是具体计算过程。<br><img src="/images/机器学习/正规方程求解流程2.png" alt="正规方程求解流程2"><br>设计矩阵X的构造方法如下图。<br><img src="/images/机器学习/正规方程求解流程3.png" alt="正规方程求解流程3"><br>求解θ的关键是</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>如果我们使用正规方程法求解θ，那么就不需要特征缩放。但特征缩放对于梯度下降法十分重要。<br>梯度下降法和标准方程法的适用范围：<br>梯度下降法(适用于特征较多，特征数为上万个)：<br>1 需要选择学习速率α。我们需要运行多次，尝试不同的α，找到运行效果最好的那个。<br>2 需要多次迭代。需要绘制迭代次数和损失函数J的曲线来检查收敛性。<br>3 在特征很多的情况下也能运行地相当好。<br>标准方程法(适用于特征较少，特征数为几千个)：<br>1 不需要选择学习速率α。<br>2 不需要迭代。<br>3 需要计算inv(X^T X),计算结果的维度为特征数(n+1)*(n+1),而逆矩阵的计算复杂度为O(n^3),当特征比较多时会比较慢。<br>当样本数据的特征数小于1万时，通常使用正规方程法进行线性回归，而不是用梯度下降法。</p>
<h3 id="10-2-正规方程在矩阵不可逆情况下的解决方法"><a href="#10-2-正规方程在矩阵不可逆情况下的解决方法" class="headerlink" title="10.2 正规方程在矩阵不可逆情况下的解决方法"></a>10.2 正规方程在矩阵不可逆情况下的解决方法</h3><p>在上文中，我们通过下式计算θ。</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>但是当XX^T不可逆时，也就是结果为奇异矩阵(或退化矩阵)时，正规方程解法可能会出现问题。在python中，我们可以通过计算矩阵的伪逆来解决这一问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个奇异阵 A 4×4</span></span><br><span class="line">A = np.zeros((<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># 第一行，倒数第一个元素为1</span></span><br><span class="line">A[<span class="number">0</span>, <span class="number">-1</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment"># 倒数第一行，第一个元素为-1</span></span><br><span class="line">A[<span class="number">-1</span>, <span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"><span class="comment"># 将A转化成矩阵</span></span><br><span class="line">A = np.matrix(A)</span><br><span class="line">print(A)</span><br><span class="line"><span class="comment"># print(A.I)  将报错，矩阵A为奇异矩阵，不可逆</span></span><br><span class="line">print(np.linalg.pinv(A))   <span class="comment"># 求矩阵A的伪逆（广义逆矩阵），对应于MATLAB中pinv()函数</span></span><br></pre></td></tr></table></figure>
<p>XX^T矩阵不可逆的原因：<br>1 数据集中包含了多余的特征(线性相关的特征)。此时我们需要删除掉一部分线性相关的特征。如area和length，area= length*length，我们可以删掉area这个特征。<br>2 数据集中包含了太多的特征。例如数据集中一共只有4个样本，但是却有40个特征。此时我们需要删除掉一些特征，或使用正则化的方法。正则化方法可以帮助我们使用很多的特征来配置很多参数，即使我们有一个相对较小的训练集。<br><a href="https://www.bilibili.com/video/BV164411b7dx?p=26&amp;vd_source=3fbda25a4b0f2f754f5986883bf96612" target="_blank" rel="noopener">学习进度</a><br>00:35</p>
<h2 id="11-单变量线性回归实现"><a href="#11-单变量线性回归实现" class="headerlink" title="11 单变量线性回归实现"></a>11 单变量线性回归实现</h2><p>题目：假设你是一家餐厅的CEO，正在考虑开一家分店，根据该城市的人口数据预测其利润。<br>我们对梯度下降的算法进行向量化，可以得到每步更新的θ表达式为</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}X^T(X\theta-Y)</script><p>具体推到过程如下：<br><img src="/images/机器学习/单变量线性回归实现1.png" alt="单变量线性回归实现1"></p>
<h3 id="11-1-读取文件"><a href="#11-1-读取文件" class="headerlink" title="11.1 读取文件"></a>11.1 读取文件</h3><p>使用pandas读文件，得到一个类似于excel的表格。使用的函数为 pandas.read_csv()，关键参数如下：<br>1 filepath_or_buffer:字符串类型，要读取的文件存放路径，可以是http和文件<br>2 sep：字符串类型，分隔符，默认为’,’<br>3 header:整数，指定第几行作为列名(忽略注解行)，如果没有指定列名默认header=0,即文件中没有列名<br>4 names列表类型，指定列名<br>使用实例为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pandas是基于numpy的一种工具，该工具是为了解决数据分析任务而创建的</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(filepath_or_buffer=<span class="string">'ex1data1.txt'</span>,names=[<span class="string">'population'</span>,<span class="string">'profit'</span>])</span><br></pre></td></tr></table></figure>
<h3 id="11-2-数据集准备"><a href="#11-2-数据集准备" class="headerlink" title="11.2 数据集准备"></a>11.2 数据集准备</h3><p>绘制数据集的散点图，DataFrame使用data.plot.scatter()方法，关键参数如下：<br>1 横坐标<br>2 纵坐标<br>3 c：绘图使用的颜色<br>4 label:表示散点图的标签<br>5 s:表示绘制的散点的大小<br>使用实例为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.plot.scatter(<span class="string">'population'</span>,<span class="string">'profit'</span>,c=<span class="string">'b'</span>,label=<span class="string">'population'</span>,s=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<p>根据上图中的推导，X矩阵的第一列全为1，所以我们需要在population前面插入一列1。使用的方法为DataFrame.insert(loc,column,value,allow_duplicates=False),关键参数为：<br>1 loc:插入的列索引，如要插入最前面，那么col=0<br>2 column:插入列的标签，字符串<br>3 value:插入列的值<br>使用实例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>在完成插入之后，我们需要将X矩阵的Y矩阵分离开。使用的方法为DataFrame.iloc(),使用的方法为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = data.iloc[:,<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># :表示所有的行都要参与切片</span></span><br><span class="line"><span class="comment"># 0:-1表示从第0列开始，到-1表示最后一列，但是取不到最后一列[0,-1)</span></span><br><span class="line">Y = data.iloc[:,<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># -1表示将最后一列切片出来</span></span><br></pre></td></tr></table></figure>
<p>因为后面我们需要涉及到矩阵运算，所以需要把DataFrame的类型变为数组型的，便于后续计算。DataFrame转换成ndarray的方法有：<br>1 df.values<br>2 df.as_matrix()<br>3 np.array(df)<br>使用实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = X.values</span><br><span class="line">Y = Y.values</span><br></pre></td></tr></table></figure>
<p>接下来我们查看一下数据的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure>
<h3 id="11-3-损失函数"><a href="#11-3-损失函数" class="headerlink" title="11.3 损失函数"></a>11.3 损失函数</h3><p>因为X是2维数组(shape有两个值)，Y是1维数组(shape只有一个值)为了后续运算方便，我们把Y也改成2维数组。方法为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = Y.reshape(<span class="number">97</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们需要定义代价函数。我们的依据是：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}SUM(X\theta-y)^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunction</span><span class="params">(X,Y,theta)</span>:</span></span><br><span class="line">    inner = np.power(X @ theta -Y,<span class="number">2</span>) <span class="comment"># @ 表示矩阵相乘</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(inner)/(<span class="number">2</span>*len(X))</span><br></pre></td></tr></table></figure>
<p>然后，我们对θ进行初始化。θ的维度为2×1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = np.zeros((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>最后我们就可以计算出损失函数的初始值了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost_init = costFunction(X,Y,theta)</span><br></pre></td></tr></table></figure>
<h3 id="11-4-梯度下降函数"><a href="#11-4-梯度下降函数" class="headerlink" title="11.4 梯度下降函数"></a>11.4 梯度下降函数</h3><p>我们更新θ的依据为：</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}X^T(X\theta-Y)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,Y,theta,alpha,iters)</span>:</span></span><br><span class="line">    costs = [] <span class="comment"># 用于存储迭代过程中的cost</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        theta = theta - (X.T @ (X @ theta - Y)) * alpha/len(X)</span><br><span class="line">        cost = costFunction(X,Y,theta)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        <span class="comment"># 打印一些cost</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(cost)</span><br><span class="line">    <span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure>
<p>我们设置的学习率和迭代次数如下,然后调用函数开始迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.02</span></span><br><span class="line">iters = <span class="number">2000</span></span><br><span class="line">theta,costs = gradientDescent(X,Y,theta,alpha,iters)</span><br></pre></td></tr></table></figure>
<h3 id="11-5-可视化损失函数"><a href="#11-5-可视化损失函数" class="headerlink" title="11.5 可视化损失函数"></a>11.5 可视化损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy是一个科学计算库，处理多维数组，进行数据分析</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># matplotlib是python的2D绘图库 matplotlib.pyplot提供一个类似matlab的绘图框架</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 通过该代码生成一幅空的图，ax是一个绘图的实例,后续可以使用ax继续往该图上添加元素</span></span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line"><span class="comment"># 如果是散点图，则为scatter</span></span><br><span class="line"><span class="comment"># 因为costs是list，所以我们需要把int型的iters变成list，方法为np.arrange()</span></span><br><span class="line">ax.plot(np.arange(iters),costs,<span class="string">'b'</span>)</span><br><span class="line"><span class="comment"># 通过ax这个实例设置x和y轴的label</span></span><br><span class="line">ax.set(xlabel=<span class="string">'iters'</span>,ylabel=<span class="string">'costs'</span>,title=<span class="string">'cost vs iters'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 更高级的subplots用法,绘制2*3个统计图</span></span><br><span class="line">fig,ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 第1行第一个图对应的实例名</span></span><br><span class="line">ax[<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="11-6-拟合函数可视化"><a href="#11-6-拟合函数可视化" class="headerlink" title="11.6 拟合函数可视化"></a>11.6 拟合函数可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(Y.min(),Y.max(),<span class="number">100</span>) <span class="comment"># 生成100个点，范围取决于Y,这是因为Y和X的范围差不多</span></span><br><span class="line">y_ = theta[<span class="number">0</span>,<span class="number">0</span>]+ theta[<span class="number">1</span>,<span class="number">0</span>]*x</span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line">ax.scatter(X[:,<span class="number">-1</span>],Y,label=<span class="string">'training data'</span>)</span><br><span class="line">ax.plot(x,y_,<span class="string">'r'</span>,label=<span class="string">'predict'</span>)</span><br><span class="line">ax.legend() <span class="comment"># 显示label</span></span><br><span class="line">ax.set(xlabel=<span class="string">'population'</span>,ylabel=<span class="string">'profit'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="11-7-多变量线性回归"><a href="#11-7-多变量线性回归" class="headerlink" title="11.7 多变量线性回归"></a>11.7 多变量线性回归</h3><p>因为不同特征取值范围相差巨大，所以首先我们需要对特征进行归一化。在实现中我们使用的是z-score normalization，量化后的特征服从标准正态分布：</p>
<script type="math/tex; mode=display">z = \frac{x_i-\mu}{\delta}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data-data.mean()/data.std()</span><br><span class="line">    <span class="comment"># mean为均值，std为标准差</span></span><br></pre></td></tr></table></figure>
<p>多α的cost-iters函数绘制方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig,ax = plt.subplots()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpha_list:</span><br><span class="line">    _,costs = gradientDescent(X,Y,theta,alpha,iters)</span><br><span class="line">    ax.plot(np.arange(iters),costs,label=alpha)</span><br><span class="line">    ax.legend()</span><br><span class="line">ax.set(xlabel=<span class="string">'iters'</span>,ylabel=<span class="string">'costs'</span>,title=<span class="string">'cost vs iters'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="11-8-正规方程解法"><a href="#11-8-正规方程解法" class="headerlink" title="11.8 正规方程解法"></a>11.8 正规方程解法</h3><p>我们求解的依据是</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><p>numpy.linalg模块包含线性代数的函数。使用这个模块，可以计算逆矩阵(inv)、求特征值、解线性方程以及求解行列式等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEquation</span><span class="params">(X,Y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T@X)@X.T@Y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<p><a href="https://www.bilibili.com/video/BV1bt411p74v/?share_source=copy_web&amp;vd_source=e4f879db5bdffb9912efa658461369b9" target="_blank" rel="noopener">学习进度</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag"># 数据挖掘</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/01/%E5%85%B4%E8%B6%A3/%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="prev" title="生物数据挖掘学习">
      <i class="fa fa-chevron-left"></i> 生物数据挖掘学习
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-什么是机器学习"><span class="nav-number">1.</span> <span class="nav-text">1 什么是机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-机器学习的定义"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 机器学习的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-学习算法"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 学习算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-监督学习"><span class="nav-number">2.</span> <span class="nav-text">2 监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-回归问题"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 回归问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-分类问题"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-总结"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-无监督学习"><span class="nav-number">3.</span> <span class="nav-text">3 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-聚类"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-鸡尾酒会算法"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 鸡尾酒会算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-模型描述"><span class="nav-number">4.</span> <span class="nav-text">4 模型描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-代价函数"><span class="nav-number">5.</span> <span class="nav-text">5 代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-代价函数的定义"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 代价函数的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-代价函数的作用"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 代价函数的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-代价函数的作用-plus"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 代价函数的作用-plus</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-梯度下降"><span class="nav-number">6.</span> <span class="nav-text">6 梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-梯度下降算法步骤"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 梯度下降算法步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-梯度下降相关知识"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 梯度下降相关知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-更新函数解释"><span class="nav-number">6.2.1.</span> <span class="nav-text">6.2.1 更新函数解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-学习率解释"><span class="nav-number">6.2.2.</span> <span class="nav-text">6.2.2 学习率解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-线性回归的梯度下降"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 线性回归的梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-矩阵知识回顾"><span class="nav-number">7.</span> <span class="nav-text">7 矩阵知识回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-矩阵和向量定义"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 矩阵和向量定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-矩阵运算"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 矩阵运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-矩阵加法"><span class="nav-number">7.2.1.</span> <span class="nav-text">7.2.1 矩阵加法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-矩阵标量运算"><span class="nav-number">7.2.2.</span> <span class="nav-text">7.2.2 矩阵标量运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-3-矩阵之间的乘法"><span class="nav-number">7.2.3.</span> <span class="nav-text">7.3.3 矩阵之间的乘法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-逆和转置"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 逆和转置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-1-矩阵的逆"><span class="nav-number">7.3.1.</span> <span class="nav-text">7.3.1 矩阵的逆</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-2-矩阵的转置"><span class="nav-number">7.3.2.</span> <span class="nav-text">7.3.2 矩阵的转置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-多元线性回归"><span class="nav-number">8.</span> <span class="nav-text">8 多元线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-多元线性回归定义"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 多元线性回归定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-多元梯度下降法"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 多元梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-1-多元梯度下降法的算法流程"><span class="nav-number">8.2.1.</span> <span class="nav-text">8.2.1 多元梯度下降法的算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-2-多元梯度下降法——特征缩放"><span class="nav-number">8.2.2.</span> <span class="nav-text">8.2.2 多元梯度下降法——特征缩放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-3-多元梯度下降法——学习率"><span class="nav-number">8.2.3.</span> <span class="nav-text">8.2.3 多元梯度下降法——学习率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-特征和多项式回归"><span class="nav-number">9.</span> <span class="nav-text">9 特征和多项式回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-正规方程-区别于迭代方法的线性回归直接解法"><span class="nav-number">10.</span> <span class="nav-text">10 正规方程(区别于迭代方法的线性回归直接解法)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-正规方程求解流程"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 正规方程求解流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-正规方程在矩阵不可逆情况下的解决方法"><span class="nav-number">10.2.</span> <span class="nav-text">10.2 正规方程在矩阵不可逆情况下的解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-单变量线性回归实现"><span class="nav-number">11.</span> <span class="nav-text">11 单变量线性回归实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-读取文件"><span class="nav-number">11.1.</span> <span class="nav-text">11.1 读取文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-数据集准备"><span class="nav-number">11.2.</span> <span class="nav-text">11.2 数据集准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-损失函数"><span class="nav-number">11.3.</span> <span class="nav-text">11.3 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-4-梯度下降函数"><span class="nav-number">11.4.</span> <span class="nav-text">11.4 梯度下降函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-5-可视化损失函数"><span class="nav-number">11.5.</span> <span class="nav-text">11.5 可视化损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-6-拟合函数可视化"><span class="nav-number">11.6.</span> <span class="nav-text">11.6 拟合函数可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-7-多变量线性回归"><span class="nav-number">11.7.</span> <span class="nav-text">11.7 多变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-8-正规方程解法"><span class="nav-number">11.8.</span> <span class="nav-text">11.8 正规方程解法</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monica</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Monica</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
